
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-22 23:47:46.058334: do_dummy_2d_data_aug: False 
2025-03-22 23:47:46.134346: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset219_AMOS2022_postChallenge_task2\splits_final.json 
2025-03-22 23:47:46.141588: The split file contains 5 splits. 
2025-03-22 23:47:46.144588: Desired fold for training: 1 
2025-03-22 23:47:46.147591: This split has 288 training and 72 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_1_spacing_batchsize_3_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [128, 112, 112], 'median_image_size_in_voxels': [450.0, 398.5, 400.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 5, 'features_per_stage': [32, 64, 128, 256, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset219_AMOS2022_postChallenge_task2', 'plans_name': 'nnUNetPlans_1_spacing_batchsize_3', 'original_median_spacing_after_transp': [5.0, 0.712890625, 0.712890625], 'original_median_shape_after_transp': [103, 512, 512], 'image_reader_writer': 'NibabelIOWithReorient', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3284530.75, 'mean': 4168.03125, 'median': 66.0, 'min': -3024.0, 'percentile_00_5': -982.0, 'percentile_99_5': 55963.9765625, 'std': 73746.4765625}}} 
 
2025-03-22 23:48:18.108425: unpacking dataset... 
2025-03-22 23:48:18.772525: unpacking done... 
2025-03-22 23:48:22.419896: Training done. 
2025-03-22 23:48:22.455900: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset219_AMOS2022_postChallenge_task2\splits_final.json 
2025-03-22 23:48:22.463901: The split file contains 5 splits. 
2025-03-22 23:48:22.470900: Desired fold for training: 1 
2025-03-22 23:48:22.477900: This split has 288 training and 72 validation cases. 
2025-03-22 23:48:22.484901: predicting amos_0006 
2025-03-22 23:48:22.491902: amos_0006, shape torch.Size([1, 495, 428, 428]), rank 0 
2025-03-22 23:49:06.504506: predicting amos_0011 
2025-03-22 23:49:06.569019: amos_0011, shape torch.Size([1, 460, 400, 400]), rank 0 
2025-03-22 23:49:50.297101: predicting amos_0018 
2025-03-22 23:49:50.355104: amos_0018, shape torch.Size([1, 520, 433, 433]), rank 0 
2025-03-22 23:50:39.651317: predicting amos_0021 
2025-03-22 23:50:39.700827: amos_0021, shape torch.Size([1, 620, 457, 457]), rank 0 
2025-03-22 23:51:51.906567: predicting amos_0027 
2025-03-22 23:51:51.982074: amos_0027, shape torch.Size([1, 620, 400, 400]), rank 0 
2025-03-22 23:52:47.578951: predicting amos_0040 
2025-03-22 23:52:47.638471: amos_0040, shape torch.Size([1, 500, 400, 400]), rank 0 
2025-03-22 23:53:31.303338: predicting amos_0043 
2025-03-22 23:53:31.356399: amos_0043, shape torch.Size([1, 425, 432, 432]), rank 0 
2025-03-22 23:54:08.757768: predicting amos_0054 
2025-03-22 23:54:08.811768: amos_0054, shape torch.Size([1, 450, 468, 468]), rank 0 
2025-03-22 23:55:04.710880: predicting amos_0058 
2025-03-22 23:55:04.773388: amos_0058, shape torch.Size([1, 475, 460, 460]), rank 0 
2025-03-22 23:56:00.576354: predicting amos_0059 
2025-03-22 23:56:00.637354: amos_0059, shape torch.Size([1, 390, 400, 400]), rank 0 
2025-03-22 23:56:37.469946: predicting amos_0060 
2025-03-22 23:56:37.522453: amos_0060, shape torch.Size([1, 560, 405, 405]), rank 0 
2025-03-22 23:57:26.657315: predicting amos_0069 
2025-03-22 23:57:26.683315: amos_0069, shape torch.Size([1, 490, 410, 410]), rank 0 
2025-03-22 23:58:09.686288: predicting amos_0070 
2025-03-22 23:58:09.739798: amos_0070, shape torch.Size([1, 515, 402, 402]), rank 0 
2025-03-22 23:58:58.828746: predicting amos_0073 
2025-03-22 23:58:58.889254: amos_0073, shape torch.Size([1, 455, 400, 400]), rank 0 
2025-03-22 23:59:41.773687: predicting amos_0086 
2025-03-22 23:59:41.821687: amos_0086, shape torch.Size([1, 400, 439, 439]), rank 0 
2025-03-23 00:00:18.639959: predicting amos_0090 
2025-03-23 00:00:18.707107: amos_0090, shape torch.Size([1, 480, 531, 531]), rank 0 
2025-03-23 00:01:30.065868: predicting amos_0098 
2025-03-23 00:01:30.140999: amos_0098, shape torch.Size([1, 485, 400, 400]), rank 0 
2025-03-23 00:02:13.054521: predicting amos_0117 
2025-03-23 00:02:13.089028: amos_0117, shape torch.Size([1, 515, 390, 390]), rank 0 
2025-03-23 00:02:49.142172: predicting amos_0129 
2025-03-23 00:02:49.181176: amos_0129, shape torch.Size([1, 620, 400, 400]), rank 0 
2025-03-23 00:03:44.155339: predicting amos_0147 
2025-03-23 00:03:44.200339: amos_0147, shape torch.Size([1, 475, 398, 398]), rank 0 
2025-03-23 00:04:26.958797: predicting amos_0154 
2025-03-23 00:04:27.004798: amos_0154, shape torch.Size([1, 440, 400, 400]), rank 0 
2025-03-23 00:05:06.650085: predicting amos_0167 
2025-03-23 00:05:06.702591: amos_0167, shape torch.Size([1, 445, 400, 400]), rank 0 
2025-03-23 00:05:43.510559: predicting amos_0171 
2025-03-23 00:05:43.539560: amos_0171, shape torch.Size([1, 485, 500, 500]), rank 0 
2025-03-23 00:06:39.647360: predicting amos_0181 
2025-03-23 00:06:39.706865: amos_0181, shape torch.Size([1, 590, 443, 443]), rank 0 
2025-03-23 00:07:34.883846: predicting amos_0191 
2025-03-23 00:07:34.954362: amos_0191, shape torch.Size([1, 470, 390, 390]), rank 0 
2025-03-23 00:08:06.449206: predicting amos_0192 
2025-03-23 00:08:06.502206: amos_0192, shape torch.Size([1, 615, 400, 400]), rank 0 
2025-03-23 00:09:01.541335: predicting amos_0195 
2025-03-23 00:09:01.567597: amos_0195, shape torch.Size([1, 670, 488, 488]), rank 0 
2025-03-23 00:10:21.887481: predicting amos_0197 
2025-03-23 00:10:21.973991: amos_0197, shape torch.Size([1, 690, 344, 344]), rank 0 
2025-03-23 00:11:06.987174: predicting amos_0202 
2025-03-23 00:11:07.033680: amos_0202, shape torch.Size([1, 440, 394, 394]), rank 0 
2025-03-23 00:11:43.958950: predicting amos_0206 
2025-03-23 00:11:43.994949: amos_0206, shape torch.Size([1, 604, 313, 313]), rank 0 
2025-03-23 00:12:12.088887: predicting amos_0212 
2025-03-23 00:12:12.121886: amos_0212, shape torch.Size([1, 430, 279, 279]), rank 0 
2025-03-23 00:12:24.263172: predicting amos_0214 
2025-03-23 00:12:24.288168: amos_0214, shape torch.Size([1, 666, 436, 436]), rank 0 
2025-03-23 00:13:25.378125: predicting amos_0217 
2025-03-23 00:13:25.464642: amos_0217, shape torch.Size([1, 612, 352, 352]), rank 0 
2025-03-23 00:14:06.159796: predicting amos_0225 
2025-03-23 00:14:06.221303: amos_0225, shape torch.Size([1, 428, 289, 289]), rank 0 
2025-03-23 00:14:25.025304: predicting amos_0226 
2025-03-23 00:14:25.051303: amos_0226, shape torch.Size([1, 606, 316, 316]), rank 0 
2025-03-23 00:14:53.640823: predicting amos_0228 
2025-03-23 00:14:53.679335: amos_0228, shape torch.Size([1, 594, 349, 349]), rank 0 
2025-03-23 00:15:34.162920: predicting amos_0244 
2025-03-23 00:15:34.203433: amos_0244, shape torch.Size([1, 610, 393, 393]), rank 0 
2025-03-23 00:16:29.182945: predicting amos_0268 
2025-03-23 00:16:29.224457: amos_0268, shape torch.Size([1, 382, 332, 332]), rank 0 
2025-03-23 00:16:44.988752: predicting amos_0279 
2025-03-23 00:16:45.018756: amos_0279, shape torch.Size([1, 441, 360, 360]), rank 0 
2025-03-23 00:17:11.938224: predicting amos_0299 
2025-03-23 00:17:11.971224: amos_0299, shape torch.Size([1, 408, 399, 399]), rank 0 
2025-03-23 00:17:49.203604: predicting amos_0301 
2025-03-23 00:17:49.246604: amos_0301, shape torch.Size([1, 478, 382, 382]), rank 0 
2025-03-23 00:18:20.783364: predicting amos_0304 
2025-03-23 00:18:20.823872: amos_0304, shape torch.Size([1, 350, 330, 330]), rank 0 
2025-03-23 00:18:36.504964: predicting amos_0313 
2025-03-23 00:18:36.529969: amos_0313, shape torch.Size([1, 460, 371, 371]), rank 0 
2025-03-23 00:19:08.023253: predicting amos_0317 
2025-03-23 00:19:08.070253: amos_0317, shape torch.Size([1, 436, 315, 315]), rank 0 
2025-03-23 00:19:26.895658: predicting amos_0321 
2025-03-23 00:19:26.933174: amos_0321, shape torch.Size([1, 384, 368, 368]), rank 0 
2025-03-23 00:19:49.495033: predicting amos_0330 
2025-03-23 00:19:49.535542: amos_0330, shape torch.Size([1, 460, 318, 318]), rank 0 
2025-03-23 00:20:11.385230: predicting amos_0334 
2025-03-23 00:20:11.420737: amos_0334, shape torch.Size([1, 590, 279, 279]), rank 0 
2025-03-23 00:20:29.420864: predicting amos_0339 
2025-03-23 00:20:29.455869: amos_0339, shape torch.Size([1, 436, 299, 299]), rank 0 
2025-03-23 00:20:48.151415: predicting amos_0341 
2025-03-23 00:20:48.180415: amos_0341, shape torch.Size([1, 605, 312, 312]), rank 0 
2025-03-23 00:21:16.150408: predicting amos_0342 
2025-03-23 00:21:16.187407: amos_0342, shape torch.Size([1, 610, 315, 315]), rank 0 
2025-03-23 00:21:44.215129: predicting amos_0348 
2025-03-23 00:21:44.266370: amos_0348, shape torch.Size([1, 634, 346, 346]), rank 0 
2025-03-23 00:22:24.472938: predicting amos_0349 
2025-03-23 00:22:24.521940: amos_0349, shape torch.Size([1, 500, 360, 360]), rank 0 
2025-03-23 00:22:55.861377: predicting amos_0353 
2025-03-23 00:22:55.906597: amos_0353, shape torch.Size([1, 434, 333, 333]), rank 0 
2025-03-23 00:23:14.666830: predicting amos_0367 
2025-03-23 00:23:14.696831: amos_0367, shape torch.Size([1, 455, 500, 500]), rank 0 
2025-03-23 00:24:12.665803: predicting amos_0372 
2025-03-23 00:24:12.724804: amos_0372, shape torch.Size([1, 485, 446, 446]), rank 0 
2025-03-23 00:24:55.669564: predicting amos_0377 
2025-03-23 00:24:55.738001: amos_0377, shape torch.Size([1, 460, 344, 344]), rank 0 
2025-03-23 00:25:27.122819: predicting amos_0387 
2025-03-23 00:25:27.159819: amos_0387, shape torch.Size([1, 525, 447, 447]), rank 0 
2025-03-23 00:26:16.251962: predicting amos_0390 
2025-03-23 00:26:16.276963: amos_0390, shape torch.Size([1, 495, 477, 477]), rank 0 
2025-03-23 00:27:12.517692: predicting amos_0396 
2025-03-23 00:27:12.628201: amos_0396, shape torch.Size([1, 430, 494, 494]), rank 0 
