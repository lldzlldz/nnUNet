
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-23 15:01:29.035893: do_dummy_2d_data_aug: False 
2025-03-23 15:01:29.131788: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset219_AMOS2022_postChallenge_task2\splits_final.json 
2025-03-23 15:01:29.136051: The split file contains 5 splits. 
2025-03-23 15:01:29.139054: Desired fold for training: 0 
2025-03-23 15:01:29.142413: This split has 288 training and 72 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_1_spacing_batchsize_3_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [128, 112, 112], 'median_image_size_in_voxels': [450.0, 398.5, 400.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 5, 'features_per_stage': [32, 64, 128, 256, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset219_AMOS2022_postChallenge_task2', 'plans_name': 'nnUNetPlans_1_spacing_batchsize_3', 'original_median_spacing_after_transp': [5.0, 0.712890625, 0.712890625], 'original_median_shape_after_transp': [103, 512, 512], 'image_reader_writer': 'NibabelIOWithReorient', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3284530.75, 'mean': 4168.03125, 'median': 66.0, 'min': -3024.0, 'percentile_00_5': -982.0, 'percentile_99_5': 55963.9765625, 'std': 73746.4765625}}} 
 
2025-03-23 15:02:00.104941: unpacking dataset... 
2025-03-23 15:02:00.351541: unpacking done... 
2025-03-23 15:02:03.665125: Training done. 
2025-03-23 15:02:03.698639: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset219_AMOS2022_postChallenge_task2\splits_final.json 
2025-03-23 15:02:03.706641: The split file contains 5 splits. 
2025-03-23 15:02:03.712642: Desired fold for training: 0 
2025-03-23 15:02:03.716642: This split has 288 training and 72 validation cases. 
2025-03-23 15:02:03.723640: predicting amos_0014 
2025-03-23 15:02:03.729641: amos_0014, shape torch.Size([1, 465, 400, 400]), rank 0 
2025-03-23 15:02:46.698827: predicting amos_0015 
2025-03-23 15:02:46.735272: amos_0015, shape torch.Size([1, 515, 499, 499]), rank 0 
2025-03-23 15:03:50.611309: predicting amos_0023 
2025-03-23 15:03:50.664820: amos_0023, shape torch.Size([1, 430, 400, 400]), rank 0 
2025-03-23 15:04:27.459935: predicting amos_0024 
2025-03-23 15:04:27.495935: amos_0024, shape torch.Size([1, 425, 400, 400]), rank 0 
2025-03-23 15:05:04.131981: predicting amos_0025 
2025-03-23 15:05:04.176492: amos_0025, shape torch.Size([1, 405, 400, 400]), rank 0 
2025-03-23 15:05:40.749609: predicting amos_0029 
2025-03-23 15:05:40.800039: amos_0029, shape torch.Size([1, 640, 400, 400]), rank 0 
2025-03-23 15:06:35.916987: predicting amos_0035 
2025-03-23 15:06:35.973989: amos_0035, shape torch.Size([1, 465, 492, 492]), rank 0 
2025-03-23 15:07:32.051314: predicting amos_0041 
2025-03-23 15:07:32.102317: amos_0041, shape torch.Size([1, 410, 400, 400]), rank 0 
2025-03-23 15:08:08.883270: predicting amos_0045 
2025-03-23 15:08:08.923272: amos_0045, shape torch.Size([1, 455, 500, 500]), rank 0 
2025-03-23 15:09:04.904457: predicting amos_0049 
2025-03-23 15:09:04.956459: amos_0049, shape torch.Size([1, 480, 390, 390]), rank 0 
2025-03-23 15:09:36.530220: predicting amos_0051 
2025-03-23 15:09:36.566220: amos_0051, shape torch.Size([1, 450, 396, 396]), rank 0 
2025-03-23 15:10:19.202855: predicting amos_0052 
2025-03-23 15:10:19.241324: amos_0052, shape torch.Size([1, 510, 469, 469]), rank 0 
2025-03-23 15:11:15.497470: predicting amos_0061 
2025-03-23 15:11:15.562975: amos_0061, shape torch.Size([1, 390, 400, 400]), rank 0 
2025-03-23 15:11:52.123965: predicting amos_0064 
2025-03-23 15:11:52.155471: amos_0064, shape torch.Size([1, 445, 400, 400]), rank 0 
2025-03-23 15:12:28.731021: predicting amos_0067 
2025-03-23 15:12:28.767530: amos_0067, shape torch.Size([1, 610, 437, 437]), rank 0 
2025-03-23 15:13:23.859982: predicting amos_0071 
2025-03-23 15:13:23.924490: amos_0071, shape torch.Size([1, 675, 428, 428]), rank 0 
2025-03-23 15:14:24.999321: predicting amos_0087 
2025-03-23 15:14:25.054324: amos_0087, shape torch.Size([1, 435, 400, 400]), rank 0 
2025-03-23 15:15:01.764380: predicting amos_0094 
2025-03-23 15:15:01.800890: amos_0094, shape torch.Size([1, 455, 422, 422]), rank 0 
2025-03-23 15:15:44.465785: predicting amos_0111 
2025-03-23 15:15:44.505295: amos_0111, shape torch.Size([1, 535, 400, 400]), rank 0 
2025-03-23 15:16:33.222700: predicting amos_0113 
2025-03-23 15:16:33.280285: amos_0113, shape torch.Size([1, 455, 497, 497]), rank 0 
2025-03-23 15:17:29.029974: predicting amos_0115 
2025-03-23 15:17:29.082974: amos_0115, shape torch.Size([1, 440, 363, 363]), rank 0 
2025-03-23 15:17:56.153911: predicting amos_0125 
2025-03-23 15:17:56.192912: amos_0125, shape torch.Size([1, 525, 426, 426]), rank 0 
2025-03-23 15:18:44.790839: predicting amos_0127 
2025-03-23 15:18:44.837349: amos_0127, shape torch.Size([1, 385, 400, 400]), rank 0 
2025-03-23 15:19:21.481704: predicting amos_0128 
2025-03-23 15:19:21.521704: amos_0128, shape torch.Size([1, 495, 405, 405]), rank 0 
2025-03-23 15:20:04.235208: predicting amos_0132 
2025-03-23 15:20:04.302715: amos_0132, shape torch.Size([1, 655, 492, 492]), rank 0 
2025-03-23 15:21:24.022335: predicting amos_0135 
2025-03-23 15:21:24.086841: amos_0135, shape torch.Size([1, 655, 506, 506]), rank 0 
2025-03-23 15:23:04.707832: predicting amos_0140 
2025-03-23 15:23:04.802340: amos_0140, shape torch.Size([1, 515, 460, 460]), rank 0 
2025-03-23 15:24:08.783677: predicting amos_0142 
2025-03-23 15:24:08.837186: amos_0142, shape torch.Size([1, 470, 467, 467]), rank 0 
2025-03-23 15:25:04.544265: predicting amos_0143 
2025-03-23 15:25:04.596264: amos_0143, shape torch.Size([1, 360, 407, 407]), rank 0 
2025-03-23 15:25:35.027642: predicting amos_0144 
2025-03-23 15:25:35.067149: amos_0144, shape torch.Size([1, 675, 433, 433]), rank 0 
2025-03-23 15:26:39.690719: predicting amos_0162 
2025-03-23 15:26:39.724719: amos_0162, shape torch.Size([1, 390, 400, 400]), rank 0 
2025-03-23 15:27:16.414857: predicting amos_0173 
2025-03-23 15:27:16.448857: amos_0173, shape torch.Size([1, 540, 487, 487]), rank 0 
2025-03-23 15:28:20.340807: predicting amos_0175 
2025-03-23 15:28:20.372807: amos_0175, shape torch.Size([1, 630, 400, 400]), rank 0 
2025-03-23 15:29:15.302026: predicting amos_0176 
2025-03-23 15:29:15.337533: amos_0176, shape torch.Size([1, 615, 385, 385]), rank 0 
2025-03-23 15:29:55.687682: predicting amos_0186 
2025-03-23 15:29:55.745192: amos_0186, shape torch.Size([1, 425, 358, 358]), rank 0 
2025-03-23 15:30:30.560679: predicting amos_0200 
2025-03-23 15:30:30.652756: amos_0200, shape torch.Size([1, 640, 461, 461]), rank 0 
2025-03-23 15:32:19.254783: predicting amos_0204 
2025-03-23 15:32:19.292783: amos_0204, shape torch.Size([1, 416, 337, 337]), rank 0 
2025-03-23 15:32:46.370491: predicting amos_0231 
2025-03-23 15:32:46.401492: amos_0231, shape torch.Size([1, 465, 393, 393]), rank 0 
2025-03-23 15:33:29.960669: predicting amos_0235 
2025-03-23 15:33:30.006671: amos_0235, shape torch.Size([1, 394, 352, 352]), rank 0 
2025-03-23 15:33:58.869734: predicting amos_0239 
2025-03-23 15:33:58.961245: amos_0239, shape torch.Size([1, 584, 342, 342]), rank 0 
2025-03-23 15:34:39.486061: predicting amos_0247 
2025-03-23 15:34:39.562061: amos_0247, shape torch.Size([1, 628, 332, 332]), rank 0 
2025-03-23 15:35:07.721402: predicting amos_0255 
2025-03-23 15:35:07.769406: amos_0255, shape torch.Size([1, 552, 385, 385]), rank 0 
2025-03-23 15:35:43.757639: predicting amos_0278 
2025-03-23 15:35:43.807153: amos_0278, shape torch.Size([1, 464, 355, 355]), rank 0 
2025-03-23 15:36:15.122875: predicting amos_0280 
2025-03-23 15:36:15.161874: amos_0280, shape torch.Size([1, 418, 323, 323]), rank 0 
2025-03-23 15:36:33.876269: predicting amos_0283 
2025-03-23 15:36:33.909778: amos_0283, shape torch.Size([1, 436, 321, 321]), rank 0 
2025-03-23 15:36:52.589577: predicting amos_0296 
2025-03-23 15:36:52.620084: amos_0296, shape torch.Size([1, 468, 407, 407]), rank 0 
2025-03-23 15:37:35.168687: predicting amos_0302 
2025-03-23 15:37:35.210196: amos_0302, shape torch.Size([1, 430, 316, 316]), rank 0 
2025-03-23 15:37:53.992652: predicting amos_0307 
2025-03-23 15:37:54.021159: amos_0307, shape torch.Size([1, 502, 344, 344]), rank 0 
2025-03-23 15:38:25.331924: predicting amos_0328 
2025-03-23 15:38:25.387046: amos_0328, shape torch.Size([1, 640, 348, 348]), rank 0 
2025-03-23 15:39:05.712412: predicting amos_0333 
2025-03-23 15:39:05.752436: amos_0333, shape torch.Size([1, 408, 358, 358]), rank 0 
2025-03-23 15:39:32.668228: predicting amos_0337 
2025-03-23 15:39:32.697228: amos_0337, shape torch.Size([1, 448, 383, 383]), rank 0 
2025-03-23 15:39:59.565773: predicting amos_0363 
2025-03-23 15:39:59.602772: amos_0363, shape torch.Size([1, 395, 390, 390]), rank 0 
2025-03-23 15:40:26.595457: predicting amos_0365 
2025-03-23 15:40:26.628964: amos_0365, shape torch.Size([1, 480, 462, 462]), rank 0 
2025-03-23 15:41:22.091864: predicting amos_0373 
2025-03-23 15:41:22.139373: amos_0373, shape torch.Size([1, 465, 473, 473]), rank 0 
2025-03-23 15:42:17.646587: predicting amos_0378 
2025-03-23 15:42:17.697587: amos_0378, shape torch.Size([1, 480, 486, 486]), rank 0 
2025-03-23 15:43:13.258913: predicting amos_0381 
2025-03-23 15:43:13.325913: amos_0381, shape torch.Size([1, 415, 496, 496]), rank 0 
2025-03-23 15:44:03.280962: predicting amos_0388 
2025-03-23 15:44:03.327311: amos_0388, shape torch.Size([1, 410, 370, 370]), rank 0 
2025-03-23 15:44:30.624875: predicting amos_0402 
2025-03-23 15:44:30.669876: amos_0402, shape torch.Size([1, 465, 424, 424]), rank 0 
2025-03-23 15:45:13.701510: predicting amos_0406 
2025-03-23 15:45:13.774023: amos_0406, shape torch.Size([1, 410, 427, 427]), rank 0 
2025-03-23 15:45:50.581194: predicting amos_0409 
2025-03-23 15:45:50.652510: amos_0409, shape torch.Size([1, 415, 468, 468]), rank 0 
2025-03-23 15:46:38.952027: predicting amos_0507 
2025-03-23 15:46:39.004544: amos_0507, shape torch.Size([1, 180, 343, 379]), rank 0 
2025-03-23 15:46:51.325436: predicting amos_0510 
2025-03-23 15:46:51.529462: amos_0510, shape torch.Size([1, 449, 156, 364]), rank 0 
2025-03-23 15:47:02.406578: predicting amos_0517 
2025-03-23 15:47:02.439579: amos_0517, shape torch.Size([1, 230, 352, 400]), rank 0 
2025-03-23 15:47:18.476350: predicting amos_0541 
2025-03-23 15:47:18.535871: amos_0541, shape torch.Size([1, 216, 343, 379]), rank 0 
2025-03-23 15:47:32.180831: predicting amos_0554 
2025-03-23 15:47:32.220343: amos_0554, shape torch.Size([1, 354, 185, 420]), rank 0 
2025-03-23 15:47:45.572206: predicting amos_0563 
2025-03-23 15:47:45.596207: amos_0563, shape torch.Size([1, 216, 324, 399]), rank 0 
2025-03-23 15:47:58.743854: predicting amos_0570 
2025-03-23 15:47:58.771855: amos_0570, shape torch.Size([1, 216, 306, 378]), rank 0 
2025-03-23 15:48:10.070877: predicting amos_0582 
2025-03-23 15:48:10.097876: amos_0582, shape torch.Size([1, 379, 180, 379]), rank 0 
2025-03-23 15:48:21.493158: predicting amos_0584 
2025-03-23 15:48:21.519157: amos_0584, shape torch.Size([1, 216, 319, 397]), rank 0 
2025-03-23 15:48:34.813455: predicting amos_0587 
2025-03-23 15:48:34.845458: amos_0587, shape torch.Size([1, 216, 308, 379]), rank 0 
2025-03-23 15:48:46.172597: predicting amos_0591 
2025-03-23 15:48:46.190599: amos_0591, shape torch.Size([1, 216, 306, 379]), rank 0 
2025-03-23 15:48:57.568167: predicting amos_0599 
2025-03-23 15:48:57.589171: amos_0599, shape torch.Size([1, 216, 306, 379]), rank 0 
2025-03-23 15:50:11.931133: Validation complete 
2025-03-23 15:50:11.936133: Mean Validation Dice:  0.8875628151491639 
