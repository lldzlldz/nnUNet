
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-07 20:29:30.126597: do_dummy_2d_data_aug: False 
2025-01-07 20:29:30.132597: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset008_HepaticVessel\splits_final.json 
2025-01-07 20:29:30.139597: The split file contains 5 splits. 
2025-01-07 20:29:30.141596: Desired fold for training: 8 
2025-01-07 20:29:30.144595: INFO: You requested fold 8 for training but splits contain only 5 folds. I am now creating a random (but seeded) 80:20 split! 
2025-01-07 20:29:30.147595: This random 80:20 split has 242 training and 61 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_avg_spacing1_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [96, 160, 160], 'median_image_size_in_voxels': [225.0, 409.0, 409.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset008_HepaticVessel', 'plans_name': 'nnUNetPlans_avg_spacing1', 'original_median_spacing_after_transp': [5.0, 0.7988280057907104, 0.7988280057907104], 'original_median_shape_after_transp': [49, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3072.0, 'mean': 128.6698455810547, 'median': 129.0, 'min': -726.0, 'percentile_00_5': 8.0, 'percentile_99_5': 268.0, 'std': 54.57704544067383}}} 
 
2025-01-07 20:30:02.414866: unpacking dataset... 
2025-01-07 20:30:45.084710: unpacking done... 
2025-01-07 20:30:49.970402:  
2025-01-07 20:30:49.970402: Epoch 0 
2025-01-07 20:30:49.975519: Current learning rate: 0.01 
2025-01-07 20:31:37.455215: train_loss 0.1411 
2025-01-07 20:31:37.456216: val_loss 0.1174 
2025-01-07 20:31:37.462738: Pseudo dice [np.float32(0.0), np.float32(0.0)] 
2025-01-07 20:31:37.466749: Epoch time: 47.49 s 
2025-01-07 20:31:37.470848: Yayy! New best EMA pseudo Dice: 0.0 
2025-01-07 20:31:38.093447:  
2025-01-07 20:31:38.093447: Epoch 1 
2025-01-07 20:31:38.098519: Current learning rate: 0.00991 
2025-01-07 20:32:20.939049: train_loss 0.0117 
2025-01-07 20:32:20.939049: val_loss -0.0039 
2025-01-07 20:32:20.944059: Pseudo dice [np.float32(0.0), np.float32(0.0)] 
2025-01-07 20:32:20.948613: Epoch time: 42.85 s 
2025-01-07 20:32:21.461828:  
2025-01-07 20:32:21.461828: Epoch 2 
2025-01-07 20:32:21.467394: Current learning rate: 0.00982 
2025-01-07 20:33:04.373096: train_loss -0.0494 
2025-01-07 20:33:04.373096: val_loss -0.0403 
2025-01-07 20:33:04.379654: Pseudo dice [np.float32(0.1085), np.float32(0.0263)] 
2025-01-07 20:33:04.384667: Epoch time: 42.91 s 
2025-01-07 20:33:04.388178: Yayy! New best EMA pseudo Dice: 0.0066999997943639755 
2025-01-07 20:33:05.079739:  
2025-01-07 20:33:05.080739: Epoch 3 
2025-01-07 20:33:05.086327: Current learning rate: 0.00973 
2025-01-07 20:33:47.952886: train_loss -0.1728 
2025-01-07 20:33:47.952886: val_loss -0.15 
2025-01-07 20:33:47.958906: Pseudo dice [np.float32(0.4873), np.float32(0.1844)] 
2025-01-07 20:33:47.962416: Epoch time: 42.87 s 
2025-01-07 20:33:47.965427: Yayy! New best EMA pseudo Dice: 0.03970000147819519 
2025-01-07 20:33:48.662009:  
2025-01-07 20:33:48.662009: Epoch 4 
2025-01-07 20:33:48.667065: Current learning rate: 0.00964 
2025-01-07 20:34:31.576298: train_loss -0.2144 
2025-01-07 20:34:31.576814: val_loss -0.1712 
2025-01-07 20:34:31.582387: Pseudo dice [np.float32(0.5648), np.float32(0.34)] 
2025-01-07 20:34:31.585924: Epoch time: 42.92 s 
2025-01-07 20:34:31.588958: Yayy! New best EMA pseudo Dice: 0.08089999854564667 
2025-01-07 20:34:32.424049:  
2025-01-07 20:34:32.424049: Epoch 5 
2025-01-07 20:34:32.429565: Current learning rate: 0.00955 
2025-01-07 20:35:15.296710: train_loss -0.2606 
2025-01-07 20:35:15.296710: val_loss -0.2127 
2025-01-07 20:35:15.303730: Pseudo dice [np.float32(0.5547), np.float32(0.3114)] 
2025-01-07 20:35:15.307769: Epoch time: 42.87 s 
2025-01-07 20:35:15.312785: Yayy! New best EMA pseudo Dice: 0.1160999983549118 
2025-01-07 20:35:16.012860:  
2025-01-07 20:35:16.012860: Epoch 6 
2025-01-07 20:35:16.017872: Current learning rate: 0.00946 
2025-01-07 20:35:58.939612: train_loss -0.2554 
2025-01-07 20:35:58.940615: val_loss -0.1639 
2025-01-07 20:35:58.945631: Pseudo dice [np.float32(0.5402), np.float32(0.3591)] 
2025-01-07 20:35:58.949639: Epoch time: 42.93 s 
2025-01-07 20:35:58.952146: Yayy! New best EMA pseudo Dice: 0.14949999749660492 
2025-01-07 20:35:59.728583:  
2025-01-07 20:35:59.728583: Epoch 7 
2025-01-07 20:35:59.733593: Current learning rate: 0.00937 
2025-01-07 20:36:42.656913: train_loss -0.2921 
2025-01-07 20:36:42.657915: val_loss -0.264 
2025-01-07 20:36:42.662934: Pseudo dice [np.float32(0.5502), np.float32(0.4107)] 
2025-01-07 20:36:42.666949: Epoch time: 42.93 s 
2025-01-07 20:36:42.671462: Yayy! New best EMA pseudo Dice: 0.1826000064611435 
2025-01-07 20:36:43.390648:  
2025-01-07 20:36:43.391159: Epoch 8 
2025-01-07 20:36:43.396234: Current learning rate: 0.00928 
2025-01-07 20:37:26.276536: train_loss -0.3184 
2025-01-07 20:37:26.277539: val_loss -0.3392 
2025-01-07 20:37:26.283188: Pseudo dice [np.float32(0.5742), np.float32(0.5823)] 
2025-01-07 20:37:26.286259: Epoch time: 42.89 s 
2025-01-07 20:37:26.288808: Yayy! New best EMA pseudo Dice: 0.22220000624656677 
2025-01-07 20:37:26.996277:  
2025-01-07 20:37:26.996277: Epoch 9 
2025-01-07 20:37:27.001321: Current learning rate: 0.00919 
2025-01-07 20:38:09.931267: train_loss -0.3265 
2025-01-07 20:38:09.931779: val_loss -0.2945 
2025-01-07 20:38:09.937366: Pseudo dice [np.float32(0.5708), np.float32(0.4972)] 
2025-01-07 20:38:09.940926: Epoch time: 42.94 s 
2025-01-07 20:38:09.943977: Yayy! New best EMA pseudo Dice: 0.2533000111579895 
2025-01-07 20:38:10.641115:  
2025-01-07 20:38:10.642117: Epoch 10 
2025-01-07 20:38:10.646718: Current learning rate: 0.0091 
2025-01-07 20:38:53.623804: train_loss -0.34 
2025-01-07 20:38:53.623804: val_loss -0.3104 
2025-01-07 20:38:53.629828: Pseudo dice [np.float32(0.5864), np.float32(0.4952)] 
2025-01-07 20:38:53.633339: Epoch time: 42.98 s 
2025-01-07 20:38:53.636351: Yayy! New best EMA pseudo Dice: 0.28209999203681946 
2025-01-07 20:38:54.337902:  
2025-01-07 20:38:54.338907: Epoch 11 
2025-01-07 20:38:54.343457: Current learning rate: 0.009 
2025-01-07 20:39:37.223300: train_loss -0.3306 
2025-01-07 20:39:37.224301: val_loss -0.3001 
2025-01-07 20:39:37.229342: Pseudo dice [np.float32(0.5902), np.float32(0.4276)] 
2025-01-07 20:39:37.232365: Epoch time: 42.89 s 
2025-01-07 20:39:37.234873: Yayy! New best EMA pseudo Dice: 0.30480000376701355 
2025-01-07 20:39:37.930103:  
2025-01-07 20:39:37.931102: Epoch 12 
2025-01-07 20:39:37.936177: Current learning rate: 0.00891 
2025-01-07 20:40:20.837915: train_loss -0.3681 
2025-01-07 20:40:20.837915: val_loss -0.2781 
2025-01-07 20:40:20.844577: Pseudo dice [np.float32(0.6114), np.float32(0.4223)] 
2025-01-07 20:40:20.847644: Epoch time: 42.91 s 
2025-01-07 20:40:20.851708: Yayy! New best EMA pseudo Dice: 0.32600000500679016 
2025-01-07 20:40:21.720341:  
2025-01-07 20:40:21.720843: Epoch 13 
2025-01-07 20:40:21.725856: Current learning rate: 0.00882 
2025-01-07 20:41:04.567987: train_loss -0.3554 
2025-01-07 20:41:04.568993: val_loss -0.2689 
2025-01-07 20:41:04.574906: Pseudo dice [np.float32(0.5675), np.float32(0.4318)] 
2025-01-07 20:41:04.577443: Epoch time: 42.85 s 
2025-01-07 20:41:04.580000: Yayy! New best EMA pseudo Dice: 0.3433000147342682 
2025-01-07 20:41:05.289876:  
2025-01-07 20:41:05.289876: Epoch 14 
2025-01-07 20:41:05.294549: Current learning rate: 0.00873 
2025-01-07 20:41:48.200075: train_loss -0.3826 
2025-01-07 20:41:48.201075: val_loss -0.2266 
2025-01-07 20:41:48.206594: Pseudo dice [np.float32(0.5847), np.float32(0.3819)] 
2025-01-07 20:41:48.212614: Epoch time: 42.91 s 
2025-01-07 20:41:48.216120: Yayy! New best EMA pseudo Dice: 0.3573000133037567 
2025-01-07 20:41:48.926538:  
2025-01-07 20:41:48.926538: Epoch 15 
2025-01-07 20:41:48.932162: Current learning rate: 0.00864 
2025-01-07 20:42:31.847458: train_loss -0.3752 
2025-01-07 20:42:31.847458: val_loss -0.2942 
2025-01-07 20:42:31.853080: Pseudo dice [np.float32(0.5771), np.float32(0.5148)] 
2025-01-07 20:42:31.857632: Epoch time: 42.92 s 
2025-01-07 20:42:31.860681: Yayy! New best EMA pseudo Dice: 0.37619999051094055 
2025-01-07 20:42:32.596507:  
2025-01-07 20:42:32.597010: Epoch 16 
2025-01-07 20:42:32.601522: Current learning rate: 0.00855 
2025-01-07 20:43:15.568685: train_loss -0.3542 
2025-01-07 20:43:15.570188: val_loss -0.3101 
2025-01-07 20:43:15.576214: Pseudo dice [np.float32(0.5217), np.float32(0.5951)] 
2025-01-07 20:43:15.578720: Epoch time: 42.97 s 
2025-01-07 20:43:15.581226: Yayy! New best EMA pseudo Dice: 0.3944000005722046 
2025-01-07 20:43:16.310360:  
2025-01-07 20:43:16.310360: Epoch 17 
2025-01-07 20:43:16.315993: Current learning rate: 0.00846 
2025-01-07 20:43:59.194384: train_loss -0.424 
2025-01-07 20:43:59.194887: val_loss -0.4003 
2025-01-07 20:43:59.200965: Pseudo dice [np.float32(0.6367), np.float32(0.5683)] 
2025-01-07 20:43:59.204489: Epoch time: 42.89 s 
2025-01-07 20:43:59.207512: Yayy! New best EMA pseudo Dice: 0.41519999504089355 
2025-01-07 20:43:59.934246:  
2025-01-07 20:43:59.934246: Epoch 18 
2025-01-07 20:43:59.939766: Current learning rate: 0.00836 
2025-01-07 20:44:42.820167: train_loss -0.399 
2025-01-07 20:44:42.820167: val_loss -0.3823 
2025-01-07 20:44:42.825755: Pseudo dice [np.float32(0.5837), np.float32(0.6082)] 
2025-01-07 20:44:42.829815: Epoch time: 42.89 s 
2025-01-07 20:44:42.833911: Yayy! New best EMA pseudo Dice: 0.4332999885082245 
2025-01-07 20:44:43.551297:  
2025-01-07 20:44:43.551297: Epoch 19 
2025-01-07 20:44:43.556344: Current learning rate: 0.00827 
2025-01-07 20:45:26.473734: train_loss -0.4029 
2025-01-07 20:45:26.474244: val_loss -0.3498 
2025-01-07 20:45:26.479302: Pseudo dice [np.float32(0.5891), np.float32(0.6372)] 
2025-01-07 20:45:26.484426: Epoch time: 42.92 s 
2025-01-07 20:45:26.486995: Yayy! New best EMA pseudo Dice: 0.4512999951839447 
2025-01-07 20:45:27.343309:  
2025-01-07 20:45:27.343309: Epoch 20 
2025-01-07 20:45:27.349352: Current learning rate: 0.00818 
2025-01-07 20:46:10.256440: train_loss -0.4435 
2025-01-07 20:46:10.256440: val_loss -0.447 
2025-01-07 20:46:10.262954: Pseudo dice [np.float32(0.5937), np.float32(0.7454)] 
2025-01-07 20:46:10.265462: Epoch time: 42.91 s 
2025-01-07 20:46:10.269517: Yayy! New best EMA pseudo Dice: 0.4731000065803528 
2025-01-07 20:46:10.995330:  
2025-01-07 20:46:10.995330: Epoch 21 
2025-01-07 20:46:11.000453: Current learning rate: 0.00809 
2025-01-07 20:46:53.886473: train_loss -0.4401 
2025-01-07 20:46:53.886473: val_loss -0.4004 
2025-01-07 20:46:53.892680: Pseudo dice [np.float32(0.6073), np.float32(0.6747)] 
2025-01-07 20:46:53.896693: Epoch time: 42.89 s 
2025-01-07 20:46:53.900198: Yayy! New best EMA pseudo Dice: 0.48989999294281006 
2025-01-07 20:46:54.597904:  
2025-01-07 20:46:54.597904: Epoch 22 
2025-01-07 20:46:54.603997: Current learning rate: 0.008 
2025-01-07 20:47:37.481771: train_loss -0.4281 
2025-01-07 20:47:37.482775: val_loss -0.4193 
2025-01-07 20:47:37.488436: Pseudo dice [np.float32(0.607), np.float32(0.7168)] 
2025-01-07 20:47:37.492446: Epoch time: 42.88 s 
2025-01-07 20:47:37.494954: Yayy! New best EMA pseudo Dice: 0.507099986076355 
2025-01-07 20:47:38.199983:  
2025-01-07 20:47:38.200485: Epoch 23 
2025-01-07 20:47:38.205496: Current learning rate: 0.0079 
2025-01-07 20:48:21.100051: train_loss -0.4119 
2025-01-07 20:48:21.100051: val_loss -0.0981 
2025-01-07 20:48:21.106570: Pseudo dice [np.float32(0.5544), np.float32(0.1393)] 
2025-01-07 20:48:21.110081: Epoch time: 42.9 s 
2025-01-07 20:48:21.648995:  
2025-01-07 20:48:21.648995: Epoch 24 
2025-01-07 20:48:21.654552: Current learning rate: 0.00781 
2025-01-07 20:49:04.576821: train_loss -0.4313 
2025-01-07 20:49:04.576821: val_loss -0.4336 
2025-01-07 20:49:04.583341: Pseudo dice [np.float32(0.593), np.float32(0.6825)] 
2025-01-07 20:49:04.587862: Epoch time: 42.93 s 
2025-01-07 20:49:05.105839:  
2025-01-07 20:49:05.106842: Epoch 25 
2025-01-07 20:49:05.111875: Current learning rate: 0.00772 
2025-01-07 20:49:48.001029: train_loss -0.4317 
2025-01-07 20:49:48.002031: val_loss -0.3979 
2025-01-07 20:49:48.008548: Pseudo dice [np.float32(0.5887), np.float32(0.7019)] 
2025-01-07 20:49:48.012060: Epoch time: 42.9 s 
2025-01-07 20:49:48.017072: Yayy! New best EMA pseudo Dice: 0.5196999907493591 
2025-01-07 20:49:48.685750:  
2025-01-07 20:49:48.686257: Epoch 26 
2025-01-07 20:49:48.691270: Current learning rate: 0.00763 
2025-01-07 20:50:31.678831: train_loss -0.4608 
2025-01-07 20:50:31.678831: val_loss -0.4139 
2025-01-07 20:50:31.684847: Pseudo dice [np.float32(0.6166), np.float32(0.7101)] 
2025-01-07 20:50:31.687860: Epoch time: 42.99 s 
2025-01-07 20:50:31.690368: Yayy! New best EMA pseudo Dice: 0.5340999960899353 
2025-01-07 20:50:32.398297:  
2025-01-07 20:50:32.399298: Epoch 27 
2025-01-07 20:50:32.404867: Current learning rate: 0.00753 
2025-01-07 20:51:15.314789: train_loss -0.4222 
2025-01-07 20:51:15.315293: val_loss -0.4591 
2025-01-07 20:51:15.321310: Pseudo dice [np.float32(0.6308), np.float32(0.761)] 
2025-01-07 20:51:15.323816: Epoch time: 42.92 s 
2025-01-07 20:51:15.327821: Yayy! New best EMA pseudo Dice: 0.5501999855041504 
2025-01-07 20:51:16.027967:  
2025-01-07 20:51:16.027967: Epoch 28 
2025-01-07 20:51:16.034505: Current learning rate: 0.00744 
2025-01-07 20:51:58.943413: train_loss -0.4691 
2025-01-07 20:51:58.943935: val_loss -0.4822 
2025-01-07 20:51:58.948953: Pseudo dice [np.float32(0.5554), np.float32(0.7865)] 
2025-01-07 20:51:58.953466: Epoch time: 42.92 s 
2025-01-07 20:51:58.956478: Yayy! New best EMA pseudo Dice: 0.5623000264167786 
2025-01-07 20:51:59.864169:  
2025-01-07 20:51:59.865172: Epoch 29 
2025-01-07 20:51:59.869752: Current learning rate: 0.00735 
2025-01-07 20:52:42.832302: train_loss -0.4701 
2025-01-07 20:52:42.832302: val_loss -0.4009 
2025-01-07 20:52:42.838319: Pseudo dice [np.float32(0.5873), np.float32(0.7015)] 
2025-01-07 20:52:42.841328: Epoch time: 42.97 s 
2025-01-07 20:52:42.844836: Yayy! New best EMA pseudo Dice: 0.5705000162124634 
2025-01-07 20:52:43.556865:  
2025-01-07 20:52:43.557868: Epoch 30 
2025-01-07 20:52:43.562895: Current learning rate: 0.00725 
2025-01-07 20:53:26.439769: train_loss -0.4834 
2025-01-07 20:53:26.440272: val_loss -0.4451 
2025-01-07 20:53:26.445956: Pseudo dice [np.float32(0.5921), np.float32(0.7556)] 
2025-01-07 20:53:26.449046: Epoch time: 42.88 s 
2025-01-07 20:53:26.452613: Yayy! New best EMA pseudo Dice: 0.5807999968528748 
2025-01-07 20:53:27.166186:  
2025-01-07 20:53:27.166186: Epoch 31 
2025-01-07 20:53:27.171785: Current learning rate: 0.00716 
2025-01-07 20:54:10.077121: train_loss -0.4517 
2025-01-07 20:54:10.078120: val_loss -0.4113 
2025-01-07 20:54:10.083687: Pseudo dice [np.float32(0.6442), np.float32(0.6874)] 
2025-01-07 20:54:10.086192: Epoch time: 42.91 s 
2025-01-07 20:54:10.089706: Yayy! New best EMA pseudo Dice: 0.5892999768257141 
2025-01-07 20:54:10.813132:  
2025-01-07 20:54:10.813641: Epoch 32 
2025-01-07 20:54:10.818186: Current learning rate: 0.00707 
2025-01-07 20:54:53.748877: train_loss -0.4739 
2025-01-07 20:54:53.748877: val_loss -0.4017 
2025-01-07 20:54:53.754895: Pseudo dice [np.float32(0.6177), np.float32(0.6177)] 
2025-01-07 20:54:53.758907: Epoch time: 42.94 s 
2025-01-07 20:54:53.763920: Yayy! New best EMA pseudo Dice: 0.592199981212616 
2025-01-07 20:54:54.483766:  
2025-01-07 20:54:54.484771: Epoch 33 
2025-01-07 20:54:54.489328: Current learning rate: 0.00697 
2025-01-07 20:55:37.358641: train_loss -0.4762 
2025-01-07 20:55:37.358641: val_loss -0.4669 
2025-01-07 20:55:37.365162: Pseudo dice [np.float32(0.6244), np.float32(0.7546)] 
2025-01-07 20:55:37.371179: Epoch time: 42.87 s 
2025-01-07 20:55:37.375190: Yayy! New best EMA pseudo Dice: 0.6018999814987183 
2025-01-07 20:55:38.081718:  
2025-01-07 20:55:38.082220: Epoch 34 
2025-01-07 20:55:38.086788: Current learning rate: 0.00688 
2025-01-07 20:56:21.012405: train_loss -0.4917 
2025-01-07 20:56:21.012405: val_loss -0.4728 
2025-01-07 20:56:21.018422: Pseudo dice [np.float32(0.6305), np.float32(0.7016)] 
2025-01-07 20:56:21.021932: Epoch time: 42.93 s 
2025-01-07 20:56:21.024941: Yayy! New best EMA pseudo Dice: 0.608299970626831 
2025-01-07 20:56:21.746356:  
2025-01-07 20:56:21.746356: Epoch 35 
2025-01-07 20:56:21.752373: Current learning rate: 0.00679 
2025-01-07 20:57:04.626744: train_loss -0.4823 
2025-01-07 20:57:04.626744: val_loss -0.4548 
2025-01-07 20:57:04.632756: Pseudo dice [np.float32(0.6364), np.float32(0.7682)] 
2025-01-07 20:57:04.636770: Epoch time: 42.88 s 
2025-01-07 20:57:04.640778: Yayy! New best EMA pseudo Dice: 0.6176999807357788 
2025-01-07 20:57:05.373164:  
2025-01-07 20:57:05.373164: Epoch 36 
2025-01-07 20:57:05.378738: Current learning rate: 0.00669 
2025-01-07 20:57:48.252231: train_loss -0.5014 
2025-01-07 20:57:48.252231: val_loss -0.4449 
2025-01-07 20:57:48.259754: Pseudo dice [np.float32(0.6155), np.float32(0.7784)] 
2025-01-07 20:57:48.263263: Epoch time: 42.88 s 
2025-01-07 20:57:48.267274: Yayy! New best EMA pseudo Dice: 0.6256999969482422 
2025-01-07 20:57:49.134212:  
2025-01-07 20:57:49.134212: Epoch 37 
2025-01-07 20:57:49.140333: Current learning rate: 0.0066 
2025-01-07 20:58:32.036298: train_loss -0.4603 
2025-01-07 20:58:32.036812: val_loss -0.488 
2025-01-07 20:58:32.041892: Pseudo dice [np.float32(0.6344), np.float32(0.8165)] 
2025-01-07 20:58:32.045454: Epoch time: 42.9 s 
2025-01-07 20:58:32.048971: Yayy! New best EMA pseudo Dice: 0.6355999708175659 
2025-01-07 20:58:32.815976:  
2025-01-07 20:58:32.815976: Epoch 38 
2025-01-07 20:58:32.821563: Current learning rate: 0.0065 
2025-01-07 20:59:15.766711: train_loss -0.4896 
2025-01-07 20:59:15.767711: val_loss -0.4413 
2025-01-07 20:59:15.773233: Pseudo dice [np.float32(0.6389), np.float32(0.698)] 
2025-01-07 20:59:15.776750: Epoch time: 42.95 s 
2025-01-07 20:59:15.780762: Yayy! New best EMA pseudo Dice: 0.6388999819755554 
2025-01-07 20:59:16.504876:  
2025-01-07 20:59:16.504876: Epoch 39 
2025-01-07 20:59:16.509491: Current learning rate: 0.00641 
2025-01-07 20:59:59.427579: train_loss -0.4868 
2025-01-07 20:59:59.428583: val_loss -0.5108 
2025-01-07 20:59:59.434193: Pseudo dice [np.float32(0.6043), np.float32(0.8293)] 
2025-01-07 20:59:59.439208: Epoch time: 42.92 s 
2025-01-07 20:59:59.442760: Yayy! New best EMA pseudo Dice: 0.6467000246047974 
2025-01-07 21:00:00.187454:  
2025-01-07 21:00:00.187454: Epoch 40 
2025-01-07 21:00:00.193568: Current learning rate: 0.00631 
2025-01-07 21:00:43.042878: train_loss -0.4971 
2025-01-07 21:00:43.043881: val_loss -0.4326 
2025-01-07 21:00:43.048403: Pseudo dice [np.float32(0.6098), np.float32(0.7269)] 
2025-01-07 21:00:43.052982: Epoch time: 42.86 s 
2025-01-07 21:00:43.056490: Yayy! New best EMA pseudo Dice: 0.6488999724388123 
2025-01-07 21:00:43.778872:  
2025-01-07 21:00:43.778872: Epoch 41 
2025-01-07 21:00:43.783881: Current learning rate: 0.00622 
2025-01-07 21:01:26.684741: train_loss -0.4938 
2025-01-07 21:01:26.685744: val_loss -0.4544 
2025-01-07 21:01:26.691763: Pseudo dice [np.float32(0.601), np.float32(0.7638)] 
2025-01-07 21:01:26.695772: Epoch time: 42.91 s 
2025-01-07 21:01:26.699779: Yayy! New best EMA pseudo Dice: 0.6521999835968018 
2025-01-07 21:01:27.402494:  
2025-01-07 21:01:27.402494: Epoch 42 
2025-01-07 21:01:27.408056: Current learning rate: 0.00612 
2025-01-07 21:02:10.409611: train_loss -0.5178 
2025-01-07 21:02:10.409611: val_loss -0.426 
2025-01-07 21:02:10.415670: Pseudo dice [np.float32(0.6285), np.float32(0.7102)] 
2025-01-07 21:02:10.420255: Epoch time: 43.01 s 
2025-01-07 21:02:10.423827: Yayy! New best EMA pseudo Dice: 0.6539000272750854 
2025-01-07 21:02:11.128250:  
2025-01-07 21:02:11.128250: Epoch 43 
2025-01-07 21:02:11.133782: Current learning rate: 0.00603 
2025-01-07 21:02:54.052306: train_loss -0.5174 
2025-01-07 21:02:54.052814: val_loss -0.4651 
2025-01-07 21:02:54.058392: Pseudo dice [np.float32(0.6128), np.float32(0.807)] 
2025-01-07 21:02:54.061454: Epoch time: 42.93 s 
2025-01-07 21:02:54.063563: Yayy! New best EMA pseudo Dice: 0.659500002861023 
2025-01-07 21:02:54.914932:  
2025-01-07 21:02:54.914932: Epoch 44 
2025-01-07 21:02:54.921021: Current learning rate: 0.00593 
2025-01-07 21:03:37.802315: train_loss -0.5133 
2025-01-07 21:03:37.802823: val_loss -0.5054 
2025-01-07 21:03:37.807905: Pseudo dice [np.float32(0.6303), np.float32(0.7979)] 
2025-01-07 21:03:37.810519: Epoch time: 42.89 s 
2025-01-07 21:03:37.813572: Yayy! New best EMA pseudo Dice: 0.6650000214576721 
2025-01-07 21:03:38.514041:  
2025-01-07 21:03:38.514041: Epoch 45 
2025-01-07 21:03:38.519162: Current learning rate: 0.00584 
2025-01-07 21:04:21.443976: train_loss -0.5177 
2025-01-07 21:04:21.445484: val_loss -0.4467 
2025-01-07 21:04:21.451080: Pseudo dice [np.float32(0.5977), np.float32(0.7343)] 
2025-01-07 21:04:21.454591: Epoch time: 42.93 s 
2025-01-07 21:04:21.459606: Yayy! New best EMA pseudo Dice: 0.6650999784469604 
2025-01-07 21:04:22.158512:  
2025-01-07 21:04:22.158512: Epoch 46 
2025-01-07 21:04:22.164580: Current learning rate: 0.00574 
2025-01-07 21:05:05.073910: train_loss -0.5241 
2025-01-07 21:05:05.074914: val_loss -0.5502 
2025-01-07 21:05:05.080929: Pseudo dice [np.float32(0.6316), np.float32(0.7942)] 
2025-01-07 21:05:05.085943: Epoch time: 42.92 s 
2025-01-07 21:05:05.091461: Yayy! New best EMA pseudo Dice: 0.6699000000953674 
2025-01-07 21:05:05.809937:  
2025-01-07 21:05:05.810440: Epoch 47 
2025-01-07 21:05:05.815454: Current learning rate: 0.00565 
2025-01-07 21:05:48.748007: train_loss -0.5105 
2025-01-07 21:05:48.748509: val_loss -0.4688 
2025-01-07 21:05:48.754529: Pseudo dice [np.float32(0.6155), np.float32(0.7766)] 
2025-01-07 21:05:48.758538: Epoch time: 42.94 s 
2025-01-07 21:05:48.762051: Yayy! New best EMA pseudo Dice: 0.6725000143051147 
2025-01-07 21:05:49.469331:  
2025-01-07 21:05:49.470834: Epoch 48 
2025-01-07 21:05:49.475842: Current learning rate: 0.00555 
2025-01-07 21:06:32.460795: train_loss -0.5167 
2025-01-07 21:06:32.461297: val_loss -0.5384 
2025-01-07 21:06:32.466872: Pseudo dice [np.float32(0.6481), np.float32(0.8219)] 
2025-01-07 21:06:32.471469: Epoch time: 42.99 s 
2025-01-07 21:06:32.476008: Yayy! New best EMA pseudo Dice: 0.6786999702453613 
2025-01-07 21:06:33.171440:  
2025-01-07 21:06:33.171440: Epoch 49 
2025-01-07 21:06:33.176455: Current learning rate: 0.00546 
2025-01-07 21:07:16.080687: train_loss -0.5361 
2025-01-07 21:07:16.081689: val_loss -0.5271 
2025-01-07 21:07:16.088296: Pseudo dice [np.float32(0.6555), np.float32(0.7967)] 
2025-01-07 21:07:16.091811: Epoch time: 42.91 s 
2025-01-07 21:07:16.246479: Yayy! New best EMA pseudo Dice: 0.6834999918937683 
2025-01-07 21:07:16.992711:  
2025-01-07 21:07:16.992711: Epoch 50 
2025-01-07 21:07:16.998248: Current learning rate: 0.00536 
2025-01-07 21:07:59.942281: train_loss -0.5258 
2025-01-07 21:07:59.942281: val_loss -0.5375 
2025-01-07 21:07:59.948305: Pseudo dice [np.float32(0.6303), np.float32(0.8316)] 
2025-01-07 21:07:59.952319: Epoch time: 42.95 s 
2025-01-07 21:07:59.956831: Yayy! New best EMA pseudo Dice: 0.6881999969482422 
2025-01-07 21:08:00.673146:  
2025-01-07 21:08:00.673146: Epoch 51 
2025-01-07 21:08:00.678195: Current learning rate: 0.00526 
2025-01-07 21:08:43.640211: train_loss -0.5423 
2025-01-07 21:08:43.640714: val_loss -0.5631 
2025-01-07 21:08:43.646307: Pseudo dice [np.float32(0.6357), np.float32(0.8369)] 
2025-01-07 21:08:43.650977: Epoch time: 42.97 s 
2025-01-07 21:08:43.656020: Yayy! New best EMA pseudo Dice: 0.6930000185966492 
2025-01-07 21:08:44.364497:  
2025-01-07 21:08:44.365496: Epoch 52 
2025-01-07 21:08:44.370562: Current learning rate: 0.00517 
2025-01-07 21:09:27.315942: train_loss -0.5253 
2025-01-07 21:09:27.316942: val_loss -0.4643 
2025-01-07 21:09:27.322459: Pseudo dice [np.float32(0.6262), np.float32(0.695)] 
2025-01-07 21:09:27.327480: Epoch time: 42.95 s 
2025-01-07 21:09:28.040776:  
2025-01-07 21:09:28.040776: Epoch 53 
2025-01-07 21:09:28.046796: Current learning rate: 0.00507 
2025-01-07 21:10:11.006824: train_loss -0.5235 
2025-01-07 21:10:11.007334: val_loss -0.5367 
2025-01-07 21:10:11.013992: Pseudo dice [np.float32(0.6391), np.float32(0.794)] 
2025-01-07 21:10:11.019003: Epoch time: 42.97 s 
2025-01-07 21:10:11.557697:  
2025-01-07 21:10:11.557697: Epoch 54 
2025-01-07 21:10:11.562716: Current learning rate: 0.00497 
2025-01-07 21:10:54.532687: train_loss -0.5153 
2025-01-07 21:10:54.533691: val_loss -0.4852 
2025-01-07 21:10:54.538737: Pseudo dice [np.float32(0.6325), np.float32(0.746)] 
2025-01-07 21:10:54.542753: Epoch time: 42.98 s 
2025-01-07 21:10:55.080576:  
2025-01-07 21:10:55.081576: Epoch 55 
2025-01-07 21:10:55.086593: Current learning rate: 0.00487 
2025-01-07 21:11:37.984565: train_loss -0.5097 
2025-01-07 21:11:37.985069: val_loss -0.5308 
2025-01-07 21:11:37.990175: Pseudo dice [np.float32(0.6355), np.float32(0.8339)] 
2025-01-07 21:11:37.993683: Epoch time: 42.9 s 
2025-01-07 21:11:37.996698: Yayy! New best EMA pseudo Dice: 0.696399986743927 
2025-01-07 21:11:38.679014:  
2025-01-07 21:11:38.679519: Epoch 56 
2025-01-07 21:11:38.684529: Current learning rate: 0.00478 
2025-01-07 21:12:21.594596: train_loss -0.5589 
2025-01-07 21:12:21.595600: val_loss -0.5273 
2025-01-07 21:12:21.600610: Pseudo dice [np.float32(0.6035), np.float32(0.827)] 
2025-01-07 21:12:21.604622: Epoch time: 42.92 s 
2025-01-07 21:12:21.608132: Yayy! New best EMA pseudo Dice: 0.6983000040054321 
2025-01-07 21:12:22.323132:  
2025-01-07 21:12:22.324138: Epoch 57 
2025-01-07 21:12:22.328678: Current learning rate: 0.00468 
2025-01-07 21:13:05.300671: train_loss -0.5258 
2025-01-07 21:13:05.300671: val_loss -0.5228 
2025-01-07 21:13:05.307297: Pseudo dice [np.float32(0.6276), np.float32(0.8238)] 
2025-01-07 21:13:05.310354: Epoch time: 42.98 s 
2025-01-07 21:13:05.313926: Yayy! New best EMA pseudo Dice: 0.7009999752044678 
2025-01-07 21:13:06.038059:  
2025-01-07 21:13:06.039062: Epoch 58 
2025-01-07 21:13:06.044623: Current learning rate: 0.00458 
2025-01-07 21:13:49.062947: train_loss -0.544 
2025-01-07 21:13:49.062947: val_loss -0.4497 
2025-01-07 21:13:49.070506: Pseudo dice [np.float32(0.6189), np.float32(0.6642)] 
2025-01-07 21:13:49.073196: Epoch time: 43.02 s 
2025-01-07 21:13:49.617951:  
2025-01-07 21:13:49.617951: Epoch 59 
2025-01-07 21:13:49.623467: Current learning rate: 0.00448 
2025-01-07 21:14:38.888996: train_loss -0.5346 
2025-01-07 21:14:38.889498: val_loss -0.4547 
2025-01-07 21:14:38.894513: Pseudo dice [np.float32(0.6406), np.float32(0.6465)] 
2025-01-07 21:14:38.898023: Epoch time: 49.27 s 
2025-01-07 21:14:39.456505:  
2025-01-07 21:14:39.457509: Epoch 60 
2025-01-07 21:14:39.462046: Current learning rate: 0.00438 
2025-01-07 21:15:22.670278: train_loss -0.5338 
2025-01-07 21:15:22.671278: val_loss -0.4566 
2025-01-07 21:15:22.676793: Pseudo dice [np.float32(0.6242), np.float32(0.7698)] 
2025-01-07 21:15:22.681805: Epoch time: 43.21 s 
2025-01-07 21:15:23.388950:  
2025-01-07 21:15:23.388950: Epoch 61 
2025-01-07 21:15:23.393971: Current learning rate: 0.00429 
2025-01-07 21:16:06.588465: train_loss -0.5382 
2025-01-07 21:16:06.588465: val_loss -0.505 
2025-01-07 21:16:06.594482: Pseudo dice [np.float32(0.6489), np.float32(0.7683)] 
2025-01-07 21:16:06.597497: Epoch time: 43.2 s 
2025-01-07 21:16:07.159273:  
2025-01-07 21:16:07.160277: Epoch 62 
2025-01-07 21:16:07.164849: Current learning rate: 0.00419 
2025-01-07 21:16:50.393611: train_loss -0.5535 
2025-01-07 21:16:50.394113: val_loss -0.4209 
2025-01-07 21:16:50.399128: Pseudo dice [np.float32(0.622), np.float32(0.6268)] 
2025-01-07 21:16:50.402179: Epoch time: 43.23 s 
2025-01-07 21:16:50.961629:  
2025-01-07 21:16:50.962132: Epoch 63 
2025-01-07 21:16:50.966646: Current learning rate: 0.00409 
2025-01-07 21:17:34.124022: train_loss -0.5346 
2025-01-07 21:17:34.125022: val_loss -0.5339 
2025-01-07 21:17:34.130540: Pseudo dice [np.float32(0.6558), np.float32(0.7495)] 
2025-01-07 21:17:34.133045: Epoch time: 43.16 s 
2025-01-07 21:17:34.693363:  
2025-01-07 21:17:34.694367: Epoch 64 
2025-01-07 21:17:34.699174: Current learning rate: 0.00399 
2025-01-07 21:18:17.866996: train_loss -0.529 
2025-01-07 21:18:17.868491: val_loss -0.5503 
2025-01-07 21:18:17.874506: Pseudo dice [np.float32(0.6342), np.float32(0.7942)] 
2025-01-07 21:18:17.879516: Epoch time: 43.17 s 
2025-01-07 21:18:18.424090:  
2025-01-07 21:18:18.425089: Epoch 65 
2025-01-07 21:18:18.430601: Current learning rate: 0.00389 
2025-01-07 21:19:01.927299: train_loss -0.5427 
2025-01-07 21:19:01.927813: val_loss -0.5182 
2025-01-07 21:19:01.933582: Pseudo dice [np.float32(0.6242), np.float32(0.8287)] 
2025-01-07 21:19:01.937657: Epoch time: 43.5 s 
2025-01-07 21:19:02.499306:  
2025-01-07 21:19:02.499306: Epoch 66 
2025-01-07 21:19:02.505336: Current learning rate: 0.00379 
2025-01-07 21:19:46.031278: train_loss -0.5504 
2025-01-07 21:19:46.031797: val_loss -0.5492 
2025-01-07 21:19:46.036968: Pseudo dice [np.float32(0.6585), np.float32(0.829)] 
2025-01-07 21:19:46.042089: Epoch time: 43.53 s 
2025-01-07 21:19:46.604291:  
2025-01-07 21:19:46.604291: Epoch 67 
2025-01-07 21:19:46.609310: Current learning rate: 0.00369 
2025-01-07 21:20:29.770595: train_loss -0.5462 
2025-01-07 21:20:29.771599: val_loss -0.4665 
2025-01-07 21:20:29.778113: Pseudo dice [np.float32(0.6405), np.float32(0.7571)] 
2025-01-07 21:20:29.781628: Epoch time: 43.17 s 
2025-01-07 21:20:30.359668:  
2025-01-07 21:20:30.359668: Epoch 68 
2025-01-07 21:20:30.365242: Current learning rate: 0.00359 
2025-01-07 21:21:13.582672: train_loss -0.5328 
2025-01-07 21:21:13.583673: val_loss -0.4934 
2025-01-07 21:21:13.589195: Pseudo dice [np.float32(0.6128), np.float32(0.7619)] 
2025-01-07 21:21:13.592705: Epoch time: 43.22 s 
2025-01-07 21:21:14.322736:  
2025-01-07 21:21:14.322736: Epoch 69 
2025-01-07 21:21:14.327250: Current learning rate: 0.00349 
2025-01-07 21:21:57.487149: train_loss -0.5134 
2025-01-07 21:21:57.487149: val_loss -0.4923 
2025-01-07 21:21:57.493167: Pseudo dice [np.float32(0.6468), np.float32(0.7576)] 
2025-01-07 21:21:57.497181: Epoch time: 43.16 s 
2025-01-07 21:21:58.061953:  
2025-01-07 21:21:58.062954: Epoch 70 
2025-01-07 21:21:58.068009: Current learning rate: 0.00338 
2025-01-07 21:22:41.198280: train_loss -0.53 
2025-01-07 21:22:41.198280: val_loss -0.5025 
2025-01-07 21:22:41.204797: Pseudo dice [np.float32(0.6468), np.float32(0.641)] 
2025-01-07 21:22:41.207303: Epoch time: 43.14 s 
2025-01-07 21:22:41.776560:  
2025-01-07 21:22:41.777566: Epoch 71 
2025-01-07 21:22:41.782100: Current learning rate: 0.00328 
2025-01-07 21:23:24.907028: train_loss -0.5618 
2025-01-07 21:23:24.907530: val_loss -0.5326 
2025-01-07 21:23:24.913131: Pseudo dice [np.float32(0.6264), np.float32(0.8477)] 
2025-01-07 21:23:24.916180: Epoch time: 43.13 s 
2025-01-07 21:23:25.489053:  
2025-01-07 21:23:25.489564: Epoch 72 
2025-01-07 21:23:25.495123: Current learning rate: 0.00318 
2025-01-07 21:24:08.696804: train_loss -0.5677 
2025-01-07 21:24:08.696804: val_loss -0.5435 
2025-01-07 21:24:08.703818: Pseudo dice [np.float32(0.6328), np.float32(0.8044)] 
2025-01-07 21:24:08.709333: Epoch time: 43.21 s 
2025-01-07 21:24:09.267019:  
2025-01-07 21:24:09.267019: Epoch 73 
2025-01-07 21:24:09.273114: Current learning rate: 0.00308 
2025-01-07 21:24:52.214298: train_loss -0.5443 
2025-01-07 21:24:52.214298: val_loss -0.4867 
2025-01-07 21:24:52.220900: Pseudo dice [np.float32(0.6173), np.float32(0.6424)] 
2025-01-07 21:24:52.224502: Epoch time: 42.95 s 
2025-01-07 21:24:52.797312:  
2025-01-07 21:24:52.797312: Epoch 74 
2025-01-07 21:24:52.802826: Current learning rate: 0.00297 
2025-01-07 21:25:35.839232: train_loss -0.5574 
2025-01-07 21:25:35.840738: val_loss -0.4948 
2025-01-07 21:25:35.845943: Pseudo dice [np.float32(0.6473), np.float32(0.7419)] 
2025-01-07 21:25:35.849500: Epoch time: 43.04 s 
2025-01-07 21:25:36.413166:  
2025-01-07 21:25:36.413166: Epoch 75 
2025-01-07 21:25:36.418683: Current learning rate: 0.00287 
2025-01-07 21:26:19.386844: train_loss -0.5879 
2025-01-07 21:26:19.387849: val_loss -0.5361 
2025-01-07 21:26:19.394366: Pseudo dice [np.float32(0.6614), np.float32(0.7764)] 
2025-01-07 21:26:19.399382: Epoch time: 42.97 s 
2025-01-07 21:26:20.164545:  
2025-01-07 21:26:20.164545: Epoch 76 
2025-01-07 21:26:20.170620: Current learning rate: 0.00277 
2025-01-07 21:27:03.141843: train_loss -0.5652 
2025-01-07 21:27:03.141843: val_loss -0.5368 
2025-01-07 21:27:03.148364: Pseudo dice [np.float32(0.6358), np.float32(0.8467)] 
2025-01-07 21:27:03.152873: Epoch time: 42.98 s 
2025-01-07 21:27:03.726663:  
2025-01-07 21:27:03.727664: Epoch 77 
2025-01-07 21:27:03.732754: Current learning rate: 0.00266 
2025-01-07 21:27:46.705021: train_loss -0.5611 
2025-01-07 21:27:46.705523: val_loss -0.5226 
2025-01-07 21:27:46.711539: Pseudo dice [np.float32(0.6477), np.float32(0.8109)] 
2025-01-07 21:27:46.715553: Epoch time: 42.98 s 
2025-01-07 21:27:46.719068: Yayy! New best EMA pseudo Dice: 0.7027000188827515 
2025-01-07 21:27:47.445911:  
2025-01-07 21:27:47.446916: Epoch 78 
2025-01-07 21:27:47.451501: Current learning rate: 0.00256 
2025-01-07 21:28:30.463522: train_loss -0.5747 
2025-01-07 21:28:30.464522: val_loss -0.5165 
2025-01-07 21:28:30.471051: Pseudo dice [np.float32(0.6281), np.float32(0.8041)] 
2025-01-07 21:28:30.475060: Epoch time: 43.02 s 
2025-01-07 21:28:30.480181: Yayy! New best EMA pseudo Dice: 0.7039999961853027 
2025-01-07 21:28:31.233801:  
2025-01-07 21:28:31.233801: Epoch 79 
2025-01-07 21:28:31.239875: Current learning rate: 0.00245 
2025-01-07 21:29:14.220443: train_loss -0.5741 
2025-01-07 21:29:14.220958: val_loss -0.4814 
2025-01-07 21:29:14.226585: Pseudo dice [np.float32(0.6443), np.float32(0.689)] 
2025-01-07 21:29:14.229685: Epoch time: 42.99 s 
2025-01-07 21:29:14.805107:  
2025-01-07 21:29:14.805107: Epoch 80 
2025-01-07 21:29:14.810639: Current learning rate: 0.00235 
2025-01-07 21:30:00.416796: train_loss -0.5915 
2025-01-07 21:30:00.417301: val_loss -0.5839 
2025-01-07 21:30:00.422317: Pseudo dice [np.float32(0.666), np.float32(0.8511)] 
2025-01-07 21:30:00.427329: Epoch time: 45.61 s 
2025-01-07 21:30:00.430837: Yayy! New best EMA pseudo Dice: 0.7060999870300293 
2025-01-07 21:30:01.153134:  
2025-01-07 21:30:01.153134: Epoch 81 
2025-01-07 21:30:01.159223: Current learning rate: 0.00224 
2025-01-07 21:30:44.108378: train_loss -0.5892 
2025-01-07 21:30:44.108378: val_loss -0.516 
2025-01-07 21:30:44.113389: Pseudo dice [np.float32(0.6798), np.float32(0.7976)] 
2025-01-07 21:30:44.116906: Epoch time: 42.96 s 
2025-01-07 21:30:44.119916: Yayy! New best EMA pseudo Dice: 0.7093999981880188 
2025-01-07 21:30:44.871264:  
2025-01-07 21:30:44.871264: Epoch 82 
2025-01-07 21:30:44.876858: Current learning rate: 0.00214 
2025-01-07 21:31:27.877296: train_loss -0.5645 
2025-01-07 21:31:27.877296: val_loss -0.5392 
2025-01-07 21:31:27.882308: Pseudo dice [np.float32(0.6328), np.float32(0.8371)] 
2025-01-07 21:31:27.885816: Epoch time: 43.01 s 
2025-01-07 21:31:27.888327: Yayy! New best EMA pseudo Dice: 0.711899995803833 
2025-01-07 21:31:28.602975:  
2025-01-07 21:31:28.603976: Epoch 83 
2025-01-07 21:31:28.609553: Current learning rate: 0.00203 
2025-01-07 21:32:11.597469: train_loss -0.5551 
2025-01-07 21:32:11.598475: val_loss -0.561 
2025-01-07 21:32:11.604169: Pseudo dice [np.float32(0.6398), np.float32(0.8259)] 
2025-01-07 21:32:11.608177: Epoch time: 42.99 s 
2025-01-07 21:32:11.610683: Yayy! New best EMA pseudo Dice: 0.7139999866485596 
2025-01-07 21:32:12.535196:  
2025-01-07 21:32:12.535196: Epoch 84 
2025-01-07 21:32:12.540756: Current learning rate: 0.00192 
2025-01-07 21:32:55.574672: train_loss -0.5878 
2025-01-07 21:32:55.574672: val_loss -0.5646 
2025-01-07 21:32:55.579687: Pseudo dice [np.float32(0.6449), np.float32(0.8679)] 
2025-01-07 21:32:55.583198: Epoch time: 43.04 s 
2025-01-07 21:32:55.586706: Yayy! New best EMA pseudo Dice: 0.7182999849319458 
2025-01-07 21:32:56.316597:  
2025-01-07 21:32:56.317603: Epoch 85 
2025-01-07 21:32:56.322166: Current learning rate: 0.00181 
2025-01-07 21:33:39.309519: train_loss -0.583 
2025-01-07 21:33:39.309519: val_loss -0.5777 
2025-01-07 21:33:39.315533: Pseudo dice [np.float32(0.6699), np.float32(0.7819)] 
2025-01-07 21:33:39.320547: Epoch time: 42.99 s 
2025-01-07 21:33:39.324557: Yayy! New best EMA pseudo Dice: 0.718999981880188 
2025-01-07 21:33:40.033006:  
2025-01-07 21:33:40.033006: Epoch 86 
2025-01-07 21:33:40.037555: Current learning rate: 0.0017 
2025-01-07 21:34:23.011138: train_loss -0.5796 
2025-01-07 21:34:23.012143: val_loss -0.5532 
2025-01-07 21:34:23.018157: Pseudo dice [np.float32(0.6541), np.float32(0.7844)] 
2025-01-07 21:34:23.022170: Epoch time: 42.98 s 
2025-01-07 21:34:23.026179: Yayy! New best EMA pseudo Dice: 0.7190999984741211 
2025-01-07 21:34:23.729661:  
2025-01-07 21:34:23.729661: Epoch 87 
2025-01-07 21:34:23.736250: Current learning rate: 0.00159 
2025-01-07 21:35:06.669379: train_loss -0.5774 
2025-01-07 21:35:06.670380: val_loss -0.5696 
2025-01-07 21:35:06.675894: Pseudo dice [np.float32(0.6531), np.float32(0.8533)] 
2025-01-07 21:35:06.678908: Epoch time: 42.94 s 
2025-01-07 21:35:06.681952: Yayy! New best EMA pseudo Dice: 0.7225000262260437 
2025-01-07 21:35:07.381355:  
2025-01-07 21:35:07.381859: Epoch 88 
2025-01-07 21:35:07.387876: Current learning rate: 0.00148 
2025-01-07 21:35:50.324306: train_loss -0.5832 
2025-01-07 21:35:50.324306: val_loss -0.4869 
2025-01-07 21:35:50.329860: Pseudo dice [np.float32(0.652), np.float32(0.6818)] 
2025-01-07 21:35:50.332892: Epoch time: 42.94 s 
2025-01-07 21:35:50.869230:  
2025-01-07 21:35:50.870231: Epoch 89 
2025-01-07 21:35:50.875744: Current learning rate: 0.00137 
2025-01-07 21:36:33.834912: train_loss -0.5851 
2025-01-07 21:36:33.835413: val_loss -0.5433 
2025-01-07 21:36:33.841518: Pseudo dice [np.float32(0.6187), np.float32(0.7947)] 
2025-01-07 21:36:33.844059: Epoch time: 42.97 s 
2025-01-07 21:36:34.370037:  
2025-01-07 21:36:34.370037: Epoch 90 
2025-01-07 21:36:34.376056: Current learning rate: 0.00126 
2025-01-07 21:37:17.351126: train_loss -0.5946 
2025-01-07 21:37:17.351629: val_loss -0.5473 
2025-01-07 21:37:17.358150: Pseudo dice [np.float32(0.6333), np.float32(0.7963)] 
2025-01-07 21:37:17.361737: Epoch time: 42.98 s 
2025-01-07 21:37:17.893578:  
2025-01-07 21:37:17.894578: Epoch 91 
2025-01-07 21:37:17.900162: Current learning rate: 0.00115 
2025-01-07 21:38:00.884645: train_loss -0.5901 
2025-01-07 21:38:00.885649: val_loss -0.6059 
2025-01-07 21:38:00.892203: Pseudo dice [np.float32(0.6603), np.float32(0.8642)] 
2025-01-07 21:38:00.899403: Epoch time: 42.99 s 
2025-01-07 21:38:01.601220:  
2025-01-07 21:38:01.601722: Epoch 92 
2025-01-07 21:38:01.607740: Current learning rate: 0.00103 
2025-01-07 21:38:44.582745: train_loss -0.5929 
2025-01-07 21:38:44.583744: val_loss -0.5476 
2025-01-07 21:38:44.590262: Pseudo dice [np.float32(0.6442), np.float32(0.8229)] 
2025-01-07 21:38:44.596277: Epoch time: 42.98 s 
2025-01-07 21:38:45.123940:  
2025-01-07 21:38:45.123940: Epoch 93 
2025-01-07 21:38:45.129985: Current learning rate: 0.00091 
2025-01-07 21:39:28.107136: train_loss -0.5909 
2025-01-07 21:39:28.107659: val_loss -0.5723 
2025-01-07 21:39:28.112857: Pseudo dice [np.float32(0.6207), np.float32(0.8309)] 
2025-01-07 21:39:28.116368: Epoch time: 42.98 s 
2025-01-07 21:39:28.649680:  
2025-01-07 21:39:28.649680: Epoch 94 
2025-01-07 21:39:28.655699: Current learning rate: 0.00079 
2025-01-07 21:40:11.664291: train_loss -0.5834 
2025-01-07 21:40:11.665291: val_loss -0.5289 
2025-01-07 21:40:11.670808: Pseudo dice [np.float32(0.6537), np.float32(0.8258)] 
2025-01-07 21:40:11.675319: Epoch time: 43.02 s 
2025-01-07 21:40:11.678329: Yayy! New best EMA pseudo Dice: 0.7239000201225281 
2025-01-07 21:40:12.373847:  
2025-01-07 21:40:12.373847: Epoch 95 
2025-01-07 21:40:12.379870: Current learning rate: 0.00067 
2025-01-07 21:40:55.376789: train_loss -0.5943 
2025-01-07 21:40:55.377293: val_loss -0.5012 
2025-01-07 21:40:55.383308: Pseudo dice [np.float32(0.6413), np.float32(0.7788)] 
2025-01-07 21:40:55.387318: Epoch time: 43.0 s 
2025-01-07 21:40:55.928356:  
2025-01-07 21:40:55.928858: Epoch 96 
2025-01-07 21:40:55.933868: Current learning rate: 0.00055 
2025-01-07 21:41:38.880525: train_loss -0.596 
2025-01-07 21:41:38.880525: val_loss -0.5649 
2025-01-07 21:41:38.886552: Pseudo dice [np.float32(0.6565), np.float32(0.8495)] 
2025-01-07 21:41:38.890064: Epoch time: 42.95 s 
2025-01-07 21:41:38.893080: Yayy! New best EMA pseudo Dice: 0.725600004196167 
2025-01-07 21:41:39.597164:  
2025-01-07 21:41:39.597668: Epoch 97 
2025-01-07 21:41:39.602678: Current learning rate: 0.00043 
2025-01-07 21:42:22.569018: train_loss -0.606 
2025-01-07 21:42:22.570019: val_loss -0.5583 
2025-01-07 21:42:22.575537: Pseudo dice [np.float32(0.643), np.float32(0.7948)] 
2025-01-07 21:42:22.579047: Epoch time: 42.97 s 
2025-01-07 21:42:23.134730:  
2025-01-07 21:42:23.134730: Epoch 98 
2025-01-07 21:42:23.140293: Current learning rate: 0.0003 
2025-01-07 21:43:12.019518: train_loss -0.6077 
2025-01-07 21:43:12.019518: val_loss -0.5644 
2025-01-07 21:43:12.026536: Pseudo dice [np.float32(0.678), np.float32(0.8705)] 
2025-01-07 21:43:12.030547: Epoch time: 48.88 s 
2025-01-07 21:43:12.034559: Yayy! New best EMA pseudo Dice: 0.7297999858856201 
2025-01-07 21:43:12.732693:  
2025-01-07 21:43:12.733195: Epoch 99 
2025-01-07 21:43:12.738206: Current learning rate: 0.00016 
2025-01-07 21:43:55.897957: train_loss -0.5967 
2025-01-07 21:43:55.898460: val_loss -0.537 
2025-01-07 21:43:55.905980: Pseudo dice [np.float32(0.6621), np.float32(0.7583)] 
2025-01-07 21:43:55.910992: Epoch time: 43.17 s 
2025-01-07 21:43:56.624939: Training done. 
2025-01-07 21:43:56.657942: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset008_HepaticVessel\splits_final.json 
2025-01-07 21:43:56.665941: The split file contains 5 splits. 
2025-01-07 21:43:56.671942: Desired fold for training: 8 
2025-01-07 21:43:56.677941: INFO: You requested fold 8 for training but splits contain only 5 folds. I am now creating a random (but seeded) 80:20 split! 
2025-01-07 21:43:56.684940: This random 80:20 split has 242 training and 61 validation cases. 
2025-01-07 21:43:56.691941: predicting hepaticvessel_008 
2025-01-07 21:43:56.698941: hepaticvessel_008, shape torch.Size([1, 235, 432, 432]), rank 0 
2025-01-07 21:44:16.318797: predicting hepaticvessel_010 
2025-01-07 21:44:16.340798: hepaticvessel_010, shape torch.Size([1, 196, 441, 441]), rank 0 
2025-01-07 21:44:36.724395: predicting hepaticvessel_018 
2025-01-07 21:44:36.742395: hepaticvessel_018, shape torch.Size([1, 180, 364, 364]), rank 0 
2025-01-07 21:44:46.175654: predicting hepaticvessel_025 
2025-01-07 21:44:46.189655: hepaticvessel_025, shape torch.Size([1, 295, 430, 430]), rank 0 
2025-01-07 21:45:16.134889: predicting hepaticvessel_027 
2025-01-07 21:45:16.157890: hepaticvessel_027, shape torch.Size([1, 245, 416, 416]), rank 0 
2025-01-07 21:45:40.015257: predicting hepaticvessel_030 
2025-01-07 21:45:40.030257: hepaticvessel_030, shape torch.Size([1, 195, 486, 486]), rank 0 
2025-01-07 21:46:07.454819: predicting hepaticvessel_033 
2025-01-07 21:46:07.471819: hepaticvessel_033, shape torch.Size([1, 207, 445, 445]), rank 0 
2025-01-07 21:46:26.419732: predicting hepaticvessel_039 
2025-01-07 21:46:26.434732: hepaticvessel_039, shape torch.Size([1, 225, 438, 438]), rank 0 
2025-01-07 21:46:46.610901: predicting hepaticvessel_042 
2025-01-07 21:46:46.631903: hepaticvessel_042, shape torch.Size([1, 225, 490, 490]), rank 0 
2025-01-07 21:47:15.490131: predicting hepaticvessel_046 
2025-01-07 21:47:15.514131: hepaticvessel_046, shape torch.Size([1, 215, 356, 356]), rank 0 
2025-01-07 21:47:28.755951: predicting hepaticvessel_058 
2025-01-07 21:47:28.766951: hepaticvessel_058, shape torch.Size([1, 165, 340, 340]), rank 0 
2025-01-07 21:47:38.445024: predicting hepaticvessel_059 
2025-01-07 21:47:38.460025: hepaticvessel_059, shape torch.Size([1, 185, 330, 330]), rank 0 
2025-01-07 21:47:48.331824: predicting hepaticvessel_065 
2025-01-07 21:47:48.344049: hepaticvessel_065, shape torch.Size([1, 195, 400, 400]), rank 0 
2025-01-07 21:48:01.430684: predicting hepaticvessel_084 
2025-01-07 21:48:01.441190: hepaticvessel_084, shape torch.Size([1, 225, 420, 420]), rank 0 
2025-01-07 21:48:21.275673: predicting hepaticvessel_087 
2025-01-07 21:48:21.289674: hepaticvessel_087, shape torch.Size([1, 195, 431, 431]), rank 0 
2025-01-07 21:48:41.039999: predicting hepaticvessel_096 
2025-01-07 21:48:41.059509: hepaticvessel_096, shape torch.Size([1, 231, 411, 411]), rank 0 
2025-01-07 21:49:00.513874: predicting hepaticvessel_100 
2025-01-07 21:49:00.532874: hepaticvessel_100, shape torch.Size([1, 230, 380, 380]), rank 0 
2025-01-07 21:49:12.860145: predicting hepaticvessel_110 
2025-01-07 21:49:12.876654: hepaticvessel_110, shape torch.Size([1, 245, 493, 493]), rank 0 
2025-01-07 21:49:48.197768: predicting hepaticvessel_111 
2025-01-07 21:49:48.228771: hepaticvessel_111, shape torch.Size([1, 160, 398, 398]), rank 0 
2025-01-07 21:49:57.755150: predicting hepaticvessel_132 
2025-01-07 21:49:57.769149: hepaticvessel_132, shape torch.Size([1, 275, 409, 409]), rank 0 
2025-01-07 21:50:21.752889: predicting hepaticvessel_142 
2025-01-07 21:50:21.768901: hepaticvessel_142, shape torch.Size([1, 250, 422, 422]), rank 0 
2025-01-07 21:50:46.536072: predicting hepaticvessel_145 
2025-01-07 21:50:46.553076: hepaticvessel_145, shape torch.Size([1, 240, 442, 442]), rank 0 
2025-01-07 21:51:06.112470: predicting hepaticvessel_150 
2025-01-07 21:51:06.138471: hepaticvessel_150, shape torch.Size([1, 442, 420, 420]), rank 0 
2025-01-07 21:51:50.364785: predicting hepaticvessel_160 
2025-01-07 21:51:50.395788: hepaticvessel_160, shape torch.Size([1, 195, 388, 388]), rank 0 
2025-01-07 21:52:02.744792: predicting hepaticvessel_164 
2025-01-07 21:52:02.759792: hepaticvessel_164, shape torch.Size([1, 202, 368, 368]), rank 0 
2025-01-07 21:52:15.099246: predicting hepaticvessel_174 
2025-01-07 21:52:15.113253: hepaticvessel_174, shape torch.Size([1, 160, 480, 480]), rank 0 
2025-01-07 21:52:29.515129: predicting hepaticvessel_185 
2025-01-07 21:52:29.532637: hepaticvessel_185, shape torch.Size([1, 230, 500, 500]), rank 0 
2025-01-07 21:52:57.504773: predicting hepaticvessel_207 
2025-01-07 21:52:57.530773: hepaticvessel_207, shape torch.Size([1, 205, 291, 291]), rank 0 
2025-01-07 21:53:04.602485: predicting hepaticvessel_209 
2025-01-07 21:53:04.611487: hepaticvessel_209, shape torch.Size([1, 220, 360, 360]), rank 0 
2025-01-07 21:53:17.015843: predicting hepaticvessel_214 
2025-01-07 21:53:17.029844: hepaticvessel_214, shape torch.Size([1, 248, 440, 440]), rank 0 
2025-01-07 21:53:41.158327: predicting hepaticvessel_218 
2025-01-07 21:53:41.182333: hepaticvessel_218, shape torch.Size([1, 250, 375, 375]), rank 0 
2025-01-07 21:53:56.608787: predicting hepaticvessel_222 
2025-01-07 21:53:56.625787: hepaticvessel_222, shape torch.Size([1, 245, 500, 500]), rank 0 
2025-01-07 21:54:31.193700: predicting hepaticvessel_224 
2025-01-07 21:54:31.219703: hepaticvessel_224, shape torch.Size([1, 225, 389, 389]), rank 0 
2025-01-07 21:54:43.495853: predicting hepaticvessel_230 
2025-01-07 21:54:43.524095: hepaticvessel_230, shape torch.Size([1, 225, 427, 427]), rank 0 
2025-01-07 21:55:02.528829: predicting hepaticvessel_237 
2025-01-07 21:55:02.546340: hepaticvessel_237, shape torch.Size([1, 225, 455, 455]), rank 0 
2025-01-07 21:55:21.580573: predicting hepaticvessel_246 
2025-01-07 21:55:21.601573: hepaticvessel_246, shape torch.Size([1, 168, 340, 340]), rank 0 
2025-01-07 21:55:30.701342: predicting hepaticvessel_255 
2025-01-07 21:55:30.711342: hepaticvessel_255, shape torch.Size([1, 196, 385, 385]), rank 0 
2025-01-07 21:55:42.801568: predicting hepaticvessel_256 
2025-01-07 21:55:42.815567: hepaticvessel_256, shape torch.Size([1, 219, 420, 420]), rank 0 
2025-01-07 21:56:02.184668: predicting hepaticvessel_266 
2025-01-07 21:56:02.201176: hepaticvessel_266, shape torch.Size([1, 205, 408, 408]), rank 0 
2025-01-07 21:56:21.927907: predicting hepaticvessel_268 
2025-01-07 21:56:21.942904: hepaticvessel_268, shape torch.Size([1, 255, 412, 412]), rank 0 
2025-01-07 21:56:47.414620: predicting hepaticvessel_290 
2025-01-07 21:56:47.430127: hepaticvessel_290, shape torch.Size([1, 225, 419, 419]), rank 0 
2025-01-07 21:57:07.457250: predicting hepaticvessel_291 
2025-01-07 21:57:07.472251: hepaticvessel_291, shape torch.Size([1, 214, 402, 402]), rank 0 
2025-01-07 21:57:27.670186: predicting hepaticvessel_300 
2025-01-07 21:57:27.682186: hepaticvessel_300, shape torch.Size([1, 190, 340, 340]), rank 0 
2025-01-07 21:57:37.628991: predicting hepaticvessel_309 
2025-01-07 21:57:37.637991: hepaticvessel_309, shape torch.Size([1, 225, 463, 463]), rank 0 
2025-01-07 21:57:57.463121: predicting hepaticvessel_314 
2025-01-07 21:57:57.480229: hepaticvessel_314, shape torch.Size([1, 290, 480, 480]), rank 0 
2025-01-07 21:58:25.826087: predicting hepaticvessel_325 
2025-01-07 21:58:25.863088: hepaticvessel_325, shape torch.Size([1, 205, 431, 431]), rank 0 
2025-01-07 21:58:44.754161: predicting hepaticvessel_330 
2025-01-07 21:58:44.769162: hepaticvessel_330, shape torch.Size([1, 261, 380, 380]), rank 0 
2025-01-07 21:58:59.962290: predicting hepaticvessel_345 
2025-01-07 21:58:59.978289: hepaticvessel_345, shape torch.Size([1, 232, 454, 454]), rank 0 
2025-01-07 21:59:19.563966: predicting hepaticvessel_348 
2025-01-07 21:59:19.587965: hepaticvessel_348, shape torch.Size([1, 204, 319, 319]), rank 0 
2025-01-07 21:59:26.455800: predicting hepaticvessel_371 
2025-01-07 21:59:26.469800: hepaticvessel_371, shape torch.Size([1, 215, 360, 360]), rank 0 
2025-01-07 21:59:38.592966: predicting hepaticvessel_375 
2025-01-07 21:59:38.607966: hepaticvessel_375, shape torch.Size([1, 267, 403, 403]), rank 0 
2025-01-07 22:00:02.222414: predicting hepaticvessel_378 
2025-01-07 22:00:02.242416: hepaticvessel_378, shape torch.Size([1, 190, 388, 388]), rank 0 
2025-01-07 22:00:11.384553: predicting hepaticvessel_382 
2025-01-07 22:00:11.399552: hepaticvessel_382, shape torch.Size([1, 250, 430, 430]), rank 0 
2025-01-07 22:00:34.962789: predicting hepaticvessel_406 
2025-01-07 22:00:34.983250: hepaticvessel_406, shape torch.Size([1, 248, 432, 432]), rank 0 
2025-01-07 22:00:58.599255: predicting hepaticvessel_412 
2025-01-07 22:00:58.622254: hepaticvessel_412, shape torch.Size([1, 185, 374, 374]), rank 0 
2025-01-07 22:01:07.748536: predicting hepaticvessel_424 
2025-01-07 22:01:07.764536: hepaticvessel_424, shape torch.Size([1, 201, 375, 375]), rank 0 
2025-01-07 22:01:19.855121: predicting hepaticvessel_425 
2025-01-07 22:01:19.869121: hepaticvessel_425, shape torch.Size([1, 255, 360, 360]), rank 0 
2025-01-07 22:01:34.969180: predicting hepaticvessel_431 
2025-01-07 22:01:34.986180: hepaticvessel_431, shape torch.Size([1, 240, 409, 409]), rank 0 
2025-01-07 22:01:53.828245: predicting hepaticvessel_437 
2025-01-07 22:01:53.842753: hepaticvessel_437, shape torch.Size([1, 207, 497, 497]), rank 0 
2025-01-07 22:02:21.020392: predicting hepaticvessel_438 
2025-01-07 22:02:21.045394: hepaticvessel_438, shape torch.Size([1, 260, 393, 393]), rank 0 
2025-01-07 22:02:36.171181: predicting hepaticvessel_456 
2025-01-07 22:02:36.188181: hepaticvessel_456, shape torch.Size([1, 570, 367, 367]), rank 0 
2025-01-07 22:03:26.854714: Validation complete 
2025-01-07 22:03:26.854714: Mean Validation Dice:  0.6575617314868145 
