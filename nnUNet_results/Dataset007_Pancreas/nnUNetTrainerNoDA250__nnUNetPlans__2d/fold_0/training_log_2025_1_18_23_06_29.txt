
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-18 23:06:30.487100: do_dummy_2d_data_aug: False 
2025-01-18 23:06:30.490100: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset007_Pancreas\splits_final.json 
2025-01-18 23:06:30.494580: The split file contains 5 splits. 
2025-01-18 23:06:30.497584: Desired fold for training: 0 
2025-01-18 23:06:30.499582: This split has 224 training and 57 validation cases. 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.8027340173721313, 0.8027340173721313], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset007_Pancreas', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.8027340173721313, 0.8027340173721313], 'original_median_shape_after_transp': [93, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 80.36482238769531, 'median': 86.0, 'min': -998.0, 'percentile_00_5': -92.0, 'percentile_99_5': 217.0, 'std': 72.70781707763672}}} 
 
2025-01-18 23:06:35.836114: unpacking dataset... 
2025-01-18 23:06:36.055448: unpacking done... 
2025-01-18 23:06:40.081439:  
2025-01-18 23:06:40.082444: Epoch 100 
2025-01-18 23:06:40.087462: Current learning rate: 0.00631 
2025-01-18 23:07:15.470780: train_loss -0.8457 
2025-01-18 23:07:15.471284: val_loss -0.4903 
2025-01-18 23:07:15.478813: Pseudo dice [np.float32(0.7102), np.float32(0.3928)] 
2025-01-18 23:07:15.481867: Epoch time: 35.39 s 
2025-01-18 23:07:16.055753:  
2025-01-18 23:07:16.056256: Epoch 101 
2025-01-18 23:07:16.061270: Current learning rate: 0.00628 
2025-01-18 23:07:48.260298: train_loss -0.8485 
2025-01-18 23:07:48.261301: val_loss -0.501 
2025-01-18 23:07:48.266884: Pseudo dice [np.float32(0.7263), np.float32(0.3419)] 
2025-01-18 23:07:48.269391: Epoch time: 32.21 s 
2025-01-18 23:07:48.827765:  
2025-01-18 23:07:48.828768: Epoch 102 
2025-01-18 23:07:48.833833: Current learning rate: 0.00624 
2025-01-18 23:08:20.911503: train_loss -0.8547 
2025-01-18 23:08:20.912522: val_loss -0.5023 
2025-01-18 23:08:20.916654: Pseudo dice [np.float32(0.7231), np.float32(0.4352)] 
2025-01-18 23:08:20.919695: Epoch time: 32.08 s 
2025-01-18 23:08:21.480685:  
2025-01-18 23:08:21.481688: Epoch 103 
2025-01-18 23:08:21.486251: Current learning rate: 0.0062 
2025-01-18 23:08:53.562690: train_loss -0.8447 
2025-01-18 23:08:53.563193: val_loss -0.4529 
2025-01-18 23:08:53.569210: Pseudo dice [np.float32(0.724), np.float32(0.3319)] 
2025-01-18 23:08:53.572240: Epoch time: 32.08 s 
2025-01-18 23:08:54.244437:  
2025-01-18 23:08:54.244437: Epoch 104 
2025-01-18 23:08:54.249470: Current learning rate: 0.00616 
2025-01-18 23:09:26.339950: train_loss -0.8545 
2025-01-18 23:09:26.340955: val_loss -0.5019 
2025-01-18 23:09:26.345969: Pseudo dice [np.float32(0.7302), np.float32(0.4389)] 
2025-01-18 23:09:26.349480: Epoch time: 32.1 s 
2025-01-18 23:09:26.906460:  
2025-01-18 23:09:26.906460: Epoch 105 
2025-01-18 23:09:26.912015: Current learning rate: 0.00612 
2025-01-18 23:09:58.991362: train_loss -0.8589 
2025-01-18 23:09:58.992867: val_loss -0.4324 
2025-01-18 23:09:58.998410: Pseudo dice [np.float32(0.7297), np.float32(0.2605)] 
2025-01-18 23:09:59.000918: Epoch time: 32.09 s 
2025-01-18 23:09:59.561210:  
2025-01-18 23:09:59.561210: Epoch 106 
2025-01-18 23:09:59.566241: Current learning rate: 0.00609 
2025-01-18 23:10:31.633923: train_loss -0.8644 
2025-01-18 23:10:31.634922: val_loss -0.5164 
2025-01-18 23:10:31.640442: Pseudo dice [np.float32(0.732), np.float32(0.4096)] 
2025-01-18 23:10:31.642949: Epoch time: 32.07 s 
2025-01-18 23:10:32.212880:  
2025-01-18 23:10:32.213879: Epoch 107 
2025-01-18 23:10:32.217432: Current learning rate: 0.00605 
2025-01-18 23:11:04.285516: train_loss -0.8551 
2025-01-18 23:11:04.285516: val_loss -0.4762 
2025-01-18 23:11:04.291531: Pseudo dice [np.float32(0.7127), np.float32(0.3346)] 
2025-01-18 23:11:04.294542: Epoch time: 32.07 s 
2025-01-18 23:11:04.860641:  
2025-01-18 23:11:04.860641: Epoch 108 
2025-01-18 23:11:04.866750: Current learning rate: 0.00601 
2025-01-18 23:11:36.944976: train_loss -0.8647 
2025-01-18 23:11:36.945478: val_loss -0.4983 
2025-01-18 23:11:36.950489: Pseudo dice [np.float32(0.7031), np.float32(0.3788)] 
2025-01-18 23:11:36.954537: Epoch time: 32.09 s 
2025-01-18 23:11:37.515123:  
2025-01-18 23:11:37.516123: Epoch 109 
2025-01-18 23:11:37.521762: Current learning rate: 0.00597 
2025-01-18 23:12:09.623251: train_loss -0.8581 
2025-01-18 23:12:09.623755: val_loss -0.5442 
2025-01-18 23:12:09.629779: Pseudo dice [np.float32(0.7181), np.float32(0.4843)] 
2025-01-18 23:12:09.632797: Epoch time: 32.11 s 
2025-01-18 23:12:10.203930:  
2025-01-18 23:12:10.203930: Epoch 110 
2025-01-18 23:12:10.209522: Current learning rate: 0.00593 
2025-01-18 23:12:42.290382: train_loss -0.8596 
2025-01-18 23:12:42.290884: val_loss -0.5469 
2025-01-18 23:12:42.295897: Pseudo dice [np.float32(0.7065), np.float32(0.4914)] 
2025-01-18 23:12:42.298401: Epoch time: 32.09 s 
2025-01-18 23:12:42.862171:  
2025-01-18 23:12:42.863174: Epoch 111 
2025-01-18 23:12:42.868226: Current learning rate: 0.0059 
2025-01-18 23:13:15.203758: train_loss -0.8548 
2025-01-18 23:13:15.204265: val_loss -0.5092 
2025-01-18 23:13:15.209896: Pseudo dice [np.float32(0.7222), np.float32(0.4078)] 
2025-01-18 23:13:15.212947: Epoch time: 32.34 s 
2025-01-18 23:13:15.928241:  
2025-01-18 23:13:15.928241: Epoch 112 
2025-01-18 23:13:15.933288: Current learning rate: 0.00586 
2025-01-18 23:13:48.038941: train_loss -0.8527 
2025-01-18 23:13:48.039460: val_loss -0.5076 
2025-01-18 23:13:48.045625: Pseudo dice [np.float32(0.7317), np.float32(0.417)] 
2025-01-18 23:13:48.048747: Epoch time: 32.11 s 
2025-01-18 23:13:48.603698:  
2025-01-18 23:13:48.604704: Epoch 113 
2025-01-18 23:13:48.609732: Current learning rate: 0.00582 
2025-01-18 23:14:20.704723: train_loss -0.8455 
2025-01-18 23:14:20.704723: val_loss -0.5175 
2025-01-18 23:14:20.710741: Pseudo dice [np.float32(0.7052), np.float32(0.406)] 
2025-01-18 23:14:20.714251: Epoch time: 32.1 s 
2025-01-18 23:14:21.273298:  
2025-01-18 23:14:21.273298: Epoch 114 
2025-01-18 23:14:21.277822: Current learning rate: 0.00578 
2025-01-18 23:14:53.375509: train_loss -0.8566 
2025-01-18 23:14:53.376513: val_loss -0.5213 
2025-01-18 23:14:53.381529: Pseudo dice [np.float32(0.7255), np.float32(0.4041)] 
2025-01-18 23:14:53.385098: Epoch time: 32.1 s 
2025-01-18 23:14:53.943092:  
2025-01-18 23:14:53.944097: Epoch 115 
2025-01-18 23:14:53.949124: Current learning rate: 0.00574 
2025-01-18 23:15:26.051742: train_loss -0.8588 
2025-01-18 23:15:26.052746: val_loss -0.5135 
2025-01-18 23:15:26.058466: Pseudo dice [np.float32(0.7253), np.float32(0.419)] 
2025-01-18 23:15:26.061476: Epoch time: 32.11 s 
2025-01-18 23:15:26.629351:  
2025-01-18 23:15:26.630353: Epoch 116 
2025-01-18 23:15:26.635486: Current learning rate: 0.0057 
2025-01-18 23:15:58.735789: train_loss -0.8593 
2025-01-18 23:15:58.736787: val_loss -0.5181 
2025-01-18 23:15:58.742336: Pseudo dice [np.float32(0.7211), np.float32(0.3844)] 
2025-01-18 23:15:58.744842: Epoch time: 32.11 s 
2025-01-18 23:15:59.310671:  
2025-01-18 23:15:59.311677: Epoch 117 
2025-01-18 23:15:59.316298: Current learning rate: 0.00567 
2025-01-18 23:16:31.415848: train_loss -0.8643 
2025-01-18 23:16:31.416853: val_loss -0.5162 
2025-01-18 23:16:31.421944: Pseudo dice [np.float32(0.7052), np.float32(0.3902)] 
2025-01-18 23:16:31.425972: Epoch time: 32.11 s 
2025-01-18 23:16:32.016454:  
2025-01-18 23:16:32.017458: Epoch 118 
2025-01-18 23:16:32.022003: Current learning rate: 0.00563 
2025-01-18 23:17:04.113495: train_loss -0.8629 
2025-01-18 23:17:04.113495: val_loss -0.5056 
2025-01-18 23:17:04.121026: Pseudo dice [np.float32(0.7388), np.float32(0.4266)] 
2025-01-18 23:17:04.124036: Epoch time: 32.1 s 
2025-01-18 23:17:04.707803:  
2025-01-18 23:17:04.708305: Epoch 119 
2025-01-18 23:17:04.713318: Current learning rate: 0.00559 
2025-01-18 23:17:36.785697: train_loss -0.8726 
2025-01-18 23:17:36.786700: val_loss -0.4923 
2025-01-18 23:17:36.791712: Pseudo dice [np.float32(0.7243), np.float32(0.3688)] 
2025-01-18 23:17:36.795755: Epoch time: 32.08 s 
2025-01-18 23:17:37.376055:  
2025-01-18 23:17:37.376055: Epoch 120 
2025-01-18 23:17:37.382146: Current learning rate: 0.00555 
2025-01-18 23:18:09.608089: train_loss -0.8697 
2025-01-18 23:18:09.609085: val_loss -0.4846 
2025-01-18 23:18:09.614102: Pseudo dice [np.float32(0.7135), np.float32(0.3763)] 
2025-01-18 23:18:09.617149: Epoch time: 32.23 s 
2025-01-18 23:18:10.376809:  
2025-01-18 23:18:10.376809: Epoch 121 
2025-01-18 23:18:10.383396: Current learning rate: 0.00551 
2025-01-18 23:18:42.516524: train_loss -0.8702 
2025-01-18 23:18:42.517546: val_loss -0.4724 
2025-01-18 23:18:42.522623: Pseudo dice [np.float32(0.7125), np.float32(0.406)] 
2025-01-18 23:18:42.527700: Epoch time: 32.14 s 
2025-01-18 23:18:43.106702:  
2025-01-18 23:18:43.106702: Epoch 122 
2025-01-18 23:18:43.113217: Current learning rate: 0.00547 
2025-01-18 23:19:15.102552: train_loss -0.8726 
2025-01-18 23:19:15.103554: val_loss -0.5097 
2025-01-18 23:19:15.108587: Pseudo dice [np.float32(0.7288), np.float32(0.4346)] 
2025-01-18 23:19:15.112604: Epoch time: 32.0 s 
2025-01-18 23:19:15.687354:  
2025-01-18 23:19:15.687857: Epoch 123 
2025-01-18 23:19:15.692871: Current learning rate: 0.00544 
2025-01-18 23:19:47.701744: train_loss -0.86 
2025-01-18 23:19:47.702748: val_loss -0.4746 
2025-01-18 23:19:47.708762: Pseudo dice [np.float32(0.7342), np.float32(0.256)] 
2025-01-18 23:19:47.711772: Epoch time: 32.02 s 
2025-01-18 23:19:48.284953:  
2025-01-18 23:19:48.284953: Epoch 124 
2025-01-18 23:19:48.290501: Current learning rate: 0.0054 
2025-01-18 23:20:20.273336: train_loss -0.8718 
2025-01-18 23:20:20.273336: val_loss -0.517 
2025-01-18 23:20:20.279757: Pseudo dice [np.float32(0.7051), np.float32(0.4445)] 
2025-01-18 23:20:20.283274: Epoch time: 31.99 s 
2025-01-18 23:20:20.848468:  
2025-01-18 23:20:20.849468: Epoch 125 
2025-01-18 23:20:20.855065: Current learning rate: 0.00536 
2025-01-18 23:20:52.873347: train_loss -0.8708 
2025-01-18 23:20:52.873850: val_loss -0.4922 
2025-01-18 23:20:52.879865: Pseudo dice [np.float32(0.6953), np.float32(0.367)] 
2025-01-18 23:20:52.883373: Epoch time: 32.02 s 
2025-01-18 23:20:53.445230:  
2025-01-18 23:20:53.445230: Epoch 126 
2025-01-18 23:20:53.450286: Current learning rate: 0.00532 
2025-01-18 23:21:25.439804: train_loss -0.8789 
2025-01-18 23:21:25.440309: val_loss -0.4785 
2025-01-18 23:21:25.445871: Pseudo dice [np.float32(0.7353), np.float32(0.3354)] 
2025-01-18 23:21:25.449393: Epoch time: 32.0 s 
2025-01-18 23:21:26.023826:  
2025-01-18 23:21:26.024830: Epoch 127 
2025-01-18 23:21:26.030569: Current learning rate: 0.00528 
2025-01-18 23:21:58.031483: train_loss -0.878 
2025-01-18 23:21:58.031985: val_loss -0.5374 
2025-01-18 23:21:58.038002: Pseudo dice [np.float32(0.7387), np.float32(0.4261)] 
2025-01-18 23:21:58.041524: Epoch time: 32.01 s 
2025-01-18 23:21:58.624775:  
2025-01-18 23:21:58.624775: Epoch 128 
2025-01-18 23:21:58.631416: Current learning rate: 0.00524 
2025-01-18 23:22:30.617896: train_loss -0.879 
2025-01-18 23:22:30.617896: val_loss -0.5071 
2025-01-18 23:22:30.623920: Pseudo dice [np.float32(0.7238), np.float32(0.3856)] 
2025-01-18 23:22:30.627939: Epoch time: 31.99 s 
2025-01-18 23:22:31.341550:  
2025-01-18 23:22:31.341550: Epoch 129 
2025-01-18 23:22:31.347660: Current learning rate: 0.0052 
2025-01-18 23:23:03.341752: train_loss -0.8699 
2025-01-18 23:23:03.343255: val_loss -0.5358 
2025-01-18 23:23:03.349326: Pseudo dice [np.float32(0.7338), np.float32(0.4374)] 
2025-01-18 23:23:03.352831: Epoch time: 32.0 s 
2025-01-18 23:23:03.918686:  
2025-01-18 23:23:03.919686: Epoch 130 
2025-01-18 23:23:03.924803: Current learning rate: 0.00517 
2025-01-18 23:23:35.955093: train_loss -0.8767 
2025-01-18 23:23:35.955598: val_loss -0.5062 
2025-01-18 23:23:35.960621: Pseudo dice [np.float32(0.7013), np.float32(0.4269)] 
2025-01-18 23:23:35.965137: Epoch time: 32.04 s 
2025-01-18 23:23:36.577177:  
2025-01-18 23:23:36.578177: Epoch 131 
2025-01-18 23:23:36.584273: Current learning rate: 0.00513 
2025-01-18 23:24:08.741376: train_loss -0.8587 
2025-01-18 23:24:08.741376: val_loss -0.5157 
2025-01-18 23:24:08.747956: Pseudo dice [np.float32(0.7078), np.float32(0.4001)] 
2025-01-18 23:24:08.751466: Epoch time: 32.16 s 
2025-01-18 23:24:09.315493:  
2025-01-18 23:24:09.315996: Epoch 132 
2025-01-18 23:24:09.321009: Current learning rate: 0.00509 
2025-01-18 23:24:41.424873: train_loss -0.8768 
2025-01-18 23:24:41.425375: val_loss -0.5298 
2025-01-18 23:24:41.430905: Pseudo dice [np.float32(0.7204), np.float32(0.3909)] 
2025-01-18 23:24:41.433987: Epoch time: 32.11 s 
2025-01-18 23:24:42.002387:  
2025-01-18 23:24:42.002387: Epoch 133 
2025-01-18 23:24:42.008512: Current learning rate: 0.00505 
2025-01-18 23:25:14.077328: train_loss -0.8753 
2025-01-18 23:25:14.077842: val_loss -0.5028 
2025-01-18 23:25:14.083862: Pseudo dice [np.float32(0.7211), np.float32(0.3924)] 
2025-01-18 23:25:14.087369: Epoch time: 32.08 s 
2025-01-18 23:25:14.660257:  
2025-01-18 23:25:14.660257: Epoch 134 
2025-01-18 23:25:14.665802: Current learning rate: 0.00501 
2025-01-18 23:25:46.857796: train_loss -0.8789 
2025-01-18 23:25:46.858299: val_loss -0.4992 
2025-01-18 23:25:46.863314: Pseudo dice [np.float32(0.7241), np.float32(0.414)] 
2025-01-18 23:25:46.866824: Epoch time: 32.2 s 
2025-01-18 23:25:47.454237:  
2025-01-18 23:25:47.454237: Epoch 135 
2025-01-18 23:25:47.459851: Current learning rate: 0.00497 
2025-01-18 23:26:19.716745: train_loss -0.8792 
2025-01-18 23:26:19.717255: val_loss -0.4587 
2025-01-18 23:26:19.723423: Pseudo dice [np.float32(0.6945), np.float32(0.3464)] 
2025-01-18 23:26:19.728022: Epoch time: 32.26 s 
2025-01-18 23:26:20.316912:  
2025-01-18 23:26:20.317912: Epoch 136 
2025-01-18 23:26:20.323483: Current learning rate: 0.00493 
2025-01-18 23:26:52.521072: train_loss -0.8842 
2025-01-18 23:26:52.522117: val_loss -0.5305 
2025-01-18 23:26:52.527668: Pseudo dice [np.float32(0.7232), np.float32(0.429)] 
2025-01-18 23:26:52.531778: Epoch time: 32.21 s 
2025-01-18 23:26:53.313392:  
2025-01-18 23:26:53.314389: Epoch 137 
2025-01-18 23:26:53.319987: Current learning rate: 0.00489 
2025-01-18 23:27:25.959940: train_loss -0.8826 
2025-01-18 23:27:25.959940: val_loss -0.4772 
2025-01-18 23:27:25.966460: Pseudo dice [np.float32(0.7177), np.float32(0.4013)] 
2025-01-18 23:27:25.969970: Epoch time: 32.65 s 
2025-01-18 23:27:26.555020:  
2025-01-18 23:27:26.556020: Epoch 138 
2025-01-18 23:27:26.561107: Current learning rate: 0.00485 
2025-01-18 23:27:59.043540: train_loss -0.8739 
2025-01-18 23:27:59.044055: val_loss -0.5363 
2025-01-18 23:27:59.050102: Pseudo dice [np.float32(0.7287), np.float32(0.478)] 
2025-01-18 23:27:59.053662: Epoch time: 32.49 s 
2025-01-18 23:27:59.640043:  
2025-01-18 23:27:59.640546: Epoch 139 
2025-01-18 23:27:59.645557: Current learning rate: 0.00482 
2025-01-18 23:28:31.824124: train_loss -0.8667 
2025-01-18 23:28:31.824124: val_loss -0.445 
2025-01-18 23:28:31.831162: Pseudo dice [np.float32(0.695), np.float32(0.3171)] 
2025-01-18 23:28:31.834194: Epoch time: 32.19 s 
2025-01-18 23:28:32.409780:  
2025-01-18 23:28:32.410779: Epoch 140 
2025-01-18 23:28:32.416377: Current learning rate: 0.00478 
2025-01-18 23:29:04.936364: train_loss -0.878 
2025-01-18 23:29:04.937363: val_loss -0.4501 
2025-01-18 23:29:04.942883: Pseudo dice [np.float32(0.7017), np.float32(0.2818)] 
2025-01-18 23:29:04.946976: Epoch time: 32.53 s 
2025-01-18 23:29:05.539840:  
2025-01-18 23:29:05.540840: Epoch 141 
2025-01-18 23:29:05.546451: Current learning rate: 0.00474 
2025-01-18 23:29:37.557579: train_loss -0.8771 
2025-01-18 23:29:37.558579: val_loss -0.5163 
2025-01-18 23:29:37.564103: Pseudo dice [np.float32(0.7118), np.float32(0.4162)] 
2025-01-18 23:29:37.567118: Epoch time: 32.02 s 
2025-01-18 23:29:38.166744:  
2025-01-18 23:29:38.167748: Epoch 142 
2025-01-18 23:29:38.172297: Current learning rate: 0.0047 
2025-01-18 23:30:10.483815: train_loss -0.8802 
2025-01-18 23:30:10.483815: val_loss -0.48 
2025-01-18 23:30:10.489835: Pseudo dice [np.float32(0.7013), np.float32(0.3717)] 
2025-01-18 23:30:10.493844: Epoch time: 32.32 s 
2025-01-18 23:30:11.097979:  
2025-01-18 23:30:11.097979: Epoch 143 
2025-01-18 23:30:11.102991: Current learning rate: 0.00466 
2025-01-18 23:30:43.268918: train_loss -0.873 
2025-01-18 23:30:43.269429: val_loss -0.5164 
2025-01-18 23:30:43.274966: Pseudo dice [np.float32(0.7304), np.float32(0.3629)] 
2025-01-18 23:30:43.278530: Epoch time: 32.17 s 
2025-01-18 23:30:43.875268:  
2025-01-18 23:30:43.875268: Epoch 144 
2025-01-18 23:30:43.881407: Current learning rate: 0.00462 
2025-01-18 23:31:16.000289: train_loss -0.8718 
2025-01-18 23:31:16.000289: val_loss -0.4892 
2025-01-18 23:31:16.005889: Pseudo dice [np.float32(0.7109), np.float32(0.3427)] 
2025-01-18 23:31:16.009408: Epoch time: 32.13 s 
2025-01-18 23:31:16.744855:  
2025-01-18 23:31:16.744855: Epoch 145 
2025-01-18 23:31:16.750919: Current learning rate: 0.00458 
2025-01-18 23:31:49.018527: train_loss -0.8639 
2025-01-18 23:31:49.020030: val_loss -0.4663 
2025-01-18 23:31:49.025566: Pseudo dice [np.float32(0.6935), np.float32(0.3874)] 
2025-01-18 23:31:49.029076: Epoch time: 32.27 s 
2025-01-18 23:31:49.616312:  
2025-01-18 23:31:49.616312: Epoch 146 
2025-01-18 23:31:49.622368: Current learning rate: 0.00454 
2025-01-18 23:32:21.718161: train_loss -0.8771 
2025-01-18 23:32:21.718664: val_loss -0.5438 
2025-01-18 23:32:21.724764: Pseudo dice [np.float32(0.7334), np.float32(0.5365)] 
2025-01-18 23:32:21.728323: Epoch time: 32.1 s 
2025-01-18 23:32:22.311288:  
2025-01-18 23:32:22.311288: Epoch 147 
2025-01-18 23:32:22.316841: Current learning rate: 0.0045 
2025-01-18 23:32:54.780606: train_loss -0.8776 
2025-01-18 23:32:54.781116: val_loss -0.4701 
2025-01-18 23:32:54.786278: Pseudo dice [np.float32(0.7124), np.float32(0.331)] 
2025-01-18 23:32:54.789377: Epoch time: 32.47 s 
2025-01-18 23:32:55.382409:  
2025-01-18 23:32:55.382912: Epoch 148 
2025-01-18 23:32:55.387928: Current learning rate: 0.00446 
