
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-08 09:53:55.699262: do_dummy_2d_data_aug: False 
2025-01-08 09:53:55.704261: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset006_Lung\splits_final.json 
2025-01-08 09:53:55.708955: The split file contains 5 splits. 
2025-01-08 09:53:55.711955: Desired fold for training: 0 
2025-01-08 09:53:55.713957: This split has 50 training and 13 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_avg_spacing1_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [314.0, 402.0, 402.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset006_Lung', 'plans_name': 'nnUNetPlans_avg_spacing1', 'original_median_spacing_after_transp': [1.244979977607727, 0.78515625, 0.78515625], 'original_median_shape_after_transp': [252, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -273.4598083496094, 'median': -162.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 311.0, 'std': 346.9495849609375}}} 
 
2025-01-08 09:54:03.986909: unpacking dataset... 
2025-01-08 09:54:04.199438: unpacking done... 
2025-01-08 09:54:08.104486:  
2025-01-08 09:54:08.104486: Epoch 0 
2025-01-08 09:54:08.110085: Current learning rate: 0.01 
2025-01-08 09:54:52.014760: train_loss 0.0314 
2025-01-08 09:54:52.015763: val_loss -0.125 
2025-01-08 09:54:52.021826: Pseudo dice [np.float32(0.0)] 
2025-01-08 09:54:52.025404: Epoch time: 43.91 s 
2025-01-08 09:54:52.028478: Yayy! New best EMA pseudo Dice: 0.0 
2025-01-08 09:54:52.732444:  
2025-01-08 09:54:52.732444: Epoch 1 
2025-01-08 09:54:52.738460: Current learning rate: 0.00991 
2025-01-08 09:55:32.488810: train_loss -0.1549 
2025-01-08 09:55:32.488810: val_loss -0.4739 
2025-01-08 09:55:32.493320: Pseudo dice [np.float32(0.5594)] 
2025-01-08 09:55:32.496330: Epoch time: 39.76 s 
2025-01-08 09:55:32.499841: Yayy! New best EMA pseudo Dice: 0.05590000003576279 
2025-01-08 09:55:33.270328:  
2025-01-08 09:55:33.271327: Epoch 2 
2025-01-08 09:55:33.276969: Current learning rate: 0.00982 
2025-01-08 09:56:12.921937: train_loss -0.3504 
2025-01-08 09:56:12.922440: val_loss -0.4183 
2025-01-08 09:56:12.927455: Pseudo dice [np.float32(0.4492)] 
2025-01-08 09:56:12.930974: Epoch time: 39.65 s 
2025-01-08 09:56:12.934478: Yayy! New best EMA pseudo Dice: 0.09529999643564224 
2025-01-08 09:56:13.795738:  
2025-01-08 09:56:13.795738: Epoch 3 
2025-01-08 09:56:13.800798: Current learning rate: 0.00973 
2025-01-08 09:56:53.517555: train_loss -0.3802 
2025-01-08 09:56:53.517555: val_loss -0.4976 
2025-01-08 09:56:53.523615: Pseudo dice [np.float32(0.5624)] 
2025-01-08 09:56:53.527146: Epoch time: 39.72 s 
2025-01-08 09:56:53.530194: Yayy! New best EMA pseudo Dice: 0.1420000046491623 
2025-01-08 09:56:54.332202:  
2025-01-08 09:56:54.333201: Epoch 4 
2025-01-08 09:56:54.338861: Current learning rate: 0.00964 
2025-01-08 09:57:34.022539: train_loss -0.452 
2025-01-08 09:57:34.023543: val_loss -0.5116 
2025-01-08 09:57:34.028558: Pseudo dice [np.float32(0.5364)] 
2025-01-08 09:57:34.032064: Epoch time: 39.69 s 
2025-01-08 09:57:34.035071: Yayy! New best EMA pseudo Dice: 0.18140000104904175 
2025-01-08 09:57:34.959333:  
2025-01-08 09:57:34.960332: Epoch 5 
2025-01-08 09:57:34.965396: Current learning rate: 0.00955 
2025-01-08 09:58:14.653667: train_loss -0.4479 
2025-01-08 09:58:14.654675: val_loss -0.6157 
2025-01-08 09:58:14.660268: Pseudo dice [np.float32(0.6239)] 
2025-01-08 09:58:14.663296: Epoch time: 39.69 s 
2025-01-08 09:58:14.666819: Yayy! New best EMA pseudo Dice: 0.2257000058889389 
2025-01-08 09:58:15.493455:  
2025-01-08 09:58:15.494459: Epoch 6 
2025-01-08 09:58:15.499999: Current learning rate: 0.00946 
2025-01-08 09:58:55.168204: train_loss -0.485 
2025-01-08 09:58:55.168204: val_loss -0.4182 
2025-01-08 09:58:55.174215: Pseudo dice [np.float32(0.4427)] 
2025-01-08 09:58:55.177223: Epoch time: 39.67 s 
2025-01-08 09:58:55.180732: Yayy! New best EMA pseudo Dice: 0.24740000069141388 
2025-01-08 09:58:55.948843:  
2025-01-08 09:58:55.949846: Epoch 7 
2025-01-08 09:58:55.954913: Current learning rate: 0.00937 
2025-01-08 09:59:35.626193: train_loss -0.4872 
2025-01-08 09:59:35.626701: val_loss -0.6344 
2025-01-08 09:59:35.631729: Pseudo dice [np.float32(0.6934)] 
2025-01-08 09:59:35.635247: Epoch time: 39.68 s 
2025-01-08 09:59:35.638117: Yayy! New best EMA pseudo Dice: 0.2919999957084656 
2025-01-08 09:59:36.456521:  
2025-01-08 09:59:36.457524: Epoch 8 
2025-01-08 09:59:36.462567: Current learning rate: 0.00928 
2025-01-08 10:00:16.145752: train_loss -0.5238 
2025-01-08 10:00:16.146752: val_loss -0.5545 
2025-01-08 10:00:16.152265: Pseudo dice [np.float32(0.6176)] 
2025-01-08 10:00:16.154771: Epoch time: 39.69 s 
2025-01-08 10:00:16.158281: Yayy! New best EMA pseudo Dice: 0.3244999945163727 
2025-01-08 10:00:16.949034:  
2025-01-08 10:00:16.949540: Epoch 9 
2025-01-08 10:00:16.954064: Current learning rate: 0.00919 
2025-01-08 10:00:56.586076: train_loss -0.5441 
2025-01-08 10:00:56.587588: val_loss -0.5831 
2025-01-08 10:00:56.593228: Pseudo dice [np.float32(0.6678)] 
2025-01-08 10:00:56.595769: Epoch time: 39.64 s 
2025-01-08 10:00:56.598314: Yayy! New best EMA pseudo Dice: 0.3589000105857849 
2025-01-08 10:00:57.395584:  
2025-01-08 10:00:57.395584: Epoch 10 
2025-01-08 10:00:57.401120: Current learning rate: 0.0091 
2025-01-08 10:01:37.080986: train_loss -0.5597 
2025-01-08 10:01:37.081489: val_loss -0.6184 
2025-01-08 10:01:37.087507: Pseudo dice [np.float32(0.6615)] 
2025-01-08 10:01:37.090012: Epoch time: 39.69 s 
2025-01-08 10:01:37.093523: Yayy! New best EMA pseudo Dice: 0.38909998536109924 
2025-01-08 10:01:37.888640:  
2025-01-08 10:01:37.888640: Epoch 11 
2025-01-08 10:01:37.894722: Current learning rate: 0.009 
2025-01-08 10:02:17.564564: train_loss -0.531 
2025-01-08 10:02:17.566066: val_loss -0.5734 
2025-01-08 10:02:17.571076: Pseudo dice [np.float32(0.6752)] 
2025-01-08 10:02:17.574586: Epoch time: 39.68 s 
2025-01-08 10:02:17.576621: Yayy! New best EMA pseudo Dice: 0.41769999265670776 
2025-01-08 10:02:18.338583:  
2025-01-08 10:02:18.338583: Epoch 12 
2025-01-08 10:02:18.344116: Current learning rate: 0.00891 
2025-01-08 10:02:58.039320: train_loss -0.5651 
2025-01-08 10:02:58.040324: val_loss -0.5909 
2025-01-08 10:02:58.046838: Pseudo dice [np.float32(0.6434)] 
2025-01-08 10:02:58.050347: Epoch time: 39.7 s 
2025-01-08 10:02:58.053852: Yayy! New best EMA pseudo Dice: 0.44029998779296875 
2025-01-08 10:02:59.018310:  
2025-01-08 10:02:59.018310: Epoch 13 
2025-01-08 10:02:59.022821: Current learning rate: 0.00882 
2025-01-08 10:03:38.629610: train_loss -0.5857 
2025-01-08 10:03:38.630613: val_loss -0.5491 
2025-01-08 10:03:38.636240: Pseudo dice [np.float32(0.617)] 
2025-01-08 10:03:38.639373: Epoch time: 39.61 s 
2025-01-08 10:03:38.642445: Yayy! New best EMA pseudo Dice: 0.4580000042915344 
2025-01-08 10:03:39.465875:  
2025-01-08 10:03:39.465875: Epoch 14 
2025-01-08 10:03:39.470887: Current learning rate: 0.00873 
2025-01-08 10:04:19.107701: train_loss -0.5787 
2025-01-08 10:04:19.108706: val_loss -0.6409 
2025-01-08 10:04:19.114721: Pseudo dice [np.float32(0.6845)] 
2025-01-08 10:04:19.117733: Epoch time: 39.64 s 
2025-01-08 10:04:19.120242: Yayy! New best EMA pseudo Dice: 0.4805999994277954 
2025-01-08 10:04:19.944938:  
2025-01-08 10:04:19.945936: Epoch 15 
2025-01-08 10:04:19.951001: Current learning rate: 0.00864 
2025-01-08 10:04:59.607156: train_loss -0.5362 
2025-01-08 10:04:59.607678: val_loss -0.647 
2025-01-08 10:04:59.613733: Pseudo dice [np.float32(0.7145)] 
2025-01-08 10:04:59.616758: Epoch time: 39.66 s 
2025-01-08 10:04:59.620001: Yayy! New best EMA pseudo Dice: 0.5040000081062317 
2025-01-08 10:05:00.406301:  
2025-01-08 10:05:00.406301: Epoch 16 
2025-01-08 10:05:00.411333: Current learning rate: 0.00855 
2025-01-08 10:05:40.078032: train_loss -0.5866 
2025-01-08 10:05:40.079563: val_loss -0.6763 
2025-01-08 10:05:40.085106: Pseudo dice [np.float32(0.7562)] 
2025-01-08 10:05:40.088619: Epoch time: 39.67 s 
2025-01-08 10:05:40.092229: Yayy! New best EMA pseudo Dice: 0.52920001745224 
2025-01-08 10:05:40.914878:  
2025-01-08 10:05:40.914878: Epoch 17 
2025-01-08 10:05:40.920440: Current learning rate: 0.00846 
2025-01-08 10:06:20.591511: train_loss -0.5779 
2025-01-08 10:06:20.592515: val_loss -0.6029 
2025-01-08 10:06:20.598534: Pseudo dice [np.float32(0.5907)] 
2025-01-08 10:06:20.601549: Epoch time: 39.68 s 
2025-01-08 10:06:20.605060: Yayy! New best EMA pseudo Dice: 0.5353999733924866 
2025-01-08 10:06:21.391745:  
2025-01-08 10:06:21.391745: Epoch 18 
2025-01-08 10:06:21.396812: Current learning rate: 0.00836 
2025-01-08 10:07:01.046657: train_loss -0.5886 
2025-01-08 10:07:01.047662: val_loss -0.6463 
2025-01-08 10:07:01.053681: Pseudo dice [np.float32(0.697)] 
2025-01-08 10:07:01.056697: Epoch time: 39.66 s 
2025-01-08 10:07:01.060210: Yayy! New best EMA pseudo Dice: 0.5515000224113464 
2025-01-08 10:07:01.894543:  
2025-01-08 10:07:01.894543: Epoch 19 
2025-01-08 10:07:01.900637: Current learning rate: 0.00827 
2025-01-08 10:07:41.567235: train_loss -0.5726 
2025-01-08 10:07:41.567235: val_loss -0.6185 
2025-01-08 10:07:41.573265: Pseudo dice [np.float32(0.6871)] 
2025-01-08 10:07:41.576773: Epoch time: 39.67 s 
2025-01-08 10:07:41.579784: Yayy! New best EMA pseudo Dice: 0.5651000142097473 
2025-01-08 10:07:42.566504:  
2025-01-08 10:07:42.566504: Epoch 20 
2025-01-08 10:07:42.572105: Current learning rate: 0.00818 
2025-01-08 10:08:22.239623: train_loss -0.5953 
2025-01-08 10:08:22.240627: val_loss -0.5838 
2025-01-08 10:08:22.244637: Pseudo dice [np.float32(0.6024)] 
2025-01-08 10:08:22.247144: Epoch time: 39.67 s 
2025-01-08 10:08:22.250654: Yayy! New best EMA pseudo Dice: 0.5687999725341797 
2025-01-08 10:08:23.082237:  
2025-01-08 10:08:23.082237: Epoch 21 
2025-01-08 10:08:23.087801: Current learning rate: 0.00809 
2025-01-08 10:09:02.730825: train_loss -0.6004 
2025-01-08 10:09:02.730825: val_loss -0.6226 
2025-01-08 10:09:02.736838: Pseudo dice [np.float32(0.7371)] 
2025-01-08 10:09:02.739448: Epoch time: 39.65 s 
2025-01-08 10:09:02.742962: Yayy! New best EMA pseudo Dice: 0.5856000185012817 
2025-01-08 10:09:03.501116:  
2025-01-08 10:09:03.501116: Epoch 22 
2025-01-08 10:09:03.505642: Current learning rate: 0.008 
2025-01-08 10:09:43.209457: train_loss -0.6088 
2025-01-08 10:09:43.209457: val_loss -0.5789 
2025-01-08 10:09:43.215474: Pseudo dice [np.float32(0.6172)] 
2025-01-08 10:09:43.218980: Epoch time: 39.71 s 
2025-01-08 10:09:43.221989: Yayy! New best EMA pseudo Dice: 0.5888000130653381 
2025-01-08 10:09:43.982387:  
2025-01-08 10:09:43.982889: Epoch 23 
2025-01-08 10:09:43.988448: Current learning rate: 0.0079 
2025-01-08 10:10:23.632183: train_loss -0.5425 
2025-01-08 10:10:23.633192: val_loss -0.6236 
2025-01-08 10:10:23.638789: Pseudo dice [np.float32(0.7101)] 
2025-01-08 10:10:23.642303: Epoch time: 39.65 s 
2025-01-08 10:10:23.644809: Yayy! New best EMA pseudo Dice: 0.6008999943733215 
2025-01-08 10:10:24.446036:  
2025-01-08 10:10:24.446036: Epoch 24 
2025-01-08 10:10:24.452054: Current learning rate: 0.00781 
2025-01-08 10:11:04.106878: train_loss -0.6102 
2025-01-08 10:11:04.107455: val_loss -0.593 
2025-01-08 10:11:04.112563: Pseudo dice [np.float32(0.6668)] 
2025-01-08 10:11:04.115594: Epoch time: 39.66 s 
2025-01-08 10:11:04.119442: Yayy! New best EMA pseudo Dice: 0.6075000166893005 
2025-01-08 10:11:04.892444:  
2025-01-08 10:11:04.892947: Epoch 25 
2025-01-08 10:11:04.898966: Current learning rate: 0.00772 
2025-01-08 10:11:44.523044: train_loss -0.6269 
2025-01-08 10:11:44.524547: val_loss -0.6986 
2025-01-08 10:11:44.529563: Pseudo dice [np.float32(0.7469)] 
2025-01-08 10:11:44.532602: Epoch time: 39.63 s 
2025-01-08 10:11:44.535640: Yayy! New best EMA pseudo Dice: 0.6215000152587891 
2025-01-08 10:11:45.354575:  
2025-01-08 10:11:45.354575: Epoch 26 
2025-01-08 10:11:45.360681: Current learning rate: 0.00763 
2025-01-08 10:12:25.035409: train_loss -0.5991 
2025-01-08 10:12:25.035409: val_loss -0.6851 
2025-01-08 10:12:25.041427: Pseudo dice [np.float32(0.7685)] 
2025-01-08 10:12:25.045436: Epoch time: 39.68 s 
2025-01-08 10:12:25.047943: Yayy! New best EMA pseudo Dice: 0.6362000107765198 
2025-01-08 10:12:25.807727:  
2025-01-08 10:12:25.807727: Epoch 27 
2025-01-08 10:12:25.812740: Current learning rate: 0.00753 
2025-01-08 10:13:05.450667: train_loss -0.6077 
2025-01-08 10:13:05.451170: val_loss -0.6837 
2025-01-08 10:13:05.457191: Pseudo dice [np.float32(0.7305)] 
2025-01-08 10:13:05.460696: Epoch time: 39.64 s 
2025-01-08 10:13:05.463708: Yayy! New best EMA pseudo Dice: 0.6456000208854675 
2025-01-08 10:13:06.427499:  
2025-01-08 10:13:06.428504: Epoch 28 
2025-01-08 10:13:06.433606: Current learning rate: 0.00744 
2025-01-08 10:13:46.088058: train_loss -0.642 
2025-01-08 10:13:46.089567: val_loss -0.6477 
2025-01-08 10:13:46.095166: Pseudo dice [np.float32(0.7111)] 
2025-01-08 10:13:46.097673: Epoch time: 39.66 s 
2025-01-08 10:13:46.101182: Yayy! New best EMA pseudo Dice: 0.6521000266075134 
2025-01-08 10:13:46.904664:  
2025-01-08 10:13:46.905663: Epoch 29 
2025-01-08 10:13:46.909674: Current learning rate: 0.00735 
2025-01-08 10:14:26.552313: train_loss -0.6211 
2025-01-08 10:14:26.552313: val_loss -0.6276 
2025-01-08 10:14:26.558825: Pseudo dice [np.float32(0.7378)] 
2025-01-08 10:14:26.561331: Epoch time: 39.65 s 
2025-01-08 10:14:26.564839: Yayy! New best EMA pseudo Dice: 0.6607000231742859 
2025-01-08 10:14:27.388419:  
2025-01-08 10:14:27.389418: Epoch 30 
2025-01-08 10:14:27.394934: Current learning rate: 0.00725 
2025-01-08 10:15:07.034775: train_loss -0.6598 
2025-01-08 10:15:07.034775: val_loss -0.5758 
2025-01-08 10:15:07.040788: Pseudo dice [np.float32(0.6682)] 
2025-01-08 10:15:07.043798: Epoch time: 39.65 s 
2025-01-08 10:15:07.046306: Yayy! New best EMA pseudo Dice: 0.6614999771118164 
2025-01-08 10:15:07.823298:  
2025-01-08 10:15:07.824295: Epoch 31 
2025-01-08 10:15:07.828309: Current learning rate: 0.00716 
2025-01-08 10:15:47.458095: train_loss -0.6081 
2025-01-08 10:15:47.458610: val_loss -0.6291 
2025-01-08 10:15:47.463158: Pseudo dice [np.float32(0.6984)] 
2025-01-08 10:15:47.466215: Epoch time: 39.63 s 
2025-01-08 10:15:47.468752: Yayy! New best EMA pseudo Dice: 0.6651999950408936 
2025-01-08 10:15:48.242527:  
2025-01-08 10:15:48.243533: Epoch 32 
2025-01-08 10:15:48.248079: Current learning rate: 0.00707 
2025-01-08 10:16:27.890125: train_loss -0.5935 
2025-01-08 10:16:27.891130: val_loss -0.6171 
2025-01-08 10:16:27.897146: Pseudo dice [np.float32(0.6597)] 
2025-01-08 10:16:27.900179: Epoch time: 39.65 s 
2025-01-08 10:16:28.468378:  
2025-01-08 10:16:28.469384: Epoch 33 
2025-01-08 10:16:28.473965: Current learning rate: 0.00697 
2025-01-08 10:17:08.126638: train_loss -0.5784 
2025-01-08 10:17:08.126638: val_loss -0.52 
2025-01-08 10:17:08.132655: Pseudo dice [np.float32(0.5597)] 
2025-01-08 10:17:08.136165: Epoch time: 39.66 s 
2025-01-08 10:17:08.711344:  
2025-01-08 10:17:08.711344: Epoch 34 
2025-01-08 10:17:08.716359: Current learning rate: 0.00688 
2025-01-08 10:17:48.355100: train_loss -0.5856 
2025-01-08 10:17:48.356602: val_loss -0.6677 
2025-01-08 10:17:48.361617: Pseudo dice [np.float32(0.736)] 
2025-01-08 10:17:48.365126: Epoch time: 39.65 s 
2025-01-08 10:17:49.094195:  
2025-01-08 10:17:49.094195: Epoch 35 
2025-01-08 10:17:49.099240: Current learning rate: 0.00679 
2025-01-08 10:18:28.748433: train_loss -0.6601 
2025-01-08 10:18:28.749436: val_loss -0.6732 
2025-01-08 10:18:28.753446: Pseudo dice [np.float32(0.7151)] 
2025-01-08 10:18:28.756956: Epoch time: 39.66 s 
2025-01-08 10:18:28.760462: Yayy! New best EMA pseudo Dice: 0.6675999760627747 
2025-01-08 10:18:29.582855:  
2025-01-08 10:18:29.582855: Epoch 36 
2025-01-08 10:18:29.588411: Current learning rate: 0.00669 
2025-01-08 10:19:09.229383: train_loss -0.6684 
2025-01-08 10:19:09.230383: val_loss -0.6018 
2025-01-08 10:19:09.235896: Pseudo dice [np.float32(0.666)] 
2025-01-08 10:19:09.239405: Epoch time: 39.65 s 
2025-01-08 10:19:09.825866:  
2025-01-08 10:19:09.825866: Epoch 37 
2025-01-08 10:19:09.830876: Current learning rate: 0.0066 
2025-01-08 10:19:49.500706: train_loss -0.6598 
2025-01-08 10:19:49.501208: val_loss -0.7346 
2025-01-08 10:19:49.506292: Pseudo dice [np.float32(0.7797)] 
2025-01-08 10:19:49.509322: Epoch time: 39.68 s 
2025-01-08 10:19:49.511841: Yayy! New best EMA pseudo Dice: 0.678600013256073 
2025-01-08 10:19:50.350464:  
2025-01-08 10:19:50.350464: Epoch 38 
2025-01-08 10:19:50.355474: Current learning rate: 0.0065 
2025-01-08 10:20:29.980060: train_loss -0.6107 
2025-01-08 10:20:29.981065: val_loss -0.6205 
2025-01-08 10:20:29.986075: Pseudo dice [np.float32(0.6641)] 
2025-01-08 10:20:29.990105: Epoch time: 39.63 s 
2025-01-08 10:20:30.573362:  
2025-01-08 10:20:30.574365: Epoch 39 
2025-01-08 10:20:30.579425: Current learning rate: 0.00641 
2025-01-08 10:21:10.222105: train_loss -0.6357 
2025-01-08 10:21:10.222105: val_loss -0.6511 
2025-01-08 10:21:10.228642: Pseudo dice [np.float32(0.6412)] 
2025-01-08 10:21:10.231653: Epoch time: 39.65 s 
2025-01-08 10:21:10.837064:  
2025-01-08 10:21:10.837064: Epoch 40 
2025-01-08 10:21:10.842633: Current learning rate: 0.00631 
2025-01-08 10:21:50.490432: train_loss -0.6342 
2025-01-08 10:21:50.490934: val_loss -0.6583 
2025-01-08 10:21:50.496512: Pseudo dice [np.float32(0.7357)] 
2025-01-08 10:21:50.499553: Epoch time: 39.65 s 
2025-01-08 10:21:50.502085: Yayy! New best EMA pseudo Dice: 0.6797999739646912 
2025-01-08 10:21:51.332330:  
2025-01-08 10:21:51.332844: Epoch 41 
2025-01-08 10:21:51.337892: Current learning rate: 0.00622 
2025-01-08 10:22:31.028797: train_loss -0.6675 
2025-01-08 10:22:31.028797: val_loss -0.6839 
2025-01-08 10:22:31.034813: Pseudo dice [np.float32(0.7489)] 
2025-01-08 10:22:31.038318: Epoch time: 39.7 s 
2025-01-08 10:22:31.041326: Yayy! New best EMA pseudo Dice: 0.6866999864578247 
2025-01-08 10:22:31.844488:  
2025-01-08 10:22:31.845995: Epoch 42 
2025-01-08 10:22:31.851108: Current learning rate: 0.00612 
2025-01-08 10:23:11.479168: train_loss -0.6631 
2025-01-08 10:23:11.479168: val_loss -0.6148 
2025-01-08 10:23:11.485182: Pseudo dice [np.float32(0.6406)] 
2025-01-08 10:23:11.488691: Epoch time: 39.63 s 
2025-01-08 10:23:12.204369:  
2025-01-08 10:23:12.205371: Epoch 43 
2025-01-08 10:23:12.210395: Current learning rate: 0.00603 
2025-01-08 10:23:51.850135: train_loss -0.6571 
2025-01-08 10:23:51.850640: val_loss -0.6224 
2025-01-08 10:23:51.855675: Pseudo dice [np.float32(0.699)] 
2025-01-08 10:23:51.859699: Epoch time: 39.65 s 
2025-01-08 10:23:52.423358:  
2025-01-08 10:23:52.424362: Epoch 44 
2025-01-08 10:23:52.428919: Current learning rate: 0.00593 
2025-01-08 10:24:32.074621: train_loss -0.6282 
2025-01-08 10:24:32.075625: val_loss -0.656 
2025-01-08 10:24:32.081255: Pseudo dice [np.float32(0.7089)] 
2025-01-08 10:24:32.084820: Epoch time: 39.65 s 
2025-01-08 10:24:32.647032:  
2025-01-08 10:24:32.648036: Epoch 45 
2025-01-08 10:24:32.653104: Current learning rate: 0.00584 
2025-01-08 10:25:12.314063: train_loss -0.6412 
2025-01-08 10:25:12.314575: val_loss -0.6868 
2025-01-08 10:25:12.320621: Pseudo dice [np.float32(0.7606)] 
2025-01-08 10:25:12.323652: Epoch time: 39.67 s 
2025-01-08 10:25:12.326676: Yayy! New best EMA pseudo Dice: 0.6937000155448914 
2025-01-08 10:25:13.116390:  
2025-01-08 10:25:13.116390: Epoch 46 
2025-01-08 10:25:13.122405: Current learning rate: 0.00574 
2025-01-08 10:25:52.758430: train_loss -0.6861 
2025-01-08 10:25:52.758430: val_loss -0.6171 
2025-01-08 10:25:52.764446: Pseudo dice [np.float32(0.6677)] 
2025-01-08 10:25:52.767954: Epoch time: 39.64 s 
2025-01-08 10:25:53.410107:  
2025-01-08 10:25:53.410107: Epoch 47 
2025-01-08 10:25:53.415121: Current learning rate: 0.00565 
2025-01-08 10:26:33.063507: train_loss -0.6749 
2025-01-08 10:26:33.063507: val_loss -0.6912 
2025-01-08 10:26:33.069520: Pseudo dice [np.float32(0.7631)] 
2025-01-08 10:26:33.071541: Epoch time: 39.65 s 
2025-01-08 10:26:33.075091: Yayy! New best EMA pseudo Dice: 0.6983000040054321 
2025-01-08 10:26:33.886832:  
2025-01-08 10:26:33.887836: Epoch 48 
2025-01-08 10:26:33.892433: Current learning rate: 0.00555 
2025-01-08 10:27:13.583415: train_loss -0.675 
2025-01-08 10:27:13.584419: val_loss -0.64 
2025-01-08 10:27:13.590932: Pseudo dice [np.float32(0.6886)] 
2025-01-08 10:27:13.593437: Epoch time: 39.7 s 
2025-01-08 10:27:14.164110:  
2025-01-08 10:27:14.164110: Epoch 49 
2025-01-08 10:27:14.170124: Current learning rate: 0.00546 
2025-01-08 10:27:53.831836: train_loss -0.6731 
2025-01-08 10:27:53.832338: val_loss -0.6615 
2025-01-08 10:27:53.839427: Pseudo dice [np.float32(0.7071)] 
2025-01-08 10:27:53.842457: Epoch time: 39.67 s 
2025-01-08 10:27:54.001454: Yayy! New best EMA pseudo Dice: 0.6983000040054321 
2025-01-08 10:27:54.958446:  
2025-01-08 10:27:54.958446: Epoch 50 
2025-01-08 10:27:54.963983: Current learning rate: 0.00536 
2025-01-08 10:28:34.616564: train_loss -0.6756 
2025-01-08 10:28:34.616564: val_loss -0.6128 
2025-01-08 10:28:34.622578: Pseudo dice [np.float32(0.7583)] 
2025-01-08 10:28:34.626084: Epoch time: 39.66 s 
2025-01-08 10:28:34.629122: Yayy! New best EMA pseudo Dice: 0.7042999863624573 
2025-01-08 10:28:35.432574:  
2025-01-08 10:28:35.432574: Epoch 51 
2025-01-08 10:28:35.438106: Current learning rate: 0.00526 
2025-01-08 10:29:15.091133: train_loss -0.6464 
2025-01-08 10:29:15.092136: val_loss -0.6359 
2025-01-08 10:29:15.097673: Pseudo dice [np.float32(0.7027)] 
2025-01-08 10:29:15.101229: Epoch time: 39.66 s 
2025-01-08 10:29:15.716865:  
2025-01-08 10:29:15.717869: Epoch 52 
2025-01-08 10:29:15.723405: Current learning rate: 0.00517 
2025-01-08 10:29:55.343005: train_loss -0.6719 
2025-01-08 10:29:55.344009: val_loss -0.7158 
2025-01-08 10:29:55.350587: Pseudo dice [np.float32(0.7857)] 
2025-01-08 10:29:55.354116: Epoch time: 39.63 s 
2025-01-08 10:29:55.357144: Yayy! New best EMA pseudo Dice: 0.7123000025749207 
2025-01-08 10:29:56.155528:  
2025-01-08 10:29:56.156531: Epoch 53 
2025-01-08 10:29:56.161134: Current learning rate: 0.00507 
2025-01-08 10:30:35.840548: train_loss -0.6475 
2025-01-08 10:30:35.841552: val_loss -0.6745 
2025-01-08 10:30:35.846660: Pseudo dice [np.float32(0.7541)] 
2025-01-08 10:30:35.851201: Epoch time: 39.69 s 
2025-01-08 10:30:35.853727: Yayy! New best EMA pseudo Dice: 0.7164999842643738 
2025-01-08 10:30:36.646642:  
2025-01-08 10:30:36.646642: Epoch 54 
2025-01-08 10:30:36.651225: Current learning rate: 0.00497 
2025-01-08 10:31:16.305235: train_loss -0.6744 
2025-01-08 10:31:16.305737: val_loss -0.6054 
2025-01-08 10:31:16.311273: Pseudo dice [np.float32(0.6478)] 
2025-01-08 10:31:16.314791: Epoch time: 39.66 s 
2025-01-08 10:31:16.880477:  
2025-01-08 10:31:16.881481: Epoch 55 
2025-01-08 10:31:16.886520: Current learning rate: 0.00487 
2025-01-08 10:31:56.491995: train_loss -0.6704 
2025-01-08 10:31:56.492996: val_loss -0.6657 
2025-01-08 10:31:56.499517: Pseudo dice [np.float32(0.7145)] 
2025-01-08 10:31:56.503525: Epoch time: 39.61 s 
2025-01-08 10:31:57.065900:  
2025-01-08 10:31:57.066903: Epoch 56 
2025-01-08 10:31:57.071937: Current learning rate: 0.00478 
2025-01-08 10:32:36.752082: train_loss -0.7075 
2025-01-08 10:32:36.752589: val_loss -0.6111 
2025-01-08 10:32:36.757638: Pseudo dice [np.float32(0.7353)] 
2025-01-08 10:32:36.761671: Epoch time: 39.69 s 
2025-01-08 10:32:37.325677:  
2025-01-08 10:32:37.326681: Epoch 57 
2025-01-08 10:32:37.331236: Current learning rate: 0.00468 
2025-01-08 10:33:17.007989: train_loss -0.6674 
2025-01-08 10:33:17.007989: val_loss -0.6772 
2025-01-08 10:33:17.014620: Pseudo dice [np.float32(0.7101)] 
2025-01-08 10:33:17.017663: Epoch time: 39.68 s 
2025-01-08 10:33:17.728722:  
2025-01-08 10:33:17.728722: Epoch 58 
2025-01-08 10:33:17.733733: Current learning rate: 0.00458 
2025-01-08 10:33:57.395642: train_loss -0.6403 
2025-01-08 10:33:57.396151: val_loss -0.6664 
2025-01-08 10:33:57.401186: Pseudo dice [np.float32(0.7321)] 
2025-01-08 10:33:57.404705: Epoch time: 39.67 s 
2025-01-08 10:33:57.982942:  
2025-01-08 10:33:57.982942: Epoch 59 
2025-01-08 10:33:57.988508: Current learning rate: 0.00448 
2025-01-08 10:34:37.642890: train_loss -0.6835 
2025-01-08 10:34:37.642890: val_loss -0.654 
2025-01-08 10:34:37.648904: Pseudo dice [np.float32(0.715)] 
2025-01-08 10:34:37.652410: Epoch time: 39.66 s 
2025-01-08 10:34:38.233603:  
2025-01-08 10:34:38.233603: Epoch 60 
2025-01-08 10:34:38.239174: Current learning rate: 0.00438 
2025-01-08 10:35:17.900254: train_loss -0.6861 
2025-01-08 10:35:17.901258: val_loss -0.715 
2025-01-08 10:35:17.907338: Pseudo dice [np.float32(0.7598)] 
2025-01-08 10:35:17.909889: Epoch time: 39.67 s 
2025-01-08 10:35:17.912917: Yayy! New best EMA pseudo Dice: 0.718999981880188 
2025-01-08 10:35:18.733812:  
2025-01-08 10:35:18.734816: Epoch 61 
2025-01-08 10:35:18.739372: Current learning rate: 0.00429 
2025-01-08 10:35:58.339533: train_loss -0.6978 
2025-01-08 10:35:58.339533: val_loss -0.6969 
2025-01-08 10:35:58.345573: Pseudo dice [np.float32(0.7564)] 
2025-01-08 10:35:58.349096: Epoch time: 39.61 s 
2025-01-08 10:35:58.352180: Yayy! New best EMA pseudo Dice: 0.7226999998092651 
2025-01-08 10:35:59.122519:  
2025-01-08 10:35:59.122519: Epoch 62 
2025-01-08 10:35:59.127530: Current learning rate: 0.00419 
2025-01-08 10:36:38.810046: train_loss -0.696 
2025-01-08 10:36:38.811048: val_loss -0.6247 
2025-01-08 10:36:38.816067: Pseudo dice [np.float32(0.6733)] 
2025-01-08 10:36:38.820083: Epoch time: 39.69 s 
2025-01-08 10:36:39.396881:  
2025-01-08 10:36:39.396881: Epoch 63 
2025-01-08 10:36:39.402942: Current learning rate: 0.00409 
2025-01-08 10:37:19.104214: train_loss -0.6748 
2025-01-08 10:37:19.104717: val_loss -0.6337 
2025-01-08 10:37:19.110734: Pseudo dice [np.float32(0.7557)] 
2025-01-08 10:37:19.113753: Epoch time: 39.71 s 
2025-01-08 10:37:19.692955:  
2025-01-08 10:37:19.692955: Epoch 64 
2025-01-08 10:37:19.698967: Current learning rate: 0.00399 
2025-01-08 10:37:59.318696: train_loss -0.7142 
2025-01-08 10:37:59.320210: val_loss -0.6454 
2025-01-08 10:37:59.325303: Pseudo dice [np.float32(0.7479)] 
2025-01-08 10:37:59.328370: Epoch time: 39.63 s 
2025-01-08 10:37:59.331879: Yayy! New best EMA pseudo Dice: 0.7242000102996826 
2025-01-08 10:38:00.161762:  
2025-01-08 10:38:00.162761: Epoch 65 
2025-01-08 10:38:00.167832: Current learning rate: 0.00389 
2025-01-08 10:38:39.880577: train_loss -0.6978 
2025-01-08 10:38:39.880577: val_loss -0.7278 
2025-01-08 10:38:39.886587: Pseudo dice [np.float32(0.7947)] 
2025-01-08 10:38:39.889596: Epoch time: 39.72 s 
2025-01-08 10:38:39.893104: Yayy! New best EMA pseudo Dice: 0.7311999797821045 
2025-01-08 10:38:40.849004:  
2025-01-08 10:38:40.849004: Epoch 66 
2025-01-08 10:38:40.854016: Current learning rate: 0.00379 
2025-01-08 10:39:20.521678: train_loss -0.7173 
2025-01-08 10:39:20.521678: val_loss -0.6552 
2025-01-08 10:39:20.527695: Pseudo dice [np.float32(0.7669)] 
2025-01-08 10:39:20.531202: Epoch time: 39.67 s 
2025-01-08 10:39:20.534213: Yayy! New best EMA pseudo Dice: 0.7347999811172485 
2025-01-08 10:39:21.317744:  
2025-01-08 10:39:21.317744: Epoch 67 
2025-01-08 10:39:21.323299: Current learning rate: 0.00369 
2025-01-08 10:40:00.987231: train_loss -0.6975 
2025-01-08 10:40:00.988735: val_loss -0.7043 
2025-01-08 10:40:00.993754: Pseudo dice [np.float32(0.7581)] 
2025-01-08 10:40:00.997267: Epoch time: 39.67 s 
2025-01-08 10:40:01.001381: Yayy! New best EMA pseudo Dice: 0.7371000051498413 
2025-01-08 10:40:01.823883:  
2025-01-08 10:40:01.823883: Epoch 68 
2025-01-08 10:40:01.828933: Current learning rate: 0.00359 
2025-01-08 10:40:41.468697: train_loss -0.7244 
2025-01-08 10:40:41.468697: val_loss -0.704 
2025-01-08 10:40:41.474716: Pseudo dice [np.float32(0.7854)] 
2025-01-08 10:40:41.478227: Epoch time: 39.65 s 
2025-01-08 10:40:41.481238: Yayy! New best EMA pseudo Dice: 0.7419999837875366 
2025-01-08 10:40:42.305984:  
2025-01-08 10:40:42.305984: Epoch 69 
2025-01-08 10:40:42.311522: Current learning rate: 0.00349 
2025-01-08 10:41:22.041533: train_loss -0.7377 
2025-01-08 10:41:22.042036: val_loss -0.6845 
2025-01-08 10:41:22.047572: Pseudo dice [np.float32(0.7895)] 
2025-01-08 10:41:22.051090: Epoch time: 39.74 s 
2025-01-08 10:41:22.054615: Yayy! New best EMA pseudo Dice: 0.7466999888420105 
2025-01-08 10:41:22.836726:  
2025-01-08 10:41:22.837726: Epoch 70 
2025-01-08 10:41:22.842347: Current learning rate: 0.00338 
2025-01-08 10:42:02.531893: train_loss -0.6806 
2025-01-08 10:42:02.533401: val_loss -0.7398 
2025-01-08 10:42:02.539050: Pseudo dice [np.float32(0.8089)] 
2025-01-08 10:42:02.542576: Epoch time: 39.7 s 
2025-01-08 10:42:02.544612: Yayy! New best EMA pseudo Dice: 0.7529000043869019 
2025-01-08 10:42:03.364698:  
2025-01-08 10:42:03.364698: Epoch 71 
2025-01-08 10:42:03.370253: Current learning rate: 0.00328 
2025-01-08 10:42:43.023878: train_loss -0.7257 
2025-01-08 10:42:43.024882: val_loss -0.6868 
2025-01-08 10:42:43.030893: Pseudo dice [np.float32(0.7565)] 
2025-01-08 10:42:43.033901: Epoch time: 39.66 s 
2025-01-08 10:42:43.036918: Yayy! New best EMA pseudo Dice: 0.7533000111579895 
2025-01-08 10:42:43.899684:  
2025-01-08 10:42:43.899684: Epoch 72 
2025-01-08 10:42:43.904754: Current learning rate: 0.00318 
2025-01-08 10:43:23.598116: train_loss -0.7162 
2025-01-08 10:43:23.598618: val_loss -0.6756 
2025-01-08 10:43:23.603631: Pseudo dice [np.float32(0.7354)] 
2025-01-08 10:43:23.607141: Epoch time: 39.7 s 
2025-01-08 10:43:24.347998:  
2025-01-08 10:43:24.347998: Epoch 73 
2025-01-08 10:43:24.354563: Current learning rate: 0.00308 
2025-01-08 10:44:04.030099: train_loss -0.7488 
2025-01-08 10:44:04.030611: val_loss -0.6198 
2025-01-08 10:44:04.036171: Pseudo dice [np.float32(0.6919)] 
2025-01-08 10:44:04.039763: Epoch time: 39.68 s 
2025-01-08 10:44:04.628885:  
2025-01-08 10:44:04.628885: Epoch 74 
2025-01-08 10:44:04.633942: Current learning rate: 0.00297 
2025-01-08 10:44:44.320502: train_loss -0.7421 
2025-01-08 10:44:44.321006: val_loss -0.7458 
2025-01-08 10:44:44.327023: Pseudo dice [np.float32(0.8057)] 
2025-01-08 10:44:44.329528: Epoch time: 39.69 s 
2025-01-08 10:44:44.926048:  
2025-01-08 10:44:44.926048: Epoch 75 
2025-01-08 10:44:44.931582: Current learning rate: 0.00287 
2025-01-08 10:45:24.611087: train_loss -0.7399 
2025-01-08 10:45:24.611087: val_loss -0.7276 
2025-01-08 10:45:24.617608: Pseudo dice [np.float32(0.7904)] 
2025-01-08 10:45:24.620116: Epoch time: 39.69 s 
2025-01-08 10:45:24.623628: Yayy! New best EMA pseudo Dice: 0.7554000020027161 
2025-01-08 10:45:25.407083:  
2025-01-08 10:45:25.408086: Epoch 76 
2025-01-08 10:45:25.413117: Current learning rate: 0.00277 
2025-01-08 10:46:05.080418: train_loss -0.7296 
2025-01-08 10:46:05.080418: val_loss -0.6673 
2025-01-08 10:46:05.086436: Pseudo dice [np.float32(0.7635)] 
2025-01-08 10:46:05.089957: Epoch time: 39.67 s 
2025-01-08 10:46:05.093506: Yayy! New best EMA pseudo Dice: 0.7562000155448914 
2025-01-08 10:46:05.913078:  
2025-01-08 10:46:05.914081: Epoch 77 
2025-01-08 10:46:05.919611: Current learning rate: 0.00266 
2025-01-08 10:46:45.564460: train_loss -0.7552 
2025-01-08 10:46:45.564460: val_loss -0.736 
2025-01-08 10:46:45.569990: Pseudo dice [np.float32(0.8151)] 
2025-01-08 10:46:45.573999: Epoch time: 39.65 s 
2025-01-08 10:46:45.576532: Yayy! New best EMA pseudo Dice: 0.7620999813079834 
2025-01-08 10:46:46.405581:  
2025-01-08 10:46:46.406584: Epoch 78 
2025-01-08 10:46:46.411141: Current learning rate: 0.00256 
2025-01-08 10:47:26.094770: train_loss -0.7277 
2025-01-08 10:47:26.095272: val_loss -0.6507 
2025-01-08 10:47:26.101287: Pseudo dice [np.float32(0.7362)] 
2025-01-08 10:47:26.104793: Epoch time: 39.69 s 
2025-01-08 10:47:26.714484:  
2025-01-08 10:47:26.714484: Epoch 79 
2025-01-08 10:47:26.720024: Current learning rate: 0.00245 
2025-01-08 10:48:06.355796: train_loss -0.704 
2025-01-08 10:48:06.356298: val_loss -0.7284 
2025-01-08 10:48:06.363371: Pseudo dice [np.float32(0.7869)] 
2025-01-08 10:48:06.367913: Epoch time: 39.64 s 
2025-01-08 10:48:06.370935: Yayy! New best EMA pseudo Dice: 0.7623000144958496 
2025-01-08 10:48:07.202282:  
2025-01-08 10:48:07.202282: Epoch 80 
2025-01-08 10:48:07.207323: Current learning rate: 0.00235 
2025-01-08 10:48:46.906001: train_loss -0.7251 
2025-01-08 10:48:46.907504: val_loss -0.6414 
2025-01-08 10:48:46.912521: Pseudo dice [np.float32(0.7137)] 
2025-01-08 10:48:46.916031: Epoch time: 39.7 s 
2025-01-08 10:48:47.658641:  
2025-01-08 10:48:47.659673: Epoch 81 
2025-01-08 10:48:47.664205: Current learning rate: 0.00224 
2025-01-08 10:49:27.322391: train_loss -0.7422 
2025-01-08 10:49:27.322391: val_loss -0.6234 
2025-01-08 10:49:27.328917: Pseudo dice [np.float32(0.7088)] 
2025-01-08 10:49:27.331426: Epoch time: 39.66 s 
2025-01-08 10:49:27.921508:  
2025-01-08 10:49:27.921508: Epoch 82 
2025-01-08 10:49:27.926540: Current learning rate: 0.00214 
2025-01-08 10:50:07.586608: train_loss -0.7349 
2025-01-08 10:50:07.586608: val_loss -0.695 
2025-01-08 10:50:07.594132: Pseudo dice [np.float32(0.761)] 
2025-01-08 10:50:07.598643: Epoch time: 39.67 s 
2025-01-08 10:50:08.169564:  
2025-01-08 10:50:08.169564: Epoch 83 
2025-01-08 10:50:08.175117: Current learning rate: 0.00203 
2025-01-08 10:50:47.857581: train_loss -0.7249 
2025-01-08 10:50:47.858092: val_loss -0.7077 
2025-01-08 10:50:47.864333: Pseudo dice [np.float32(0.7731)] 
2025-01-08 10:50:47.867364: Epoch time: 39.69 s 
2025-01-08 10:50:48.428946:  
2025-01-08 10:50:48.429451: Epoch 84 
2025-01-08 10:50:48.432961: Current learning rate: 0.00192 
2025-01-08 10:51:28.116302: train_loss -0.7488 
2025-01-08 10:51:28.117302: val_loss -0.6991 
2025-01-08 10:51:28.122818: Pseudo dice [np.float32(0.7305)] 
2025-01-08 10:51:28.126329: Epoch time: 39.69 s 
2025-01-08 10:51:28.682108:  
2025-01-08 10:51:28.682611: Epoch 85 
2025-01-08 10:51:28.687625: Current learning rate: 0.00181 
2025-01-08 10:52:08.363223: train_loss -0.7642 
2025-01-08 10:52:08.364227: val_loss -0.5927 
2025-01-08 10:52:08.369242: Pseudo dice [np.float32(0.6581)] 
2025-01-08 10:52:08.372272: Epoch time: 39.68 s 
2025-01-08 10:52:08.940429:  
2025-01-08 10:52:08.940429: Epoch 86 
2025-01-08 10:52:08.945441: Current learning rate: 0.0017 
2025-01-08 10:52:48.627306: train_loss -0.7569 
2025-01-08 10:52:48.628337: val_loss -0.75 
2025-01-08 10:52:48.633396: Pseudo dice [np.float32(0.804)] 
2025-01-08 10:52:48.636935: Epoch time: 39.69 s 
2025-01-08 10:52:49.198556:  
2025-01-08 10:52:49.198556: Epoch 87 
2025-01-08 10:52:49.202584: Current learning rate: 0.00159 
2025-01-08 10:53:28.903052: train_loss -0.7652 
2025-01-08 10:53:28.903052: val_loss -0.6848 
2025-01-08 10:53:28.909073: Pseudo dice [np.float32(0.7605)] 
2025-01-08 10:53:28.912580: Epoch time: 39.71 s 
2025-01-08 10:53:29.463347:  
2025-01-08 10:53:29.463347: Epoch 88 
2025-01-08 10:53:29.468453: Current learning rate: 0.00148 
2025-01-08 10:54:09.164519: train_loss -0.7652 
2025-01-08 10:54:09.165022: val_loss -0.653 
2025-01-08 10:54:09.170695: Pseudo dice [np.float32(0.7405)] 
2025-01-08 10:54:09.174110: Epoch time: 39.7 s 
2025-01-08 10:54:09.895862:  
2025-01-08 10:54:09.895862: Epoch 89 
2025-01-08 10:54:09.900918: Current learning rate: 0.00137 
2025-01-08 10:54:49.591431: train_loss -0.7449 
2025-01-08 10:54:49.591941: val_loss -0.7723 
2025-01-08 10:54:49.598086: Pseudo dice [np.float32(0.8333)] 
2025-01-08 10:54:49.601138: Epoch time: 39.7 s 
2025-01-08 10:54:50.147475:  
2025-01-08 10:54:50.147475: Epoch 90 
2025-01-08 10:54:50.152488: Current learning rate: 0.00126 
2025-01-08 10:55:29.819541: train_loss -0.7662 
2025-01-08 10:55:29.820541: val_loss -0.678 
2025-01-08 10:55:29.826057: Pseudo dice [np.float32(0.7787)] 
2025-01-08 10:55:29.829569: Epoch time: 39.67 s 
2025-01-08 10:55:30.382295:  
2025-01-08 10:55:30.383300: Epoch 91 
2025-01-08 10:55:30.388353: Current learning rate: 0.00115 
2025-01-08 10:56:10.072388: train_loss -0.7718 
2025-01-08 10:56:10.073393: val_loss -0.6429 
2025-01-08 10:56:10.079411: Pseudo dice [np.float32(0.7372)] 
2025-01-08 10:56:10.082426: Epoch time: 39.69 s 
2025-01-08 10:56:10.637439:  
2025-01-08 10:56:10.637439: Epoch 92 
2025-01-08 10:56:10.642474: Current learning rate: 0.00103 
2025-01-08 10:56:50.358751: train_loss -0.7701 
2025-01-08 10:56:50.359778: val_loss -0.6796 
2025-01-08 10:56:50.365375: Pseudo dice [np.float32(0.7644)] 
2025-01-08 10:56:50.368397: Epoch time: 39.72 s 
2025-01-08 10:56:50.921109:  
2025-01-08 10:56:50.921109: Epoch 93 
2025-01-08 10:56:50.927173: Current learning rate: 0.00091 
2025-01-08 10:57:30.580033: train_loss -0.7676 
2025-01-08 10:57:30.581038: val_loss -0.6881 
2025-01-08 10:57:30.587554: Pseudo dice [np.float32(0.7398)] 
2025-01-08 10:57:30.591571: Epoch time: 39.66 s 
2025-01-08 10:57:31.143970:  
2025-01-08 10:57:31.143970: Epoch 94 
2025-01-08 10:57:31.149499: Current learning rate: 0.00079 
2025-01-08 10:58:10.844269: train_loss -0.7703 
2025-01-08 10:58:10.845269: val_loss -0.6525 
2025-01-08 10:58:10.850788: Pseudo dice [np.float32(0.7236)] 
2025-01-08 10:58:10.854299: Epoch time: 39.7 s 
2025-01-08 10:58:11.416062:  
2025-01-08 10:58:11.416566: Epoch 95 
2025-01-08 10:58:11.420077: Current learning rate: 0.00067 
2025-01-08 10:58:51.128714: train_loss -0.7774 
2025-01-08 10:58:51.129717: val_loss -0.7692 
2025-01-08 10:58:51.135732: Pseudo dice [np.float32(0.8192)] 
2025-01-08 10:58:51.138742: Epoch time: 39.71 s 
2025-01-08 10:58:51.689853:  
2025-01-08 10:58:51.689853: Epoch 96 
2025-01-08 10:58:51.694912: Current learning rate: 0.00055 
2025-01-08 10:59:31.374365: train_loss -0.7725 
2025-01-08 10:59:31.374365: val_loss -0.6519 
2025-01-08 10:59:31.380388: Pseudo dice [np.float32(0.7168)] 
2025-01-08 10:59:31.383897: Epoch time: 39.69 s 
2025-01-08 10:59:32.097950:  
2025-01-08 10:59:32.097950: Epoch 97 
2025-01-08 10:59:32.104008: Current learning rate: 0.00043 
2025-01-08 11:00:11.781885: train_loss -0.7708 
2025-01-08 11:00:11.782889: val_loss -0.6876 
2025-01-08 11:00:11.788926: Pseudo dice [np.float32(0.761)] 
2025-01-08 11:00:11.791936: Epoch time: 39.68 s 
2025-01-08 11:00:12.364438:  
2025-01-08 11:00:12.364952: Epoch 98 
2025-01-08 11:00:12.370509: Current learning rate: 0.0003 
2025-01-08 11:00:52.066754: train_loss -0.7539 
2025-01-08 11:00:52.067759: val_loss -0.6714 
2025-01-08 11:00:52.072779: Pseudo dice [np.float32(0.757)] 
2025-01-08 11:00:52.079300: Epoch time: 39.7 s 
2025-01-08 11:00:52.647145:  
2025-01-08 11:00:52.648145: Epoch 99 
2025-01-08 11:00:52.652694: Current learning rate: 0.00016 
2025-01-08 11:01:32.347932: train_loss -0.7598 
2025-01-08 11:01:32.347932: val_loss -0.6548 
2025-01-08 11:01:32.354452: Pseudo dice [np.float32(0.7572)] 
2025-01-08 11:01:32.357962: Epoch time: 39.7 s 
2025-01-08 11:01:33.137244: Training done. 
2025-01-08 11:01:33.163762: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset006_Lung\splits_final.json 
2025-01-08 11:01:33.171761: The split file contains 5 splits. 
2025-01-08 11:01:33.177761: Desired fold for training: 0 
2025-01-08 11:01:33.181761: This split has 50 training and 13 validation cases. 
2025-01-08 11:01:33.186761: predicting lung_006 
2025-01-08 11:01:33.190761: lung_006, shape torch.Size([1, 354, 500, 500]), rank 0 
2025-01-08 11:02:17.832542: predicting lung_010 
2025-01-08 11:02:17.867542: lung_010, shape torch.Size([1, 301, 306, 306]), rank 0 
2025-01-08 11:02:28.385745: predicting lung_033 
2025-01-08 11:02:28.402251: lung_033, shape torch.Size([1, 324, 420, 420]), rank 0 
2025-01-08 11:02:54.574736: predicting lung_034 
2025-01-08 11:02:54.598740: lung_034, shape torch.Size([1, 369, 460, 460]), rank 0 
2025-01-08 11:03:31.231107: predicting lung_041 
2025-01-08 11:03:31.262613: lung_041, shape torch.Size([1, 299, 420, 420]), rank 0 
2025-01-08 11:03:57.481228: predicting lung_042 
2025-01-08 11:03:57.505226: lung_042, shape torch.Size([1, 312, 375, 375]), rank 0 
2025-01-08 11:04:14.976703: predicting lung_046 
2025-01-08 11:04:14.995701: lung_046, shape torch.Size([1, 281, 400, 400]), rank 0 
2025-01-08 11:04:35.974297: predicting lung_048 
2025-01-08 11:04:35.993297: lung_048, shape torch.Size([1, 322, 417, 417]), rank 0 
2025-01-08 11:05:02.140126: predicting lung_059 
2025-01-08 11:05:02.163130: lung_059, shape torch.Size([1, 271, 420, 420]), rank 0 
2025-01-08 11:05:23.166813: predicting lung_065 
2025-01-08 11:05:23.188324: lung_065, shape torch.Size([1, 320, 372, 372]), rank 0 
2025-01-08 11:05:40.659501: predicting lung_066 
2025-01-08 11:05:40.678501: lung_066, shape torch.Size([1, 300, 454, 454]), rank 0 
2025-01-08 11:06:11.162150: predicting lung_070 
2025-01-08 11:06:11.197151: lung_070, shape torch.Size([1, 331, 390, 390]), rank 0 
2025-01-08 11:06:32.191787: predicting lung_079 
2025-01-08 11:06:32.213787: lung_079, shape torch.Size([1, 312, 476, 476]), rank 0 
2025-01-08 11:07:14.104646: Validation complete 
2025-01-08 11:07:14.104646: Mean Validation Dice:  0.6544925390953984 
