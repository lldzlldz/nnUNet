
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-05 16:16:49.542558: do_dummy_2d_data_aug: False 
2025-01-05 16:16:49.547475: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset006_Lung\splits_final.json 
2025-01-05 16:16:49.552478: The split file contains 5 splits. 
2025-01-05 16:16:49.555477: Desired fold for training: 0 
2025-01-05 16:16:49.557478: This split has 50 training and 13 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_avg_spacing_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [324.0, 416.0, 416.0], 'spacing': [0.9673181374867758, 0.9673181374867758, 0.9673181374867758], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset006_Lung', 'plans_name': 'nnUNetPlans_avg_spacing', 'original_median_spacing_after_transp': [1.244979977607727, 0.78515625, 0.78515625], 'original_median_shape_after_transp': [252, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -273.4598083496094, 'median': -162.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 311.0, 'std': 346.9495849609375}}} 
 
2025-01-05 16:17:02.396856: unpacking dataset... 
2025-01-05 16:17:18.474984: unpacking done... 
2025-01-05 16:17:23.084108:  
2025-01-05 16:17:23.084108: Epoch 0 
2025-01-05 16:17:23.089121: Current learning rate: 0.01 
2025-01-05 16:18:07.426083: train_loss 0.0457 
2025-01-05 16:18:07.426606: val_loss -0.1343 
2025-01-05 16:18:07.431728: Pseudo dice [np.float32(0.0)] 
2025-01-05 16:18:07.437931: Epoch time: 44.34 s 
2025-01-05 16:18:07.442516: Yayy! New best EMA pseudo Dice: 0.0 
2025-01-05 16:18:08.140308:  
2025-01-05 16:18:08.141309: Epoch 1 
2025-01-05 16:18:08.146919: Current learning rate: 0.00998 
2025-01-05 16:18:47.980629: train_loss -0.2011 
2025-01-05 16:18:47.980629: val_loss -0.3451 
2025-01-05 16:18:47.987145: Pseudo dice [np.float32(0.4442)] 
2025-01-05 16:18:47.992156: Epoch time: 39.84 s 
2025-01-05 16:18:47.995663: Yayy! New best EMA pseudo Dice: 0.04439999908208847 
2025-01-05 16:18:48.730595:  
2025-01-05 16:18:48.730595: Epoch 2 
2025-01-05 16:18:48.736110: Current learning rate: 0.00996 
2025-01-05 16:19:28.557262: train_loss -0.3132 
2025-01-05 16:19:28.557771: val_loss -0.4594 
2025-01-05 16:19:28.563902: Pseudo dice [np.float32(0.5119)] 
2025-01-05 16:19:28.566933: Epoch time: 39.83 s 
2025-01-05 16:19:28.571521: Yayy! New best EMA pseudo Dice: 0.09120000153779984 
2025-01-05 16:19:29.353630:  
2025-01-05 16:19:29.353630: Epoch 3 
2025-01-05 16:19:29.359675: Current learning rate: 0.00995 
2025-01-05 16:20:09.522731: train_loss -0.362 
2025-01-05 16:20:09.523733: val_loss -0.5696 
2025-01-05 16:20:09.529246: Pseudo dice [np.float32(0.5827)] 
2025-01-05 16:20:09.531752: Epoch time: 40.17 s 
2025-01-05 16:20:09.535261: Yayy! New best EMA pseudo Dice: 0.14030000567436218 
2025-01-05 16:20:10.342256:  
2025-01-05 16:20:10.342256: Epoch 4 
2025-01-05 16:20:10.347284: Current learning rate: 0.00993 
2025-01-05 16:20:50.353044: train_loss -0.4255 
2025-01-05 16:20:50.353044: val_loss -0.5737 
2025-01-05 16:20:50.359060: Pseudo dice [np.float32(0.6228)] 
2025-01-05 16:20:50.362565: Epoch time: 40.01 s 
2025-01-05 16:20:50.365573: Yayy! New best EMA pseudo Dice: 0.18860000371932983 
2025-01-05 16:20:51.268655:  
2025-01-05 16:20:51.269158: Epoch 5 
2025-01-05 16:20:51.272667: Current learning rate: 0.00991 
2025-01-05 16:21:32.093805: train_loss -0.5183 
2025-01-05 16:21:32.093805: val_loss -0.6184 
2025-01-05 16:21:32.101417: Pseudo dice [np.float32(0.6385)] 
2025-01-05 16:21:32.105459: Epoch time: 40.83 s 
2025-01-05 16:21:32.108499: Yayy! New best EMA pseudo Dice: 0.23360000550746918 
2025-01-05 16:21:32.867513:  
2025-01-05 16:21:32.868518: Epoch 6 
2025-01-05 16:21:32.873637: Current learning rate: 0.00989 
2025-01-05 16:22:13.853664: train_loss -0.4639 
2025-01-05 16:22:13.854667: val_loss -0.6361 
2025-01-05 16:22:13.859616: Pseudo dice [np.float32(0.6994)] 
2025-01-05 16:22:13.863633: Epoch time: 40.99 s 
2025-01-05 16:22:13.867144: Yayy! New best EMA pseudo Dice: 0.2800999879837036 
2025-01-05 16:22:14.632926:  
2025-01-05 16:22:14.633926: Epoch 7 
2025-01-05 16:22:14.639567: Current learning rate: 0.00987 
2025-01-05 16:22:54.654683: train_loss -0.4422 
2025-01-05 16:22:54.655684: val_loss -0.542 
2025-01-05 16:22:54.661199: Pseudo dice [np.float32(0.557)] 
2025-01-05 16:22:54.663705: Epoch time: 40.02 s 
2025-01-05 16:22:54.667214: Yayy! New best EMA pseudo Dice: 0.3077999949455261 
2025-01-05 16:22:55.420768:  
2025-01-05 16:22:55.421771: Epoch 8 
2025-01-05 16:22:55.426900: Current learning rate: 0.00986 
2025-01-05 16:23:35.947965: train_loss -0.4604 
2025-01-05 16:23:35.947965: val_loss -0.5944 
2025-01-05 16:23:35.955664: Pseudo dice [np.float32(0.6439)] 
2025-01-05 16:23:35.958734: Epoch time: 40.53 s 
2025-01-05 16:23:35.961268: Yayy! New best EMA pseudo Dice: 0.34139999747276306 
2025-01-05 16:23:36.767327:  
2025-01-05 16:23:36.767327: Epoch 9 
2025-01-05 16:23:36.773344: Current learning rate: 0.00984 
2025-01-05 16:24:17.660411: train_loss -0.4687 
2025-01-05 16:24:17.661414: val_loss -0.5486 
2025-01-05 16:24:17.666934: Pseudo dice [np.float32(0.608)] 
2025-01-05 16:24:17.670448: Epoch time: 40.89 s 
2025-01-05 16:24:17.674463: Yayy! New best EMA pseudo Dice: 0.36809998750686646 
2025-01-05 16:24:18.467301:  
2025-01-05 16:24:18.467301: Epoch 10 
2025-01-05 16:24:18.473315: Current learning rate: 0.00982 
