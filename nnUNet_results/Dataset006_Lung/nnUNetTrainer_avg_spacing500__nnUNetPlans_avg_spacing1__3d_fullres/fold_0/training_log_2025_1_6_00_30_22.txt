
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-06 00:30:22.977375: do_dummy_2d_data_aug: False 
2025-01-06 00:30:22.983372: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset006_Lung\splits_final.json 
2025-01-06 00:30:22.989371: The split file contains 5 splits. 
2025-01-06 00:30:22.991370: Desired fold for training: 0 
2025-01-06 00:30:22.994370: This split has 50 training and 13 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_avg_spacing1_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.244979977607727, 0.78515625, 0.78515625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset006_Lung', 'plans_name': 'nnUNetPlans_avg_spacing1', 'original_median_spacing_after_transp': [1.244979977607727, 0.78515625, 0.78515625], 'original_median_shape_after_transp': [252, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -273.4598083496094, 'median': -162.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 311.0, 'std': 346.9495849609375}}} 
 
2025-01-06 00:30:34.861212: unpacking dataset... 
2025-01-06 00:30:48.646737: unpacking done... 
2025-01-06 00:30:53.370420:  
2025-01-06 00:30:53.370420: Epoch 0 
2025-01-06 00:30:53.375432: Current learning rate: 0.01 
2025-01-06 00:31:40.516377: train_loss 0.0742 
2025-01-06 00:31:40.516377: val_loss -0.0454 
2025-01-06 00:31:40.522621: Pseudo dice [np.float32(0.0)] 
2025-01-06 00:31:40.526192: Epoch time: 47.15 s 
2025-01-06 00:31:40.528285: Yayy! New best EMA pseudo Dice: 0.0 
2025-01-06 00:31:41.217990:  
2025-01-06 00:31:41.218496: Epoch 1 
2025-01-06 00:31:41.223509: Current learning rate: 0.00998 
2025-01-06 00:32:23.962351: train_loss -0.1271 
2025-01-06 00:32:23.962351: val_loss -0.4232 
2025-01-06 00:32:23.968375: Pseudo dice [np.float32(0.359)] 
2025-01-06 00:32:23.971387: Epoch time: 42.75 s 
2025-01-06 00:32:23.974898: Yayy! New best EMA pseudo Dice: 0.03590000048279762 
2025-01-06 00:32:24.754732:  
2025-01-06 00:32:24.755235: Epoch 2 
2025-01-06 00:32:24.760288: Current learning rate: 0.00996 
2025-01-06 00:33:07.361307: train_loss -0.3201 
2025-01-06 00:33:07.362310: val_loss -0.4064 
2025-01-06 00:33:07.368439: Pseudo dice [np.float32(0.461)] 
2025-01-06 00:33:07.371067: Epoch time: 42.61 s 
2025-01-06 00:33:07.373615: Yayy! New best EMA pseudo Dice: 0.07840000092983246 
2025-01-06 00:33:08.190107:  
2025-01-06 00:33:08.191623: Epoch 3 
2025-01-06 00:33:08.197186: Current learning rate: 0.00995 
2025-01-06 00:33:50.789227: train_loss -0.3526 
2025-01-06 00:33:50.790227: val_loss -0.5038 
2025-01-06 00:33:50.795741: Pseudo dice [np.float32(0.5103)] 
2025-01-06 00:33:50.799249: Epoch time: 42.6 s 
2025-01-06 00:33:50.802259: Yayy! New best EMA pseudo Dice: 0.12160000205039978 
2025-01-06 00:33:51.612268:  
2025-01-06 00:33:51.613270: Epoch 4 
2025-01-06 00:33:51.618410: Current learning rate: 0.00993 
2025-01-06 00:34:34.230606: train_loss -0.3966 
2025-01-06 00:34:34.231112: val_loss -0.5861 
2025-01-06 00:34:34.237128: Pseudo dice [np.float32(0.5402)] 
2025-01-06 00:34:34.241140: Epoch time: 42.62 s 
2025-01-06 00:34:34.243648: Yayy! New best EMA pseudo Dice: 0.16349999606609344 
2025-01-06 00:34:35.172012:  
2025-01-06 00:34:35.172012: Epoch 5 
2025-01-06 00:34:35.178028: Current learning rate: 0.00991 
2025-01-06 00:35:17.779050: train_loss -0.4343 
2025-01-06 00:35:17.779558: val_loss -0.6151 
2025-01-06 00:35:17.785420: Pseudo dice [np.float32(0.6788)] 
2025-01-06 00:35:17.789001: Epoch time: 42.61 s 
2025-01-06 00:35:17.792140: Yayy! New best EMA pseudo Dice: 0.2150000035762787 
2025-01-06 00:35:18.609872:  
2025-01-06 00:35:18.609872: Epoch 6 
2025-01-06 00:35:18.615896: Current learning rate: 0.00989 
2025-01-06 00:36:01.219205: train_loss -0.4962 
2025-01-06 00:36:01.220708: val_loss -0.5908 
2025-01-06 00:36:01.226238: Pseudo dice [np.float32(0.6369)] 
2025-01-06 00:36:01.229310: Epoch time: 42.61 s 
2025-01-06 00:36:01.232815: Yayy! New best EMA pseudo Dice: 0.2572000026702881 
2025-01-06 00:36:02.022513:  
2025-01-06 00:36:02.022513: Epoch 7 
2025-01-06 00:36:02.026554: Current learning rate: 0.00987 
2025-01-06 00:36:44.650706: train_loss -0.4316 
2025-01-06 00:36:44.651709: val_loss -0.59 
2025-01-06 00:36:44.657243: Pseudo dice [np.float32(0.6138)] 
2025-01-06 00:36:44.660754: Epoch time: 42.63 s 
2025-01-06 00:36:44.663783: Yayy! New best EMA pseudo Dice: 0.2928999960422516 
2025-01-06 00:36:45.481407:  
2025-01-06 00:36:45.481909: Epoch 8 
2025-01-06 00:36:45.485419: Current learning rate: 0.00986 
2025-01-06 00:37:33.489331: train_loss -0.5209 
2025-01-06 00:37:33.490334: val_loss -0.6216 
2025-01-06 00:37:33.495858: Pseudo dice [np.float32(0.6612)] 
2025-01-06 00:37:33.499372: Epoch time: 48.01 s 
2025-01-06 00:37:33.502886: Yayy! New best EMA pseudo Dice: 0.3296999931335449 
2025-01-06 00:37:34.338782:  
2025-01-06 00:37:34.339784: Epoch 9 
2025-01-06 00:37:34.344802: Current learning rate: 0.00984 
