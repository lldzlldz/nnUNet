
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-02 15:36:38.588722: do_dummy_2d_data_aug: False 
2025-01-02 15:36:38.589725: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset006_Lung\splits_final.json 
2025-01-02 15:36:38.597728: The split file contains 5 splits. 
2025-01-02 15:36:38.599726: Desired fold for training: 1 
2025-01-02 15:36:38.602725: This split has 50 training and 13 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.244979977607727, 0.78515625, 0.78515625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset006_Lung', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.244979977607727, 0.78515625, 0.78515625], 'original_median_shape_after_transp': [252, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -273.4598083496094, 'median': -162.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 311.0, 'std': 346.9495849609375}}} 
 
2025-01-02 15:36:46.552149: unpacking dataset... 
2025-01-02 15:36:46.795715: unpacking done... 
2025-01-02 15:36:51.253485:  
2025-01-02 15:36:51.253485: Epoch 50 
2025-01-02 15:36:51.258509: Current learning rate: 0.00536 
2025-01-02 15:37:38.488463: train_loss -0.7021 
2025-01-02 15:37:38.489468: val_loss -0.4659 
2025-01-02 15:37:38.495981: Pseudo dice [np.float32(0.3143)] 
2025-01-02 15:37:38.498488: Epoch time: 47.24 s 
2025-01-02 15:37:38.501997: Yayy! New best EMA pseudo Dice: 0.29429998993873596 
2025-01-02 15:37:39.297367:  
2025-01-02 15:37:39.297869: Epoch 51 
2025-01-02 15:37:39.302879: Current learning rate: 0.00526 
2025-01-02 15:38:23.292399: train_loss -0.6865 
2025-01-02 15:38:23.292907: val_loss -0.4711 
2025-01-02 15:38:23.298004: Pseudo dice [np.float32(0.2627)] 
2025-01-02 15:38:23.301530: Epoch time: 44.0 s 
2025-01-02 15:38:23.901558:  
2025-01-02 15:38:23.901558: Epoch 52 
2025-01-02 15:38:23.909089: Current learning rate: 0.00517 
2025-01-02 15:39:08.757426: train_loss -0.7165 
2025-01-02 15:39:08.758437: val_loss -0.5106 
2025-01-02 15:39:08.763561: Pseudo dice [np.float32(0.5377)] 
2025-01-02 15:39:08.767758: Epoch time: 44.86 s 
2025-01-02 15:39:08.770843: Yayy! New best EMA pseudo Dice: 0.3158000111579895 
2025-01-02 15:39:09.565536:  
2025-01-02 15:39:09.565536: Epoch 53 
2025-01-02 15:39:09.571108: Current learning rate: 0.00507 
2025-01-02 15:39:54.712423: train_loss -0.7026 
2025-01-02 15:39:54.713427: val_loss -0.5111 
2025-01-02 15:39:54.718445: Pseudo dice [np.float32(0.4816)] 
2025-01-02 15:39:54.721953: Epoch time: 45.15 s 
2025-01-02 15:39:54.725493: Yayy! New best EMA pseudo Dice: 0.33239999413490295 
2025-01-02 15:39:55.556830:  
2025-01-02 15:39:55.556830: Epoch 54 
2025-01-02 15:39:55.562847: Current learning rate: 0.00497 
2025-01-02 15:40:39.802604: train_loss -0.7123 
2025-01-02 15:40:39.803609: val_loss -0.4687 
2025-01-02 15:40:39.808623: Pseudo dice [np.float32(0.3332)] 
2025-01-02 15:40:39.812634: Epoch time: 44.25 s 
2025-01-02 15:40:39.815141: Yayy! New best EMA pseudo Dice: 0.33250001072883606 
2025-01-02 15:40:40.866385:  
2025-01-02 15:40:40.867390: Epoch 55 
2025-01-02 15:40:40.872452: Current learning rate: 0.00487 
2025-01-02 15:41:24.989040: train_loss -0.6905 
2025-01-02 15:41:24.989544: val_loss -0.3842 
2025-01-02 15:41:24.995571: Pseudo dice [np.float32(0.2916)] 
2025-01-02 15:41:24.999589: Epoch time: 44.12 s 
2025-01-02 15:41:25.567253:  
2025-01-02 15:41:25.567253: Epoch 56 
2025-01-02 15:41:25.573373: Current learning rate: 0.00478 
2025-01-02 15:42:11.052841: train_loss -0.6911 
2025-01-02 15:42:11.053841: val_loss -0.3568 
2025-01-02 15:42:11.059363: Pseudo dice [np.float32(0.1868)] 
2025-01-02 15:42:11.062875: Epoch time: 45.49 s 
2025-01-02 15:42:11.638921:  
2025-01-02 15:42:11.639926: Epoch 57 
2025-01-02 15:42:11.644739: Current learning rate: 0.00468 
2025-01-02 15:42:56.838746: train_loss -0.7126 
2025-01-02 15:42:56.839770: val_loss -0.3844 
2025-01-02 15:42:56.844889: Pseudo dice [np.float32(0.346)] 
2025-01-02 15:42:56.849997: Epoch time: 45.2 s 
2025-01-02 15:42:57.538130:  
2025-01-02 15:42:57.539632: Epoch 58 
2025-01-02 15:42:57.545653: Current learning rate: 0.00458 
2025-01-02 15:43:41.607705: train_loss -0.7431 
2025-01-02 15:43:41.607705: val_loss -0.4427 
2025-01-02 15:43:41.613720: Pseudo dice [np.float32(0.3993)] 
2025-01-02 15:43:41.617733: Epoch time: 44.07 s 
2025-01-02 15:43:42.189706:  
2025-01-02 15:43:42.189706: Epoch 59 
2025-01-02 15:43:42.194721: Current learning rate: 0.00448 
2025-01-02 15:44:26.001634: train_loss -0.7241 
2025-01-02 15:44:26.001634: val_loss -0.3932 
2025-01-02 15:44:26.008148: Pseudo dice [np.float32(0.3251)] 
2025-01-02 15:44:26.010655: Epoch time: 43.81 s 
2025-01-02 15:44:26.666593:  
2025-01-02 15:44:26.666593: Epoch 60 
2025-01-02 15:44:26.672610: Current learning rate: 0.00438 
2025-01-02 15:45:10.816781: train_loss -0.7164 
2025-01-02 15:45:10.817779: val_loss -0.2604 
2025-01-02 15:45:10.823296: Pseudo dice [np.float32(0.2036)] 
2025-01-02 15:45:10.826807: Epoch time: 44.15 s 
2025-01-02 15:45:11.404434:  
2025-01-02 15:45:11.404434: Epoch 61 
2025-01-02 15:45:11.409448: Current learning rate: 0.00429 
2025-01-02 15:45:55.142991: train_loss -0.7136 
2025-01-02 15:45:55.143495: val_loss -0.5056 
2025-01-02 15:45:55.149515: Pseudo dice [np.float32(0.509)] 
2025-01-02 15:45:55.153524: Epoch time: 43.74 s 
2025-01-02 15:45:55.156030: Yayy! New best EMA pseudo Dice: 0.3328999876976013 
2025-01-02 15:45:55.934441:  
2025-01-02 15:45:55.935945: Epoch 62 
2025-01-02 15:45:55.940958: Current learning rate: 0.00419 
2025-01-02 15:46:40.631472: train_loss -0.7139 
2025-01-02 15:46:40.631472: val_loss -0.4521 
2025-01-02 15:46:40.638071: Pseudo dice [np.float32(0.369)] 
2025-01-02 15:46:40.641125: Epoch time: 44.7 s 
2025-01-02 15:46:40.644210: Yayy! New best EMA pseudo Dice: 0.33649998903274536 
2025-01-02 15:46:41.617364:  
2025-01-02 15:46:41.618368: Epoch 63 
2025-01-02 15:46:41.623404: Current learning rate: 0.00409 
2025-01-02 15:47:26.306224: train_loss -0.735 
2025-01-02 15:47:26.306224: val_loss -0.4914 
2025-01-02 15:47:26.312245: Pseudo dice [np.float32(0.4725)] 
2025-01-02 15:47:26.315754: Epoch time: 44.69 s 
2025-01-02 15:47:26.318870: Yayy! New best EMA pseudo Dice: 0.35010001063346863 
2025-01-02 15:47:27.138811:  
2025-01-02 15:47:27.139815: Epoch 64 
2025-01-02 15:47:27.143892: Current learning rate: 0.00399 
2025-01-02 15:48:11.624854: train_loss -0.7535 
2025-01-02 15:48:11.624854: val_loss -0.4724 
2025-01-02 15:48:11.631370: Pseudo dice [np.float32(0.5221)] 
2025-01-02 15:48:11.634881: Epoch time: 44.49 s 
2025-01-02 15:48:11.637389: Yayy! New best EMA pseudo Dice: 0.36730000376701355 
2025-01-02 15:48:12.461626:  
2025-01-02 15:48:12.461626: Epoch 65 
2025-01-02 15:48:12.465137: Current learning rate: 0.00389 
2025-01-02 15:48:55.884509: train_loss -0.7361 
2025-01-02 15:48:55.885513: val_loss -0.6521 
2025-01-02 15:48:55.890583: Pseudo dice [np.float32(0.6706)] 
2025-01-02 15:48:55.894166: Epoch time: 43.42 s 
2025-01-02 15:48:55.896722: Yayy! New best EMA pseudo Dice: 0.3977000117301941 
2025-01-02 15:48:56.741389:  
2025-01-02 15:48:56.741389: Epoch 66 
2025-01-02 15:48:56.745408: Current learning rate: 0.00379 
2025-01-02 15:49:40.211623: train_loss -0.7406 
2025-01-02 15:49:40.211623: val_loss -0.4164 
2025-01-02 15:49:40.216635: Pseudo dice [np.float32(0.3757)] 
2025-01-02 15:49:40.220145: Epoch time: 43.47 s 
2025-01-02 15:49:40.799657:  
2025-01-02 15:49:40.800660: Epoch 67 
2025-01-02 15:49:40.805747: Current learning rate: 0.00369 
2025-01-02 15:50:24.420891: train_loss -0.7254 
2025-01-02 15:50:24.420891: val_loss -0.4454 
2025-01-02 15:50:24.426915: Pseudo dice [np.float32(0.4582)] 
2025-01-02 15:50:24.431930: Epoch time: 43.62 s 
2025-01-02 15:50:24.435942: Yayy! New best EMA pseudo Dice: 0.4016999900341034 
2025-01-02 15:50:25.272529:  
2025-01-02 15:50:25.272529: Epoch 68 
2025-01-02 15:50:25.277546: Current learning rate: 0.00359 
2025-01-02 15:51:09.110908: train_loss -0.7394 
2025-01-02 15:51:09.111412: val_loss -0.3345 
2025-01-02 15:51:09.116428: Pseudo dice [np.float32(0.3682)] 
2025-01-02 15:51:09.119939: Epoch time: 43.84 s 
2025-01-02 15:51:09.702623:  
2025-01-02 15:51:09.702623: Epoch 69 
2025-01-02 15:51:09.707638: Current learning rate: 0.00349 
2025-01-02 15:51:53.083033: train_loss -0.7413 
2025-01-02 15:51:53.083033: val_loss -0.4491 
2025-01-02 15:51:53.089053: Pseudo dice [np.float32(0.227)] 
2025-01-02 15:51:53.093070: Epoch time: 43.38 s 
2025-01-02 15:51:53.669207:  
2025-01-02 15:51:53.670208: Epoch 70 
2025-01-02 15:51:53.675359: Current learning rate: 0.00338 
2025-01-02 15:52:36.825752: train_loss -0.7267 
2025-01-02 15:52:36.826753: val_loss -0.5092 
2025-01-02 15:52:36.832278: Pseudo dice [np.float32(0.4915)] 
2025-01-02 15:52:36.835788: Epoch time: 43.16 s 
2025-01-02 15:52:37.589566:  
2025-01-02 15:52:37.590069: Epoch 71 
2025-01-02 15:52:37.595092: Current learning rate: 0.00328 
2025-01-02 15:53:22.250836: train_loss -0.7352 
2025-01-02 15:53:22.251840: val_loss -0.446 
2025-01-02 15:53:22.256859: Pseudo dice [np.float32(0.2877)] 
2025-01-02 15:53:22.260366: Epoch time: 44.66 s 
2025-01-02 15:53:22.850312:  
2025-01-02 15:53:22.851312: Epoch 72 
2025-01-02 15:53:22.856210: Current learning rate: 0.00318 
2025-01-02 15:54:06.115195: train_loss -0.7229 
2025-01-02 15:54:06.115195: val_loss -0.5966 
2025-01-02 15:54:06.121210: Pseudo dice [np.float32(0.5094)] 
2025-01-02 15:54:06.124721: Epoch time: 43.26 s 
2025-01-02 15:54:06.730136:  
2025-01-02 15:54:06.730136: Epoch 73 
2025-01-02 15:54:06.735014: Current learning rate: 0.00308 
2025-01-02 15:54:50.527907: train_loss -0.7337 
2025-01-02 15:54:50.529410: val_loss -0.5254 
2025-01-02 15:54:50.534426: Pseudo dice [np.float32(0.3717)] 
2025-01-02 15:54:50.536932: Epoch time: 43.8 s 
2025-01-02 15:54:51.120068:  
2025-01-02 15:54:51.120068: Epoch 74 
2025-01-02 15:54:51.125085: Current learning rate: 0.00297 
2025-01-02 15:55:34.576840: train_loss -0.7463 
2025-01-02 15:55:34.576840: val_loss -0.3504 
2025-01-02 15:55:34.582950: Pseudo dice [np.float32(0.178)] 
2025-01-02 15:55:34.586782: Epoch time: 43.46 s 
2025-01-02 15:55:35.191885:  
2025-01-02 15:55:35.191885: Epoch 75 
2025-01-02 15:55:35.196901: Current learning rate: 0.00287 
2025-01-02 15:56:19.318877: train_loss -0.7225 
2025-01-02 15:56:19.318877: val_loss -0.4302 
2025-01-02 15:56:19.324898: Pseudo dice [np.float32(0.4726)] 
2025-01-02 15:56:19.327407: Epoch time: 44.13 s 
2025-01-02 15:56:19.920500:  
2025-01-02 15:56:19.921003: Epoch 76 
2025-01-02 15:56:19.926022: Current learning rate: 0.00277 
2025-01-02 15:57:03.301947: train_loss -0.7377 
2025-01-02 15:57:03.302951: val_loss -0.4704 
2025-01-02 15:57:03.308476: Pseudo dice [np.float32(0.4709)] 
2025-01-02 15:57:03.312027: Epoch time: 43.38 s 
2025-01-02 15:57:03.901429:  
2025-01-02 15:57:03.901429: Epoch 77 
2025-01-02 15:57:03.906385: Current learning rate: 0.00266 
2025-01-02 15:57:47.477210: train_loss -0.7389 
2025-01-02 15:57:47.478718: val_loss -0.4583 
2025-01-02 15:57:47.484734: Pseudo dice [np.float32(0.4286)] 
2025-01-02 15:57:47.487242: Epoch time: 43.58 s 
2025-01-02 15:57:48.073039:  
2025-01-02 15:57:48.073039: Epoch 78 
2025-01-02 15:57:48.078617: Current learning rate: 0.00256 
2025-01-02 15:58:31.460723: train_loss -0.7131 
2025-01-02 15:58:31.460723: val_loss -0.4943 
2025-01-02 15:58:31.467743: Pseudo dice [np.float32(0.4264)] 
2025-01-02 15:58:31.470753: Epoch time: 43.39 s 
2025-01-02 15:58:32.217718:  
2025-01-02 15:58:32.217718: Epoch 79 
2025-01-02 15:58:32.223743: Current learning rate: 0.00245 
2025-01-02 15:59:15.915333: train_loss -0.7437 
2025-01-02 15:59:15.916833: val_loss -0.4633 
2025-01-02 15:59:15.922858: Pseudo dice [np.float32(0.4206)] 
2025-01-02 15:59:15.926867: Epoch time: 43.7 s 
2025-01-02 15:59:16.518322:  
2025-01-02 15:59:16.518322: Epoch 80 
2025-01-02 15:59:16.524343: Current learning rate: 0.00235 
2025-01-02 15:59:59.253584: train_loss -0.7646 
2025-01-02 15:59:59.254588: val_loss -0.4808 
2025-01-02 15:59:59.261105: Pseudo dice [np.float32(0.3638)] 
2025-01-02 15:59:59.264616: Epoch time: 42.74 s 
2025-01-02 15:59:59.863816:  
2025-01-02 15:59:59.863816: Epoch 81 
2025-01-02 15:59:59.867865: Current learning rate: 0.00224 
2025-01-02 16:00:42.584287: train_loss -0.7577 
2025-01-02 16:00:42.584790: val_loss -0.4882 
2025-01-02 16:00:42.591806: Pseudo dice [np.float32(0.3437)] 
2025-01-02 16:00:42.596816: Epoch time: 42.72 s 
2025-01-02 16:00:43.201424:  
2025-01-02 16:00:43.202423: Epoch 82 
2025-01-02 16:00:43.208017: Current learning rate: 0.00214 
2025-01-02 16:01:25.892871: train_loss -0.766 
2025-01-02 16:01:25.892871: val_loss -0.5192 
2025-01-02 16:01:25.899391: Pseudo dice [np.float32(0.4566)] 
2025-01-02 16:01:25.902902: Epoch time: 42.69 s 
2025-01-02 16:01:26.472816:  
2025-01-02 16:01:26.472816: Epoch 83 
2025-01-02 16:01:26.477852: Current learning rate: 0.00203 
2025-01-02 16:02:10.304287: train_loss -0.7719 
2025-01-02 16:02:10.304809: val_loss -0.5529 
2025-01-02 16:02:10.310933: Pseudo dice [np.float32(0.4894)] 
2025-01-02 16:02:10.314546: Epoch time: 43.83 s 
2025-01-02 16:02:10.318118: Yayy! New best EMA pseudo Dice: 0.4065000116825104 
2025-01-02 16:02:11.134833:  
2025-01-02 16:02:11.135335: Epoch 84 
2025-01-02 16:02:11.140347: Current learning rate: 0.00192 
2025-01-02 16:02:55.555881: train_loss -0.7552 
2025-01-02 16:02:55.555881: val_loss -0.4773 
2025-01-02 16:02:55.562397: Pseudo dice [np.float32(0.3986)] 
2025-01-02 16:02:55.566910: Epoch time: 44.42 s 
2025-01-02 16:02:56.142549:  
2025-01-02 16:02:56.142549: Epoch 85 
2025-01-02 16:02:56.148623: Current learning rate: 0.00181 
2025-01-02 16:03:40.678002: train_loss -0.7486 
2025-01-02 16:03:40.679002: val_loss -0.2367 
2025-01-02 16:03:40.684012: Pseudo dice [np.float32(0.2133)] 
2025-01-02 16:03:40.687021: Epoch time: 44.54 s 
2025-01-02 16:03:41.290110:  
2025-01-02 16:03:41.291114: Epoch 86 
2025-01-02 16:03:41.299706: Current learning rate: 0.0017 
2025-01-02 16:04:25.040382: train_loss -0.7545 
2025-01-02 16:04:25.040885: val_loss -0.4014 
2025-01-02 16:04:25.046901: Pseudo dice [np.float32(0.3084)] 
2025-01-02 16:04:25.050908: Epoch time: 43.75 s 
2025-01-02 16:04:25.759083:  
2025-01-02 16:04:25.759083: Epoch 87 
2025-01-02 16:04:25.764651: Current learning rate: 0.00159 
2025-01-02 16:05:10.197745: train_loss -0.7509 
2025-01-02 16:05:10.198250: val_loss -0.5747 
2025-01-02 16:05:10.204295: Pseudo dice [np.float32(0.5154)] 
2025-01-02 16:05:10.207815: Epoch time: 44.44 s 
2025-01-02 16:05:10.765172:  
2025-01-02 16:05:10.765674: Epoch 88 
2025-01-02 16:05:10.771224: Current learning rate: 0.00148 
2025-01-02 16:05:54.970387: train_loss -0.766 
2025-01-02 16:05:54.970904: val_loss -0.55 
2025-01-02 16:05:54.976505: Pseudo dice [np.float32(0.5714)] 
2025-01-02 16:05:54.980532: Epoch time: 44.21 s 
2025-01-02 16:05:54.983573: Yayy! New best EMA pseudo Dice: 0.41019999980926514 
2025-01-02 16:05:55.790296:  
2025-01-02 16:05:55.790296: Epoch 89 
2025-01-02 16:05:55.796339: Current learning rate: 0.00137 
2025-01-02 16:06:39.512261: train_loss -0.7695 
2025-01-02 16:06:39.513264: val_loss -0.5849 
2025-01-02 16:06:39.519275: Pseudo dice [np.float32(0.4407)] 
2025-01-02 16:06:39.522283: Epoch time: 43.72 s 
2025-01-02 16:06:39.525791: Yayy! New best EMA pseudo Dice: 0.4133000075817108 
2025-01-02 16:06:40.301749:  
2025-01-02 16:06:40.303332: Epoch 90 
2025-01-02 16:06:40.308400: Current learning rate: 0.00126 
2025-01-02 16:07:24.387227: train_loss -0.7507 
2025-01-02 16:07:24.388230: val_loss -0.4606 
2025-01-02 16:07:24.394760: Pseudo dice [np.float32(0.4556)] 
2025-01-02 16:07:24.399273: Epoch time: 44.09 s 
2025-01-02 16:07:24.403286: Yayy! New best EMA pseudo Dice: 0.41749998927116394 
2025-01-02 16:07:25.190396:  
2025-01-02 16:07:25.190898: Epoch 91 
2025-01-02 16:07:25.195910: Current learning rate: 0.00115 
2025-01-02 16:08:09.039494: train_loss -0.757 
2025-01-02 16:08:09.039494: val_loss -0.574 
2025-01-02 16:08:09.046637: Pseudo dice [np.float32(0.5896)] 
2025-01-02 16:08:09.049695: Epoch time: 43.85 s 
2025-01-02 16:08:09.053227: Yayy! New best EMA pseudo Dice: 0.43470001220703125 
2025-01-02 16:08:09.851332:  
2025-01-02 16:08:09.852335: Epoch 92 
2025-01-02 16:08:09.857867: Current learning rate: 0.00103 
2025-01-02 16:08:53.984364: train_loss -0.7407 
2025-01-02 16:08:53.984876: val_loss -0.5006 
2025-01-02 16:08:53.991440: Pseudo dice [np.float32(0.4141)] 
2025-01-02 16:08:53.994978: Epoch time: 44.13 s 
2025-01-02 16:08:54.550955:  
2025-01-02 16:08:54.551457: Epoch 93 
2025-01-02 16:08:54.556468: Current learning rate: 0.00091 
2025-01-02 16:09:38.094073: train_loss -0.7681 
2025-01-02 16:09:38.095076: val_loss -0.4883 
2025-01-02 16:09:38.101591: Pseudo dice [np.float32(0.4226)] 
2025-01-02 16:09:38.105102: Epoch time: 43.54 s 
2025-01-02 16:09:38.666262:  
2025-01-02 16:09:38.667773: Epoch 94 
2025-01-02 16:09:38.672854: Current learning rate: 0.00079 
2025-01-02 16:10:22.054251: train_loss -0.7722 
2025-01-02 16:10:22.055251: val_loss -0.5157 
2025-01-02 16:10:22.060766: Pseudo dice [np.float32(0.5797)] 
2025-01-02 16:10:22.064275: Epoch time: 43.39 s 
2025-01-02 16:10:22.068282: Yayy! New best EMA pseudo Dice: 0.446399986743927 
2025-01-02 16:10:22.884606:  
2025-01-02 16:10:22.885610: Epoch 95 
2025-01-02 16:10:22.891145: Current learning rate: 0.00067 
2025-01-02 16:11:06.471597: train_loss -0.763 
2025-01-02 16:11:06.471597: val_loss -0.5371 
2025-01-02 16:11:06.478614: Pseudo dice [np.float32(0.475)] 
2025-01-02 16:11:06.481621: Epoch time: 43.59 s 
2025-01-02 16:11:06.485131: Yayy! New best EMA pseudo Dice: 0.44929999113082886 
2025-01-02 16:11:07.416720:  
2025-01-02 16:11:07.416720: Epoch 96 
2025-01-02 16:11:07.422750: Current learning rate: 0.00055 
2025-01-02 16:11:51.326139: train_loss -0.7745 
2025-01-02 16:11:51.327139: val_loss -0.5861 
2025-01-02 16:11:51.332150: Pseudo dice [np.float32(0.6475)] 
2025-01-02 16:11:51.335669: Epoch time: 43.91 s 
2025-01-02 16:11:51.338700: Yayy! New best EMA pseudo Dice: 0.4690999984741211 
2025-01-02 16:11:52.123489:  
2025-01-02 16:11:52.123489: Epoch 97 
2025-01-02 16:11:52.129507: Current learning rate: 0.00043 
2025-01-02 16:12:36.672381: train_loss -0.7478 
2025-01-02 16:12:36.672381: val_loss -0.5419 
2025-01-02 16:12:36.678896: Pseudo dice [np.float32(0.4409)] 
2025-01-02 16:12:36.682405: Epoch time: 44.55 s 
2025-01-02 16:12:37.248312:  
2025-01-02 16:12:37.248312: Epoch 98 
2025-01-02 16:12:37.253867: Current learning rate: 0.0003 
2025-01-02 16:13:21.431115: train_loss -0.7803 
2025-01-02 16:13:21.431618: val_loss -0.4428 
2025-01-02 16:13:21.437634: Pseudo dice [np.float32(0.3363)] 
2025-01-02 16:13:21.441642: Epoch time: 44.18 s 
2025-01-02 16:13:22.026371:  
2025-01-02 16:13:22.027371: Epoch 99 
2025-01-02 16:13:22.030411: Current learning rate: 0.00016 
2025-01-02 16:14:05.777536: train_loss -0.7692 
2025-01-02 16:14:05.778045: val_loss -0.5533 
2025-01-02 16:14:05.784666: Pseudo dice [np.float32(0.4031)] 
2025-01-02 16:14:05.788209: Epoch time: 43.75 s 
2025-01-02 16:14:06.558971: Training done. 
2025-01-02 16:14:06.595972: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset006_Lung\splits_final.json 
2025-01-02 16:14:06.604972: The split file contains 5 splits. 
2025-01-02 16:14:06.612971: Desired fold for training: 1 
2025-01-02 16:14:06.619971: This split has 50 training and 13 validation cases. 
2025-01-02 16:14:06.629974: predicting lung_004 
2025-01-02 16:14:06.638974: lung_004, shape torch.Size([1, 276, 540, 540]), rank 0 
2025-01-02 16:14:41.833562: predicting lung_015 
2025-01-02 16:14:41.866562: lung_015, shape torch.Size([1, 278, 535, 535]), rank 0 
2025-01-02 16:15:16.341935: predicting lung_022 
2025-01-02 16:15:16.379938: lung_022, shape torch.Size([1, 242, 549, 549]), rank 0 
2025-01-02 16:15:50.863123: predicting lung_031 
2025-01-02 16:15:50.894123: lung_031, shape torch.Size([1, 304, 459, 459]), rank 0 
2025-01-02 16:16:17.836785: predicting lung_036 
2025-01-02 16:16:17.862786: lung_036, shape torch.Size([1, 271, 509, 509]), rank 0 
2025-01-02 16:16:52.153246: predicting lung_038 
2025-01-02 16:16:52.181759: lung_038, shape torch.Size([1, 251, 509, 509]), rank 0 
2025-01-02 16:17:26.593899: predicting lung_053 
2025-01-02 16:17:26.621898: lung_053, shape torch.Size([1, 252, 611, 611]), rank 0 
2025-01-02 16:18:16.326988: predicting lung_062 
2025-01-02 16:18:16.366990: lung_062, shape torch.Size([1, 243, 535, 535]), rank 0 
2025-01-02 16:18:51.187731: predicting lung_064 
2025-01-02 16:18:51.215735: lung_064, shape torch.Size([1, 248, 459, 459]), rank 0 
2025-01-02 16:19:15.131952: predicting lung_069 
2025-01-02 16:19:15.153461: lung_069, shape torch.Size([1, 242, 513, 513]), rank 0 
2025-01-02 16:19:49.676898: predicting lung_071 
2025-01-02 16:19:49.701899: lung_071, shape torch.Size([1, 289, 459, 459]), rank 0 
2025-01-02 16:20:16.932007: predicting lung_075 
2025-01-02 16:20:16.969007: lung_075, shape torch.Size([1, 340, 521, 521]), rank 0 
2025-01-02 16:21:04.028537: predicting lung_081 
2025-01-02 16:21:04.064537: lung_081, shape torch.Size([1, 249, 574, 574]), rank 0 
2025-01-02 16:21:56.809649: Validation complete 
2025-01-02 16:21:56.809649: Mean Validation Dice:  0.4839589333151522 
