
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-01-02 20:45:35.398609: do_dummy_2d_data_aug: True 
2025-01-02 20:45:35.412607: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset010_Colon\splits_final.json 
2025-01-02 20:45:35.418364: The split file contains 5 splits. 
2025-01-02 20:45:35.421363: Desired fold for training: 1 
2025-01-02 20:45:35.423361: This split has 101 training and 25 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [56, 192, 192], 'median_image_size_in_voxels': [150.0, 512.0, 512.0], 'spacing': [3.0, 0.78125, 0.78125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset010_Colon', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [5.0, 0.78125, 0.78125], 'original_median_shape_after_transp': [95, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 445.0, 'mean': 67.2009506225586, 'median': 67.0, 'min': -848.0, 'percentile_00_5': -40.0, 'percentile_99_5': 188.0, 'std': 37.13160705566406}}} 
 
2025-01-02 20:45:42.927051: unpacking dataset... 
2025-01-02 20:45:43.174682: unpacking done... 
2025-01-02 20:45:46.085487:  
2025-01-02 20:45:46.085989: Epoch 0 
2025-01-02 20:45:46.090499: Current learning rate: 0.01 
2025-01-02 20:46:32.156787: train_loss 0.056 
2025-01-02 20:46:32.156787: val_loss -1e-04 
2025-01-02 20:46:32.162808: Pseudo dice [np.float32(0.0)] 
2025-01-02 20:46:32.166316: Epoch time: 46.07 s 
2025-01-02 20:46:32.169325: Yayy! New best EMA pseudo Dice: 0.0 
2025-01-02 20:46:32.887876:  
2025-01-02 20:46:32.888879: Epoch 1 
2025-01-02 20:46:32.893909: Current learning rate: 0.00991 
2025-01-02 20:47:14.431115: train_loss -0.0556 
2025-01-02 20:47:14.431115: val_loss -0.1196 
2025-01-02 20:47:14.437229: Pseudo dice [np.float32(0.0)] 
2025-01-02 20:47:14.440789: Epoch time: 41.54 s 
2025-01-02 20:47:14.992316:  
2025-01-02 20:47:14.992316: Epoch 2 
2025-01-02 20:47:14.997329: Current learning rate: 0.00982 
2025-01-02 20:47:56.519156: train_loss -0.2466 
2025-01-02 20:47:56.519659: val_loss -0.1457 
2025-01-02 20:47:56.526677: Pseudo dice [np.float32(0.1891)] 
2025-01-02 20:47:56.529686: Epoch time: 41.53 s 
2025-01-02 20:47:56.533199: Yayy! New best EMA pseudo Dice: 0.01889999955892563 
2025-01-02 20:47:57.350463:  
2025-01-02 20:47:57.351464: Epoch 3 
2025-01-02 20:47:57.356563: Current learning rate: 0.00973 
2025-01-02 20:48:38.876291: train_loss -0.2795 
2025-01-02 20:48:38.877291: val_loss -0.2343 
2025-01-02 20:48:38.882809: Pseudo dice [np.float32(0.2837)] 
2025-01-02 20:48:38.885317: Epoch time: 41.53 s 
2025-01-02 20:48:38.888832: Yayy! New best EMA pseudo Dice: 0.04540000110864639 
2025-01-02 20:48:39.687230:  
2025-01-02 20:48:39.688233: Epoch 4 
2025-01-02 20:48:39.691955: Current learning rate: 0.00964 
2025-01-02 20:49:21.172406: train_loss -0.3284 
2025-01-02 20:49:21.172406: val_loss -0.274 
2025-01-02 20:49:21.178925: Pseudo dice [np.float32(0.3345)] 
2025-01-02 20:49:21.182437: Epoch time: 41.49 s 
2025-01-02 20:49:21.184942: Yayy! New best EMA pseudo Dice: 0.07429999858140945 
2025-01-02 20:49:22.143593:  
2025-01-02 20:49:22.144598: Epoch 5 
2025-01-02 20:49:22.149143: Current learning rate: 0.00955 
2025-01-02 20:50:03.649383: train_loss -0.3737 
2025-01-02 20:50:03.650384: val_loss -0.2908 
2025-01-02 20:50:03.655909: Pseudo dice [np.float32(0.3763)] 
2025-01-02 20:50:03.658416: Epoch time: 41.51 s 
2025-01-02 20:50:03.661929: Yayy! New best EMA pseudo Dice: 0.10450000315904617 
2025-01-02 20:50:04.440976:  
2025-01-02 20:50:04.441479: Epoch 6 
2025-01-02 20:50:04.446491: Current learning rate: 0.00946 
2025-01-02 20:50:45.907751: train_loss -0.388 
2025-01-02 20:50:45.907751: val_loss -0.3582 
2025-01-02 20:50:45.913774: Pseudo dice [np.float32(0.4598)] 
2025-01-02 20:50:45.916282: Epoch time: 41.47 s 
2025-01-02 20:50:45.919812: Yayy! New best EMA pseudo Dice: 0.14000000059604645 
2025-01-02 20:50:46.773683:  
2025-01-02 20:50:46.774687: Epoch 7 
2025-01-02 20:50:46.779257: Current learning rate: 0.00937 
2025-01-02 20:51:28.294802: train_loss -0.4037 
2025-01-02 20:51:28.295306: val_loss -0.3187 
2025-01-02 20:51:28.299829: Pseudo dice [np.float32(0.3918)] 
2025-01-02 20:51:28.304098: Epoch time: 41.52 s 
2025-01-02 20:51:28.306692: Yayy! New best EMA pseudo Dice: 0.16519999504089355 
2025-01-02 20:51:29.078672:  
2025-01-02 20:51:29.078672: Epoch 8 
2025-01-02 20:51:29.084203: Current learning rate: 0.00928 
2025-01-02 20:52:10.556503: train_loss -0.4186 
2025-01-02 20:52:10.557506: val_loss -0.3142 
2025-01-02 20:52:10.563027: Pseudo dice [np.float32(0.3733)] 
2025-01-02 20:52:10.565583: Epoch time: 41.48 s 
2025-01-02 20:52:10.568662: Yayy! New best EMA pseudo Dice: 0.1860000044107437 
2025-01-02 20:52:11.366612:  
2025-01-02 20:52:11.366612: Epoch 9 
2025-01-02 20:52:11.371626: Current learning rate: 0.00919 
2025-01-02 20:52:52.852943: train_loss -0.4089 
2025-01-02 20:52:52.852943: val_loss -0.2917 
2025-01-02 20:52:52.859466: Pseudo dice [np.float32(0.3596)] 
2025-01-02 20:52:52.862984: Epoch time: 41.49 s 
2025-01-02 20:52:52.865496: Yayy! New best EMA pseudo Dice: 0.20340000092983246 
2025-01-02 20:52:53.634253:  
2025-01-02 20:52:53.634253: Epoch 10 
2025-01-02 20:52:53.639262: Current learning rate: 0.0091 
2025-01-02 20:53:35.123504: train_loss -0.4303 
2025-01-02 20:53:35.124505: val_loss -0.3809 
2025-01-02 20:53:35.130027: Pseudo dice [np.float32(0.4314)] 
2025-01-02 20:53:35.133536: Epoch time: 41.49 s 
2025-01-02 20:53:35.135549: Yayy! New best EMA pseudo Dice: 0.22619999945163727 
2025-01-02 20:53:35.951794:  
2025-01-02 20:53:35.952301: Epoch 11 
2025-01-02 20:53:35.956446: Current learning rate: 0.009 
2025-01-02 20:54:17.423153: train_loss -0.4521 
2025-01-02 20:54:17.424157: val_loss -0.381 
2025-01-02 20:54:17.429171: Pseudo dice [np.float32(0.469)] 
2025-01-02 20:54:17.433184: Epoch time: 41.47 s 
2025-01-02 20:54:17.435692: Yayy! New best EMA pseudo Dice: 0.25049999356269836 
2025-01-02 20:54:18.211023:  
2025-01-02 20:54:18.211023: Epoch 12 
2025-01-02 20:54:18.215067: Current learning rate: 0.00891 
2025-01-02 20:54:59.694196: train_loss -0.4393 
2025-01-02 20:54:59.694699: val_loss -0.3858 
2025-01-02 20:54:59.699710: Pseudo dice [np.float32(0.4682)] 
2025-01-02 20:54:59.703219: Epoch time: 41.48 s 
2025-01-02 20:54:59.705726: Yayy! New best EMA pseudo Dice: 0.27219998836517334 
2025-01-02 20:55:00.769191:  
2025-01-02 20:55:00.770704: Epoch 13 
2025-01-02 20:55:00.775778: Current learning rate: 0.00882 
2025-01-02 20:55:42.246872: train_loss -0.4152 
2025-01-02 20:55:42.247873: val_loss -0.2902 
2025-01-02 20:55:42.253418: Pseudo dice [np.float32(0.4102)] 
2025-01-02 20:55:42.256031: Epoch time: 41.48 s 
2025-01-02 20:55:42.259579: Yayy! New best EMA pseudo Dice: 0.28600001335144043 
2025-01-02 20:55:43.047793:  
2025-01-02 20:55:43.048796: Epoch 14 
2025-01-02 20:55:43.053367: Current learning rate: 0.00873 
2025-01-02 20:56:24.506775: train_loss -0.449 
2025-01-02 20:56:24.506775: val_loss -0.2902 
2025-01-02 20:56:24.513293: Pseudo dice [np.float32(0.3698)] 
2025-01-02 20:56:24.516803: Epoch time: 41.46 s 
2025-01-02 20:56:24.519419: Yayy! New best EMA pseudo Dice: 0.29440000653266907 
2025-01-02 20:56:25.346744:  
2025-01-02 20:56:25.346744: Epoch 15 
2025-01-02 20:56:25.351813: Current learning rate: 0.00864 
2025-01-02 20:57:06.814883: train_loss -0.4361 
2025-01-02 20:57:06.816388: val_loss -0.3496 
2025-01-02 20:57:06.821972: Pseudo dice [np.float32(0.4536)] 
2025-01-02 20:57:06.825016: Epoch time: 41.47 s 
2025-01-02 20:57:06.827543: Yayy! New best EMA pseudo Dice: 0.31029999256134033 
2025-01-02 20:57:07.625924:  
2025-01-02 20:57:07.626924: Epoch 16 
2025-01-02 20:57:07.630007: Current learning rate: 0.00855 
2025-01-02 20:57:49.089070: train_loss -0.4789 
2025-01-02 20:57:49.090572: val_loss -0.281 
2025-01-02 20:57:49.095587: Pseudo dice [np.float32(0.326)] 
2025-01-02 20:57:49.099099: Epoch time: 41.46 s 
2025-01-02 20:57:49.102647: Yayy! New best EMA pseudo Dice: 0.31189998984336853 
2025-01-02 20:57:50.018508:  
2025-01-02 20:57:50.018508: Epoch 17 
2025-01-02 20:57:50.024527: Current learning rate: 0.00846 
2025-01-02 20:58:31.491714: train_loss -0.4442 
2025-01-02 20:58:31.492717: val_loss -0.4055 
2025-01-02 20:58:31.497739: Pseudo dice [np.float32(0.4831)] 
2025-01-02 20:58:31.501758: Epoch time: 41.47 s 
2025-01-02 20:58:31.504268: Yayy! New best EMA pseudo Dice: 0.32899999618530273 
2025-01-02 20:58:32.271979:  
2025-01-02 20:58:32.273524: Epoch 18 
2025-01-02 20:58:32.279089: Current learning rate: 0.00836 
2025-01-02 20:59:13.736403: train_loss -0.4778 
2025-01-02 20:59:13.736905: val_loss -0.3986 
2025-01-02 20:59:13.742949: Pseudo dice [np.float32(0.4443)] 
2025-01-02 20:59:13.745273: Epoch time: 41.46 s 
2025-01-02 20:59:13.749345: Yayy! New best EMA pseudo Dice: 0.34049999713897705 
2025-01-02 20:59:14.543197:  
2025-01-02 20:59:14.544201: Epoch 19 
2025-01-02 20:59:14.548790: Current learning rate: 0.00827 
2025-01-02 20:59:56.021703: train_loss -0.4874 
2025-01-02 20:59:56.021703: val_loss -0.4777 
2025-01-02 20:59:56.029222: Pseudo dice [np.float32(0.5735)] 
2025-01-02 20:59:56.032731: Epoch time: 41.48 s 
2025-01-02 20:59:56.035740: Yayy! New best EMA pseudo Dice: 0.3637999892234802 
2025-01-02 20:59:56.829887:  
2025-01-02 20:59:56.829887: Epoch 20 
2025-01-02 20:59:56.835958: Current learning rate: 0.00818 
2025-01-02 21:00:38.272092: train_loss -0.4952 
2025-01-02 21:00:38.272092: val_loss -0.4606 
2025-01-02 21:00:38.278614: Pseudo dice [np.float32(0.5784)] 
2025-01-02 21:00:38.281121: Epoch time: 41.44 s 
2025-01-02 21:00:38.284630: Yayy! New best EMA pseudo Dice: 0.38530001044273376 
2025-01-02 21:00:39.264318:  
2025-01-02 21:00:39.264821: Epoch 21 
2025-01-02 21:00:39.269834: Current learning rate: 0.00809 
2025-01-02 21:01:20.708938: train_loss -0.4916 
2025-01-02 21:01:20.709941: val_loss -0.3962 
2025-01-02 21:01:20.715992: Pseudo dice [np.float32(0.4351)] 
2025-01-02 21:01:20.719615: Epoch time: 41.45 s 
2025-01-02 21:01:20.722639: Yayy! New best EMA pseudo Dice: 0.3903000056743622 
2025-01-02 21:01:21.457354:  
2025-01-02 21:01:21.458893: Epoch 22 
2025-01-02 21:01:21.463963: Current learning rate: 0.008 
2025-01-02 21:02:11.296452: train_loss -0.539 
2025-01-02 21:02:11.296955: val_loss -0.3889 
2025-01-02 21:02:11.303051: Pseudo dice [np.float32(0.4742)] 
2025-01-02 21:02:11.306562: Epoch time: 49.84 s 
2025-01-02 21:02:11.309615: Yayy! New best EMA pseudo Dice: 0.3986999988555908 
2025-01-02 21:02:12.083761:  
2025-01-02 21:02:12.084276: Epoch 23 
2025-01-02 21:02:12.089288: Current learning rate: 0.0079 
2025-01-02 21:02:54.768696: train_loss -0.488 
2025-01-02 21:02:54.768696: val_loss -0.4837 
2025-01-02 21:02:54.776892: Pseudo dice [np.float32(0.5605)] 
2025-01-02 21:02:54.781435: Epoch time: 42.69 s 
2025-01-02 21:02:54.784505: Yayy! New best EMA pseudo Dice: 0.414900004863739 
2025-01-02 21:02:55.529800:  
2025-01-02 21:02:55.529800: Epoch 24 
2025-01-02 21:02:55.535819: Current learning rate: 0.00781 
2025-01-02 21:03:38.627444: train_loss -0.528 
2025-01-02 21:03:38.627948: val_loss -0.4957 
2025-01-02 21:03:38.633512: Pseudo dice [np.float32(0.5635)] 
2025-01-02 21:03:38.636538: Epoch time: 43.1 s 
2025-01-02 21:03:38.640046: Yayy! New best EMA pseudo Dice: 0.42969998717308044 
2025-01-02 21:03:39.450823:  
2025-01-02 21:03:39.450823: Epoch 25 
2025-01-02 21:03:39.456502: Current learning rate: 0.00772 
2025-01-02 21:04:22.080716: train_loss -0.4959 
2025-01-02 21:04:22.081218: val_loss -0.4496 
2025-01-02 21:04:22.087235: Pseudo dice [np.float32(0.5271)] 
2025-01-02 21:04:22.089300: Epoch time: 42.63 s 
2025-01-02 21:04:22.093328: Yayy! New best EMA pseudo Dice: 0.43950000405311584 
2025-01-02 21:04:22.924357:  
2025-01-02 21:04:22.925362: Epoch 26 
2025-01-02 21:04:22.929900: Current learning rate: 0.00763 
2025-01-02 21:05:05.731682: train_loss -0.5459 
2025-01-02 21:05:05.731682: val_loss -0.4104 
2025-01-02 21:05:05.737833: Pseudo dice [np.float32(0.4524)] 
2025-01-02 21:05:05.742948: Epoch time: 42.81 s 
2025-01-02 21:05:05.747056: Yayy! New best EMA pseudo Dice: 0.4408000111579895 
2025-01-02 21:05:06.540954:  
2025-01-02 21:05:06.540954: Epoch 27 
2025-01-02 21:05:06.545964: Current learning rate: 0.00753 
2025-01-02 21:05:49.412993: train_loss -0.4976 
2025-01-02 21:05:49.413495: val_loss -0.4604 
2025-01-02 21:05:49.418556: Pseudo dice [np.float32(0.5142)] 
2025-01-02 21:05:49.422073: Epoch time: 42.87 s 
2025-01-02 21:05:49.425576: Yayy! New best EMA pseudo Dice: 0.4481000006198883 
2025-01-02 21:05:50.153537:  
2025-01-02 21:05:50.154538: Epoch 28 
2025-01-02 21:05:50.160174: Current learning rate: 0.00744 
2025-01-02 21:06:33.001298: train_loss -0.4806 
2025-01-02 21:06:33.001802: val_loss -0.3035 
2025-01-02 21:06:33.006815: Pseudo dice [np.float32(0.3875)] 
2025-01-02 21:06:33.011325: Epoch time: 42.85 s 
2025-01-02 21:06:33.719808:  
2025-01-02 21:06:33.720806: Epoch 29 
2025-01-02 21:06:33.726370: Current learning rate: 0.00735 
2025-01-02 21:07:17.131388: train_loss -0.502 
2025-01-02 21:07:17.131892: val_loss -0.4704 
2025-01-02 21:07:17.137919: Pseudo dice [np.float32(0.5529)] 
2025-01-02 21:07:17.142465: Epoch time: 43.41 s 
2025-01-02 21:07:17.147009: Yayy! New best EMA pseudo Dice: 0.4530999958515167 
2025-01-02 21:07:17.964903:  
2025-01-02 21:07:17.965424: Epoch 30 
2025-01-02 21:07:17.970445: Current learning rate: 0.00725 
2025-01-02 21:08:02.852777: train_loss -0.5144 
2025-01-02 21:08:02.852777: val_loss -0.5277 
2025-01-02 21:08:02.859329: Pseudo dice [np.float32(0.6026)] 
2025-01-02 21:08:02.861860: Epoch time: 44.89 s 
2025-01-02 21:08:02.866622: Yayy! New best EMA pseudo Dice: 0.46810001134872437 
2025-01-02 21:08:03.659997:  
2025-01-02 21:08:03.659997: Epoch 31 
2025-01-02 21:08:03.665591: Current learning rate: 0.00716 
2025-01-02 21:08:49.307259: train_loss -0.5625 
2025-01-02 21:08:49.307769: val_loss -0.4577 
2025-01-02 21:08:49.313902: Pseudo dice [np.float32(0.5132)] 
2025-01-02 21:08:49.317522: Epoch time: 45.65 s 
2025-01-02 21:08:49.320565: Yayy! New best EMA pseudo Dice: 0.4726000130176544 
2025-01-02 21:08:50.089090:  
2025-01-02 21:08:50.089090: Epoch 32 
2025-01-02 21:08:50.094139: Current learning rate: 0.00707 
2025-01-02 21:09:34.172791: train_loss -0.5344 
2025-01-02 21:09:34.173301: val_loss -0.488 
2025-01-02 21:09:34.178401: Pseudo dice [np.float32(0.5214)] 
2025-01-02 21:09:34.182470: Epoch time: 44.08 s 
2025-01-02 21:09:34.185554: Yayy! New best EMA pseudo Dice: 0.47749999165534973 
2025-01-02 21:09:35.011027:  
2025-01-02 21:09:35.012027: Epoch 33 
2025-01-02 21:09:35.017048: Current learning rate: 0.00697 
2025-01-02 21:10:17.431205: train_loss -0.5442 
2025-01-02 21:10:17.431762: val_loss -0.3671 
2025-01-02 21:10:17.437351: Pseudo dice [np.float32(0.4283)] 
2025-01-02 21:10:17.440885: Epoch time: 42.42 s 
2025-01-02 21:10:17.985929:  
2025-01-02 21:10:17.986928: Epoch 34 
2025-01-02 21:10:17.992447: Current learning rate: 0.00688 
2025-01-02 21:11:01.191120: train_loss -0.5958 
2025-01-02 21:11:01.191622: val_loss -0.4858 
2025-01-02 21:11:01.197272: Pseudo dice [np.float32(0.6047)] 
2025-01-02 21:11:01.200864: Epoch time: 43.21 s 
2025-01-02 21:11:01.203386: Yayy! New best EMA pseudo Dice: 0.48579999804496765 
2025-01-02 21:11:02.025671:  
2025-01-02 21:11:02.026672: Epoch 35 
2025-01-02 21:11:02.032286: Current learning rate: 0.00679 
2025-01-02 21:11:44.858103: train_loss -0.5127 
2025-01-02 21:11:44.858103: val_loss -0.2984 
2025-01-02 21:11:44.864625: Pseudo dice [np.float32(0.3393)] 
2025-01-02 21:11:44.868138: Epoch time: 42.83 s 
2025-01-02 21:11:45.445771:  
2025-01-02 21:11:45.445771: Epoch 36 
2025-01-02 21:11:45.451838: Current learning rate: 0.00669 
2025-01-02 21:12:27.887490: train_loss -0.5117 
2025-01-02 21:12:27.887490: val_loss -0.3489 
2025-01-02 21:12:27.893616: Pseudo dice [np.float32(0.4545)] 
2025-01-02 21:12:27.897192: Epoch time: 42.44 s 
2025-01-02 21:12:28.632030:  
2025-01-02 21:12:28.633533: Epoch 37 
2025-01-02 21:12:28.638548: Current learning rate: 0.0066 
2025-01-02 21:13:11.056048: train_loss -0.5202 
2025-01-02 21:13:11.057054: val_loss -0.5054 
2025-01-02 21:13:11.064166: Pseudo dice [np.float32(0.5687)] 
2025-01-02 21:13:11.069894: Epoch time: 42.42 s 
2025-01-02 21:13:11.667552:  
2025-01-02 21:13:11.667552: Epoch 38 
2025-01-02 21:13:11.673676: Current learning rate: 0.0065 
2025-01-02 21:13:53.984854: train_loss -0.5551 
2025-01-02 21:13:53.985861: val_loss -0.3816 
2025-01-02 21:13:53.991205: Pseudo dice [np.float32(0.4151)] 
2025-01-02 21:13:53.994720: Epoch time: 42.32 s 
2025-01-02 21:13:54.562079:  
2025-01-02 21:13:54.562079: Epoch 39 
2025-01-02 21:13:54.568175: Current learning rate: 0.00641 
2025-01-02 21:14:37.509339: train_loss -0.5267 
2025-01-02 21:14:37.509843: val_loss -0.4684 
2025-01-02 21:14:37.515449: Pseudo dice [np.float32(0.5216)] 
2025-01-02 21:14:37.517956: Epoch time: 42.95 s 
2025-01-02 21:14:38.070215:  
2025-01-02 21:14:38.070215: Epoch 40 
2025-01-02 21:14:38.076315: Current learning rate: 0.00631 
2025-01-02 21:15:20.366292: train_loss -0.547 
2025-01-02 21:15:20.366292: val_loss -0.4988 
2025-01-02 21:15:20.373323: Pseudo dice [np.float32(0.6208)] 
2025-01-02 21:15:20.376832: Epoch time: 42.3 s 
2025-01-02 21:15:20.380338: Yayy! New best EMA pseudo Dice: 0.4921000003814697 
2025-01-02 21:15:21.158469:  
2025-01-02 21:15:21.158469: Epoch 41 
2025-01-02 21:15:21.163531: Current learning rate: 0.00622 
2025-01-02 21:16:03.902912: train_loss -0.5511 
2025-01-02 21:16:03.903414: val_loss -0.4412 
2025-01-02 21:16:03.909425: Pseudo dice [np.float32(0.4846)] 
2025-01-02 21:16:03.913438: Epoch time: 42.75 s 
2025-01-02 21:16:04.463202:  
2025-01-02 21:16:04.463202: Epoch 42 
2025-01-02 21:16:04.468223: Current learning rate: 0.00612 
2025-01-02 21:16:46.734365: train_loss -0.5365 
2025-01-02 21:16:46.734869: val_loss -0.583 
2025-01-02 21:16:46.740404: Pseudo dice [np.float32(0.6472)] 
2025-01-02 21:16:46.742917: Epoch time: 42.27 s 
2025-01-02 21:16:46.746421: Yayy! New best EMA pseudo Dice: 0.5070000290870667 
2025-01-02 21:16:47.520088:  
2025-01-02 21:16:47.520088: Epoch 43 
2025-01-02 21:16:47.525831: Current learning rate: 0.00603 
2025-01-02 21:17:30.287872: train_loss -0.5498 
2025-01-02 21:17:30.288875: val_loss -0.4063 
2025-01-02 21:17:30.294413: Pseudo dice [np.float32(0.4351)] 
2025-01-02 21:17:30.297549: Epoch time: 42.77 s 
2025-01-02 21:17:30.832995:  
2025-01-02 21:17:30.833999: Epoch 44 
2025-01-02 21:17:30.838564: Current learning rate: 0.00593 
2025-01-02 21:18:13.674352: train_loss -0.5829 
2025-01-02 21:18:13.675352: val_loss -0.429 
2025-01-02 21:18:13.680897: Pseudo dice [np.float32(0.5028)] 
2025-01-02 21:18:13.684408: Epoch time: 42.84 s 
2025-01-02 21:18:14.410601:  
2025-01-02 21:18:14.411103: Epoch 45 
2025-01-02 21:18:14.416120: Current learning rate: 0.00584 
2025-01-02 21:18:57.378779: train_loss -0.5854 
2025-01-02 21:18:57.379779: val_loss -0.2978 
2025-01-02 21:18:57.386298: Pseudo dice [np.float32(0.4129)] 
2025-01-02 21:18:57.389809: Epoch time: 42.97 s 
2025-01-02 21:18:57.909471:  
2025-01-02 21:18:57.909977: Epoch 46 
2025-01-02 21:18:57.914991: Current learning rate: 0.00574 
2025-01-02 21:19:40.303872: train_loss -0.5752 
2025-01-02 21:19:40.303872: val_loss -0.543 
2025-01-02 21:19:40.309486: Pseudo dice [np.float32(0.5731)] 
2025-01-02 21:19:40.312044: Epoch time: 42.4 s 
2025-01-02 21:19:40.847798:  
2025-01-02 21:19:40.847798: Epoch 47 
2025-01-02 21:19:40.853346: Current learning rate: 0.00565 
2025-01-02 21:20:24.180144: train_loss -0.5775 
2025-01-02 21:20:24.180653: val_loss -0.4943 
2025-01-02 21:20:24.186222: Pseudo dice [np.float32(0.5453)] 
2025-01-02 21:20:24.189249: Epoch time: 43.33 s 
2025-01-02 21:20:24.722803:  
2025-01-02 21:20:24.723315: Epoch 48 
2025-01-02 21:20:24.727396: Current learning rate: 0.00555 
2025-01-02 21:21:06.951407: train_loss -0.5907 
2025-01-02 21:21:06.951910: val_loss -0.3845 
2025-01-02 21:21:06.957019: Pseudo dice [np.float32(0.4364)] 
2025-01-02 21:21:06.960557: Epoch time: 42.23 s 
2025-01-02 21:21:07.541542:  
2025-01-02 21:21:07.542544: Epoch 49 
2025-01-02 21:21:07.547131: Current learning rate: 0.00546 
2025-01-02 21:21:50.072569: train_loss -0.5831 
2025-01-02 21:21:50.073072: val_loss -0.4889 
2025-01-02 21:21:50.078084: Pseudo dice [np.float32(0.5685)] 
2025-01-02 21:21:50.081597: Epoch time: 42.53 s 
2025-01-02 21:21:50.837271:  
2025-01-02 21:21:50.837773: Epoch 50 
2025-01-02 21:21:50.842785: Current learning rate: 0.00536 
2025-01-02 21:22:33.189599: train_loss -0.5909 
2025-01-02 21:22:33.190106: val_loss -0.5766 
2025-01-02 21:22:33.195183: Pseudo dice [np.float32(0.689)] 
2025-01-02 21:22:33.199765: Epoch time: 42.35 s 
2025-01-02 21:22:33.203278: Yayy! New best EMA pseudo Dice: 0.5228999853134155 
2025-01-02 21:22:34.003269:  
2025-01-02 21:22:34.003269: Epoch 51 
2025-01-02 21:22:34.008314: Current learning rate: 0.00526 
2025-01-02 21:23:16.259354: train_loss -0.5275 
2025-01-02 21:23:16.259354: val_loss -0.4402 
2025-01-02 21:23:16.265371: Pseudo dice [np.float32(0.5401)] 
2025-01-02 21:23:16.268404: Epoch time: 42.26 s 
2025-01-02 21:23:16.271924: Yayy! New best EMA pseudo Dice: 0.5246000289916992 
2025-01-02 21:23:17.036404:  
2025-01-02 21:23:17.036404: Epoch 52 
2025-01-02 21:23:17.041473: Current learning rate: 0.00517 
2025-01-02 21:23:59.462481: train_loss -0.5558 
2025-01-02 21:23:59.463484: val_loss -0.5002 
2025-01-02 21:23:59.470246: Pseudo dice [np.float32(0.6438)] 
2025-01-02 21:23:59.473761: Epoch time: 42.43 s 
2025-01-02 21:23:59.476267: Yayy! New best EMA pseudo Dice: 0.5364999771118164 
2025-01-02 21:24:00.403339:  
2025-01-02 21:24:00.403843: Epoch 53 
2025-01-02 21:24:00.409865: Current learning rate: 0.00507 
2025-01-02 21:24:42.749388: train_loss -0.5626 
2025-01-02 21:24:42.749388: val_loss -0.4701 
2025-01-02 21:24:42.754484: Pseudo dice [np.float32(0.5044)] 
2025-01-02 21:24:42.759626: Epoch time: 42.35 s 
2025-01-02 21:24:43.372826:  
2025-01-02 21:24:43.372826: Epoch 54 
2025-01-02 21:24:43.378853: Current learning rate: 0.00497 
2025-01-02 21:25:25.628947: train_loss -0.6101 
2025-01-02 21:25:25.628947: val_loss -0.4386 
2025-01-02 21:25:25.634552: Pseudo dice [np.float32(0.4852)] 
2025-01-02 21:25:25.637087: Epoch time: 42.26 s 
2025-01-02 21:25:26.194074:  
2025-01-02 21:25:26.194074: Epoch 55 
2025-01-02 21:25:26.199085: Current learning rate: 0.00487 
2025-01-02 21:26:08.218250: train_loss -0.5881 
2025-01-02 21:26:08.218752: val_loss -0.4646 
2025-01-02 21:26:08.224859: Pseudo dice [np.float32(0.5486)] 
2025-01-02 21:26:08.229939: Epoch time: 42.02 s 
2025-01-02 21:26:08.787548:  
2025-01-02 21:26:08.787548: Epoch 56 
2025-01-02 21:26:08.792832: Current learning rate: 0.00478 
2025-01-02 21:26:52.023630: train_loss -0.578 
2025-01-02 21:26:52.024133: val_loss -0.5733 
2025-01-02 21:26:52.030230: Pseudo dice [np.float32(0.6374)] 
2025-01-02 21:26:52.035334: Epoch time: 43.24 s 
2025-01-02 21:26:52.038402: Yayy! New best EMA pseudo Dice: 0.5411999821662903 
2025-01-02 21:26:52.809488:  
2025-01-02 21:26:52.810488: Epoch 57 
2025-01-02 21:26:52.816105: Current learning rate: 0.00468 
2025-01-02 21:27:38.093544: train_loss -0.5735 
2025-01-02 21:27:38.094549: val_loss -0.4477 
2025-01-02 21:27:38.100352: Pseudo dice [np.float32(0.5108)] 
2025-01-02 21:27:38.102859: Epoch time: 45.28 s 
2025-01-02 21:27:38.648169:  
2025-01-02 21:27:38.649172: Epoch 58 
2025-01-02 21:27:38.654256: Current learning rate: 0.00458 
2025-01-02 21:28:23.608087: train_loss -0.6012 
2025-01-02 21:28:23.609091: val_loss -0.5688 
2025-01-02 21:28:23.615315: Pseudo dice [np.float32(0.6402)] 
2025-01-02 21:28:23.618853: Epoch time: 44.96 s 
2025-01-02 21:28:23.621887: Yayy! New best EMA pseudo Dice: 0.5483999848365784 
2025-01-02 21:28:24.444483:  
2025-01-02 21:28:24.444985: Epoch 59 
2025-01-02 21:28:24.449995: Current learning rate: 0.00448 
2025-01-02 21:29:06.464201: train_loss -0.5645 
2025-01-02 21:29:06.464201: val_loss -0.4469 
2025-01-02 21:29:06.471222: Pseudo dice [np.float32(0.5163)] 
2025-01-02 21:29:06.474231: Epoch time: 42.02 s 
2025-01-02 21:29:07.016146:  
2025-01-02 21:29:07.016650: Epoch 60 
2025-01-02 21:29:07.021705: Current learning rate: 0.00438 
2025-01-02 21:29:49.504249: train_loss -0.5824 
2025-01-02 21:29:49.504249: val_loss -0.5041 
2025-01-02 21:29:49.510704: Pseudo dice [np.float32(0.5372)] 
2025-01-02 21:29:49.513077: Epoch time: 42.49 s 
2025-01-02 21:29:50.229429:  
2025-01-02 21:29:50.229933: Epoch 61 
2025-01-02 21:29:50.234952: Current learning rate: 0.00429 
2025-01-02 21:30:32.993006: train_loss -0.5843 
2025-01-02 21:30:32.994009: val_loss -0.4822 
2025-01-02 21:30:32.998634: Pseudo dice [np.float32(0.4985)] 
2025-01-02 21:30:33.002765: Epoch time: 42.76 s 
2025-01-02 21:30:33.554958:  
2025-01-02 21:30:33.555468: Epoch 62 
2025-01-02 21:30:33.560604: Current learning rate: 0.00419 
2025-01-02 21:31:16.456154: train_loss -0.6031 
2025-01-02 21:31:16.456669: val_loss -0.5567 
2025-01-02 21:31:16.462388: Pseudo dice [np.float32(0.5673)] 
2025-01-02 21:31:16.465626: Epoch time: 42.9 s 
2025-01-02 21:31:17.031090:  
2025-01-02 21:31:17.031090: Epoch 63 
2025-01-02 21:31:17.036164: Current learning rate: 0.00409 
2025-01-02 21:31:59.280726: train_loss -0.6044 
2025-01-02 21:31:59.281230: val_loss -0.5118 
2025-01-02 21:31:59.288277: Pseudo dice [np.float32(0.5982)] 
2025-01-02 21:31:59.293298: Epoch time: 42.25 s 
2025-01-02 21:31:59.867820:  
2025-01-02 21:31:59.867820: Epoch 64 
2025-01-02 21:31:59.873409: Current learning rate: 0.00399 
2025-01-02 21:32:42.013482: train_loss -0.6123 
2025-01-02 21:32:42.013984: val_loss -0.4913 
2025-01-02 21:32:42.020037: Pseudo dice [np.float32(0.5302)] 
2025-01-02 21:32:42.023606: Epoch time: 42.15 s 
2025-01-02 21:32:42.701973:  
2025-01-02 21:32:42.702977: Epoch 65 
2025-01-02 21:32:42.707531: Current learning rate: 0.00389 
2025-01-02 21:33:24.819638: train_loss -0.627 
2025-01-02 21:33:24.819638: val_loss -0.4736 
2025-01-02 21:33:24.831322: Pseudo dice [np.float32(0.4988)] 
2025-01-02 21:33:24.834399: Epoch time: 42.12 s 
2025-01-02 21:33:25.386107:  
2025-01-02 21:33:25.386107: Epoch 66 
2025-01-02 21:33:25.391652: Current learning rate: 0.00379 
2025-01-02 21:34:08.285534: train_loss -0.6194 
2025-01-02 21:34:08.286038: val_loss -0.5529 
2025-01-02 21:34:08.291073: Pseudo dice [np.float32(0.6273)] 
2025-01-02 21:34:08.294138: Epoch time: 42.9 s 
2025-01-02 21:34:08.298753: Yayy! New best EMA pseudo Dice: 0.5501000285148621 
2025-01-02 21:34:09.059802:  
2025-01-02 21:34:09.059802: Epoch 67 
2025-01-02 21:34:09.065875: Current learning rate: 0.00369 
2025-01-02 21:34:51.854321: train_loss -0.606 
2025-01-02 21:34:51.854841: val_loss -0.5029 
2025-01-02 21:34:51.860963: Pseudo dice [np.float32(0.5765)] 
2025-01-02 21:34:51.864004: Epoch time: 42.8 s 
2025-01-02 21:34:51.867628: Yayy! New best EMA pseudo Dice: 0.5527999997138977 
2025-01-02 21:34:52.670150:  
2025-01-02 21:34:52.671153: Epoch 68 
2025-01-02 21:34:52.675732: Current learning rate: 0.00359 
2025-01-02 21:35:35.195430: train_loss -0.6087 
2025-01-02 21:35:35.196435: val_loss -0.5052 
2025-01-02 21:35:35.203126: Pseudo dice [np.float32(0.5887)] 
2025-01-02 21:35:35.207747: Epoch time: 42.53 s 
2025-01-02 21:35:35.212313: Yayy! New best EMA pseudo Dice: 0.5564000010490417 
2025-01-02 21:35:36.216452:  
2025-01-02 21:35:36.217455: Epoch 69 
2025-01-02 21:35:36.222510: Current learning rate: 0.00349 
2025-01-02 21:36:18.448606: train_loss -0.6314 
2025-01-02 21:36:18.449123: val_loss -0.5182 
2025-01-02 21:36:18.455178: Pseudo dice [np.float32(0.6004)] 
2025-01-02 21:36:18.457271: Epoch time: 42.23 s 
2025-01-02 21:36:18.461320: Yayy! New best EMA pseudo Dice: 0.5608000159263611 
2025-01-02 21:36:19.264056:  
2025-01-02 21:36:19.264559: Epoch 70 
2025-01-02 21:36:19.269639: Current learning rate: 0.00338 
2025-01-02 21:37:01.896902: train_loss -0.6177 
2025-01-02 21:37:01.897405: val_loss -0.507 
2025-01-02 21:37:01.902986: Pseudo dice [np.float32(0.6152)] 
2025-01-02 21:37:01.906559: Epoch time: 42.63 s 
2025-01-02 21:37:01.909091: Yayy! New best EMA pseudo Dice: 0.5662000179290771 
2025-01-02 21:37:02.679768:  
2025-01-02 21:37:02.679768: Epoch 71 
2025-01-02 21:37:02.685343: Current learning rate: 0.00328 
2025-01-02 21:37:45.478616: train_loss -0.6395 
2025-01-02 21:37:45.479620: val_loss -0.5152 
2025-01-02 21:37:45.484638: Pseudo dice [np.float32(0.585)] 
2025-01-02 21:37:45.488144: Epoch time: 42.8 s 
2025-01-02 21:37:45.491157: Yayy! New best EMA pseudo Dice: 0.5680999755859375 
2025-01-02 21:37:46.341027:  
2025-01-02 21:37:46.341027: Epoch 72 
2025-01-02 21:37:46.346563: Current learning rate: 0.00318 
2025-01-02 21:38:28.379938: train_loss -0.6285 
2025-01-02 21:38:28.379938: val_loss -0.6004 
2025-01-02 21:38:28.386482: Pseudo dice [np.float32(0.703)] 
2025-01-02 21:38:28.389021: Epoch time: 42.04 s 
2025-01-02 21:38:28.392626: Yayy! New best EMA pseudo Dice: 0.58160001039505 
2025-01-02 21:38:29.262926:  
2025-01-02 21:38:29.262926: Epoch 73 
2025-01-02 21:38:29.268989: Current learning rate: 0.00308 
2025-01-02 21:39:11.355838: train_loss -0.6131 
2025-01-02 21:39:11.355838: val_loss -0.3471 
2025-01-02 21:39:11.361458: Pseudo dice [np.float32(0.4103)] 
2025-01-02 21:39:11.365009: Epoch time: 42.09 s 
2025-01-02 21:39:11.954014:  
2025-01-02 21:39:11.954014: Epoch 74 
2025-01-02 21:39:11.959538: Current learning rate: 0.00297 
2025-01-02 21:39:55.750643: train_loss -0.5893 
2025-01-02 21:39:55.750643: val_loss -0.4158 
2025-01-02 21:39:55.755717: Pseudo dice [np.float32(0.5452)] 
2025-01-02 21:39:55.760729: Epoch time: 43.8 s 
2025-01-02 21:39:56.323226:  
2025-01-02 21:39:56.323226: Epoch 75 
2025-01-02 21:39:56.329792: Current learning rate: 0.00287 
2025-01-02 21:40:39.186896: train_loss -0.6238 
2025-01-02 21:40:39.186896: val_loss -0.4999 
2025-01-02 21:40:39.193439: Pseudo dice [np.float32(0.6311)] 
2025-01-02 21:40:39.196447: Epoch time: 42.86 s 
2025-01-02 21:40:39.775481:  
2025-01-02 21:40:39.775481: Epoch 76 
2025-01-02 21:40:39.780579: Current learning rate: 0.00277 
2025-01-02 21:41:22.420347: train_loss -0.6312 
2025-01-02 21:41:22.421350: val_loss -0.529 
2025-01-02 21:41:22.426874: Pseudo dice [np.float32(0.5783)] 
2025-01-02 21:41:22.430384: Epoch time: 42.65 s 
2025-01-02 21:41:23.157325:  
2025-01-02 21:41:23.157325: Epoch 77 
2025-01-02 21:41:23.163474: Current learning rate: 0.00266 
2025-01-02 21:42:06.841909: train_loss -0.6328 
2025-01-02 21:42:06.842427: val_loss -0.4659 
2025-01-02 21:42:06.846979: Pseudo dice [np.float32(0.5811)] 
2025-01-02 21:42:06.850574: Epoch time: 43.68 s 
2025-01-02 21:42:07.417071:  
2025-01-02 21:42:07.417573: Epoch 78 
2025-01-02 21:42:07.422636: Current learning rate: 0.00256 
2025-01-02 21:42:49.896225: train_loss -0.6569 
2025-01-02 21:42:49.896225: val_loss -0.6064 
2025-01-02 21:42:49.902234: Pseudo dice [np.float32(0.7036)] 
2025-01-02 21:42:49.905245: Epoch time: 42.48 s 
2025-01-02 21:42:49.908759: Yayy! New best EMA pseudo Dice: 0.5845999717712402 
2025-01-02 21:42:50.740322:  
2025-01-02 21:42:50.741317: Epoch 79 
2025-01-02 21:42:50.746839: Current learning rate: 0.00245 
2025-01-02 21:43:33.734838: train_loss -0.6438 
2025-01-02 21:43:33.735346: val_loss -0.511 
2025-01-02 21:43:33.740955: Pseudo dice [np.float32(0.6653)] 
2025-01-02 21:43:33.744468: Epoch time: 42.99 s 
2025-01-02 21:43:33.747977: Yayy! New best EMA pseudo Dice: 0.5925999879837036 
2025-01-02 21:43:34.550720:  
2025-01-02 21:43:34.551224: Epoch 80 
2025-01-02 21:43:34.556244: Current learning rate: 0.00235 
2025-01-02 21:44:17.417859: train_loss -0.6183 
2025-01-02 21:44:17.418360: val_loss -0.4828 
2025-01-02 21:44:17.423375: Pseudo dice [np.float32(0.5519)] 
2025-01-02 21:44:17.427886: Epoch time: 42.87 s 
2025-01-02 21:44:18.029308:  
2025-01-02 21:44:18.030424: Epoch 81 
2025-01-02 21:44:18.035004: Current learning rate: 0.00224 
2025-01-02 21:45:01.020470: train_loss -0.6483 
2025-01-02 21:45:01.020989: val_loss -0.5102 
2025-01-02 21:45:01.026557: Pseudo dice [np.float32(0.6015)] 
2025-01-02 21:45:01.029577: Epoch time: 42.99 s 
2025-01-02 21:45:01.611590:  
2025-01-02 21:45:01.611590: Epoch 82 
2025-01-02 21:45:01.616604: Current learning rate: 0.00214 
2025-01-02 21:45:44.166167: train_loss -0.6175 
2025-01-02 21:45:44.166676: val_loss -0.5539 
2025-01-02 21:45:44.172266: Pseudo dice [np.float32(0.5809)] 
2025-01-02 21:45:44.175829: Epoch time: 42.56 s 
2025-01-02 21:45:44.720470:  
2025-01-02 21:45:44.720470: Epoch 83 
2025-01-02 21:45:44.725503: Current learning rate: 0.00203 
2025-01-02 21:46:27.362824: train_loss -0.6415 
2025-01-02 21:46:27.363828: val_loss -0.5627 
2025-01-02 21:46:27.369502: Pseudo dice [np.float32(0.6456)] 
2025-01-02 21:46:27.372536: Epoch time: 42.64 s 
2025-01-02 21:46:27.376103: Yayy! New best EMA pseudo Dice: 0.5946000218391418 
2025-01-02 21:46:28.159424:  
2025-01-02 21:46:28.159424: Epoch 84 
2025-01-02 21:46:28.164958: Current learning rate: 0.00192 
2025-01-02 21:47:10.847082: train_loss -0.6336 
2025-01-02 21:47:10.848085: val_loss -0.4948 
2025-01-02 21:47:10.853603: Pseudo dice [np.float32(0.5618)] 
2025-01-02 21:47:10.857112: Epoch time: 42.69 s 
2025-01-02 21:47:11.561662:  
2025-01-02 21:47:11.561662: Epoch 85 
2025-01-02 21:47:11.566678: Current learning rate: 0.00181 
2025-01-02 21:47:54.478652: train_loss -0.6332 
2025-01-02 21:47:54.478652: val_loss -0.488 
2025-01-02 21:47:54.484798: Pseudo dice [np.float32(0.5129)] 
2025-01-02 21:47:54.488314: Epoch time: 42.92 s 
2025-01-02 21:47:55.024669:  
2025-01-02 21:47:55.024669: Epoch 86 
2025-01-02 21:47:55.031222: Current learning rate: 0.0017 
2025-01-02 21:48:37.620959: train_loss -0.6277 
2025-01-02 21:48:37.621461: val_loss -0.4864 
2025-01-02 21:48:37.626475: Pseudo dice [np.float32(0.5879)] 
2025-01-02 21:48:37.629993: Epoch time: 42.6 s 
2025-01-02 21:48:38.204584:  
2025-01-02 21:48:38.205590: Epoch 87 
2025-01-02 21:48:38.210167: Current learning rate: 0.00159 
2025-01-02 21:49:21.087333: train_loss -0.6522 
2025-01-02 21:49:21.087333: val_loss -0.5688 
2025-01-02 21:49:21.093348: Pseudo dice [np.float32(0.614)] 
2025-01-02 21:49:21.096357: Epoch time: 42.88 s 
2025-01-02 21:49:21.620912:  
2025-01-02 21:49:21.621915: Epoch 88 
2025-01-02 21:49:21.627078: Current learning rate: 0.00148 
2025-01-02 21:50:04.400393: train_loss -0.656 
2025-01-02 21:50:04.400393: val_loss -0.5094 
2025-01-02 21:50:04.406404: Pseudo dice [np.float32(0.5948)] 
2025-01-02 21:50:04.410430: Epoch time: 42.78 s 
2025-01-02 21:50:04.951967:  
2025-01-02 21:50:04.951967: Epoch 89 
2025-01-02 21:50:04.956980: Current learning rate: 0.00137 
2025-01-02 21:50:47.315945: train_loss -0.6667 
2025-01-02 21:50:47.315945: val_loss -0.5746 
2025-01-02 21:50:47.321972: Pseudo dice [np.float32(0.6107)] 
2025-01-02 21:50:47.324476: Epoch time: 42.36 s 
2025-01-02 21:50:47.903190:  
2025-01-02 21:50:47.903190: Epoch 90 
2025-01-02 21:50:47.908728: Current learning rate: 0.00126 
2025-01-02 21:51:30.139345: train_loss -0.6722 
2025-01-02 21:51:30.139853: val_loss -0.4844 
2025-01-02 21:51:30.145435: Pseudo dice [np.float32(0.5836)] 
2025-01-02 21:51:30.148494: Epoch time: 42.24 s 
2025-01-02 21:51:30.704408:  
2025-01-02 21:51:30.704408: Epoch 91 
2025-01-02 21:51:30.709930: Current learning rate: 0.00115 
2025-01-02 21:52:12.405670: train_loss -0.6845 
2025-01-02 21:52:12.407177: val_loss -0.5542 
2025-01-02 21:52:12.413249: Pseudo dice [np.float32(0.6445)] 
2025-01-02 21:52:12.415580: Epoch time: 41.7 s 
2025-01-02 21:52:12.419619: Yayy! New best EMA pseudo Dice: 0.5949000120162964 
2025-01-02 21:52:13.174573:  
2025-01-02 21:52:13.174573: Epoch 92 
2025-01-02 21:52:13.180130: Current learning rate: 0.00103 
2025-01-02 21:52:54.737866: train_loss -0.6882 
2025-01-02 21:52:54.738375: val_loss -0.588 
2025-01-02 21:52:54.743960: Pseudo dice [np.float32(0.7025)] 
2025-01-02 21:52:54.747000: Epoch time: 41.56 s 
2025-01-02 21:52:54.749530: Yayy! New best EMA pseudo Dice: 0.6057000160217285 
2025-01-02 21:52:55.620492:  
2025-01-02 21:52:55.620995: Epoch 93 
2025-01-02 21:52:55.626009: Current learning rate: 0.00091 
2025-01-02 21:53:37.179492: train_loss -0.6753 
2025-01-02 21:53:37.179997: val_loss -0.5787 
2025-01-02 21:53:37.185571: Pseudo dice [np.float32(0.6757)] 
2025-01-02 21:53:37.188025: Epoch time: 41.56 s 
2025-01-02 21:53:37.191233: Yayy! New best EMA pseudo Dice: 0.6126999855041504 
2025-01-02 21:53:37.951484:  
2025-01-02 21:53:37.952022: Epoch 94 
2025-01-02 21:53:37.957111: Current learning rate: 0.00079 
2025-01-02 21:54:19.505212: train_loss -0.6804 
2025-01-02 21:54:19.505212: val_loss -0.5595 
2025-01-02 21:54:19.512729: Pseudo dice [np.float32(0.6389)] 
2025-01-02 21:54:19.515235: Epoch time: 41.56 s 
2025-01-02 21:54:19.518742: Yayy! New best EMA pseudo Dice: 0.6152999997138977 
2025-01-02 21:54:20.244798:  
2025-01-02 21:54:20.244798: Epoch 95 
2025-01-02 21:54:20.250397: Current learning rate: 0.00067 
2025-01-02 21:55:01.781661: train_loss -0.6879 
2025-01-02 21:55:01.782169: val_loss -0.5495 
2025-01-02 21:55:01.787720: Pseudo dice [np.float32(0.6787)] 
2025-01-02 21:55:01.792296: Epoch time: 41.54 s 
2025-01-02 21:55:01.794826: Yayy! New best EMA pseudo Dice: 0.6215999722480774 
2025-01-02 21:55:02.511345:  
2025-01-02 21:55:02.511345: Epoch 96 
2025-01-02 21:55:02.516358: Current learning rate: 0.00055 
2025-01-02 21:55:44.054549: train_loss -0.6832 
2025-01-02 21:55:44.054549: val_loss -0.4774 
2025-01-02 21:55:44.060795: Pseudo dice [np.float32(0.5226)] 
2025-01-02 21:55:44.064317: Epoch time: 41.54 s 
2025-01-02 21:55:44.606173:  
2025-01-02 21:55:44.606173: Epoch 97 
2025-01-02 21:55:44.611703: Current learning rate: 0.00043 
2025-01-02 21:56:26.147950: train_loss -0.6848 
2025-01-02 21:56:26.148452: val_loss -0.5452 
2025-01-02 21:56:26.153999: Pseudo dice [np.float32(0.6488)] 
2025-01-02 21:56:26.157520: Epoch time: 41.54 s 
2025-01-02 21:56:26.717988:  
2025-01-02 21:56:26.718988: Epoch 98 
2025-01-02 21:56:26.724562: Current learning rate: 0.0003 
2025-01-02 21:57:09.515460: train_loss -0.6905 
2025-01-02 21:57:09.515460: val_loss -0.5191 
2025-01-02 21:57:09.520998: Pseudo dice [np.float32(0.611)] 
2025-01-02 21:57:09.525007: Epoch time: 42.8 s 
2025-01-02 21:57:10.075558:  
2025-01-02 21:57:10.076558: Epoch 99 
2025-01-02 21:57:10.081641: Current learning rate: 0.00016 
2025-01-02 21:57:53.283432: train_loss -0.6545 
2025-01-02 21:57:53.284431: val_loss -0.5803 
2025-01-02 21:57:53.289986: Pseudo dice [np.float32(0.664)] 
2025-01-02 21:57:53.293495: Epoch time: 43.21 s 
2025-01-02 21:57:54.037155: Training done. 
2025-01-02 21:57:54.072665: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset010_Colon\splits_final.json 
2025-01-02 21:57:54.080665: The split file contains 5 splits. 
2025-01-02 21:57:54.088665: Desired fold for training: 1 
2025-01-02 21:57:54.095665: This split has 101 training and 25 validation cases. 
2025-01-02 21:57:54.101666: predicting colon_006 
2025-01-02 21:57:54.110665: colon_006, shape torch.Size([1, 159, 410, 410]), rank 0 
2025-01-02 21:58:10.352856: predicting colon_011 
2025-01-02 21:58:10.363860: colon_011, shape torch.Size([1, 150, 454, 454]), rank 0 
2025-01-02 21:58:26.016115: predicting colon_015 
2025-01-02 21:58:26.032116: colon_015, shape torch.Size([1, 90, 640, 640]), rank 0 
2025-01-02 21:58:46.648892: predicting colon_022 
2025-01-02 21:58:46.664895: colon_022, shape torch.Size([1, 127, 351, 351]), rank 0 
2025-01-02 21:58:53.671081: predicting colon_036 
2025-01-02 21:58:53.679586: colon_036, shape torch.Size([1, 162, 461, 461]), rank 0 
2025-01-02 21:59:09.391080: predicting colon_050 
2025-01-02 21:59:09.405081: colon_050, shape torch.Size([1, 142, 421, 421]), rank 0 
2025-01-02 21:59:24.761990: predicting colon_059 
2025-01-02 21:59:24.772990: colon_059, shape torch.Size([1, 73, 627, 627]), rank 0 
2025-01-02 21:59:38.719016: predicting colon_069 
2025-01-02 21:59:38.731016: colon_069, shape torch.Size([1, 107, 462, 462]), rank 0 
2025-01-02 21:59:47.999742: predicting colon_078 
2025-01-02 21:59:48.010076: colon_078, shape torch.Size([1, 158, 504, 504]), rank 0 
2025-01-02 22:00:11.689571: predicting colon_098 
2025-01-02 22:00:11.710576: colon_098, shape torch.Size([1, 147, 468, 468]), rank 0 
2025-01-02 22:00:27.046018: predicting colon_099 
2025-01-02 22:00:27.061018: colon_099, shape torch.Size([1, 143, 480, 480]), rank 0 
2025-01-02 22:00:42.244887: predicting colon_120 
2025-01-02 22:00:42.265396: colon_120, shape torch.Size([1, 155, 434, 434]), rank 0 
2025-01-02 22:00:57.658685: predicting colon_122 
2025-01-02 22:00:57.669686: colon_122, shape torch.Size([1, 150, 476, 476]), rank 0 
2025-01-02 22:01:13.313241: predicting colon_126 
2025-01-02 22:01:13.331241: colon_126, shape torch.Size([1, 142, 512, 512]), rank 0 
2025-01-02 22:01:37.396231: predicting colon_133 
2025-01-02 22:01:37.411231: colon_133, shape torch.Size([1, 151, 556, 556]), rank 0 
2025-01-02 22:02:01.140900: predicting colon_143 
2025-01-02 22:02:01.159901: colon_143, shape torch.Size([1, 170, 543, 543]), rank 0 
2025-01-02 22:02:29.904389: predicting colon_145 
2025-01-02 22:02:29.927893: colon_145, shape torch.Size([1, 98, 550, 550]), rank 0 
2025-01-02 22:02:44.210853: predicting colon_157 
2025-01-02 22:02:44.225363: colon_157, shape torch.Size([1, 68, 556, 556]), rank 0 
2025-01-02 22:02:53.723183: predicting colon_163 
2025-01-02 22:02:53.732688: colon_163, shape torch.Size([1, 157, 611, 611]), rank 0 
2025-01-02 22:03:28.511379: predicting colon_164 
2025-01-02 22:03:28.537379: colon_164, shape torch.Size([1, 150, 481, 481]), rank 0 
2025-01-02 22:03:52.490750: predicting colon_181 
2025-01-02 22:03:52.513752: colon_181, shape torch.Size([1, 137, 513, 513]), rank 0 
2025-01-02 22:04:11.518407: predicting colon_193 
2025-01-02 22:04:11.537406: colon_193, shape torch.Size([1, 207, 530, 530]), rank 0 
2025-01-02 22:04:44.737119: predicting colon_194 
2025-01-02 22:04:44.759119: colon_194, shape torch.Size([1, 172, 529, 529]), rank 0 
2025-01-02 22:05:13.266493: predicting colon_207 
2025-01-02 22:05:13.291494: colon_207, shape torch.Size([1, 140, 511, 511]), rank 0 
2025-01-02 22:05:32.349336: predicting colon_219 
2025-01-02 22:05:32.364333: colon_219, shape torch.Size([1, 228, 461, 461]), rank 0 
2025-01-02 22:06:07.248255: Validation complete 
2025-01-02 22:06:07.249758: Mean Validation Dice:  0.3676098791832125 
