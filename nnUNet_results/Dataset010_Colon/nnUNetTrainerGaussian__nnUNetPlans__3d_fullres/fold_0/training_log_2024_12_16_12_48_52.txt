
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-12-16 12:48:52.921221: do_dummy_2d_data_aug: True 
2024-12-16 12:48:52.922725: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset010_Colon\splits_final.json 
2024-12-16 12:48:52.927389: The split file contains 5 splits. 
2024-12-16 12:48:52.930392: Desired fold for training: 0 
2024-12-16 12:48:52.932394: This split has 100 training and 26 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [56, 192, 192], 'median_image_size_in_voxels': [150.0, 512.0, 512.0], 'spacing': [3.0, 0.78125, 0.78125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset010_Colon', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [5.0, 0.78125, 0.78125], 'original_median_shape_after_transp': [95, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 445.0, 'mean': 67.2009506225586, 'median': 67.0, 'min': -848.0, 'percentile_00_5': -40.0, 'percentile_99_5': 188.0, 'std': 37.13160705566406}}} 
 
2024-12-16 12:48:59.402515: unpacking dataset... 
2024-12-16 12:48:59.588991: unpacking done... 
2024-12-16 12:49:02.195712:  
2024-12-16 12:49:02.200247: Epoch 0 
2024-12-16 12:49:02.201781: Current learning rate: 0.01 
2024-12-16 12:49:47.836850: train_loss 0.039 
2024-12-16 12:49:47.842441: val_loss 0.011 
2024-12-16 12:49:47.845621: Pseudo dice [np.float32(0.0)] 
2024-12-16 12:49:47.848154: Epoch time: 45.64 s 
2024-12-16 12:49:47.851178: Yayy! New best EMA pseudo Dice: 0.0 
2024-12-16 12:49:48.439172:  
2024-12-16 12:49:48.444187: Epoch 1 
2024-12-16 12:49:48.446696: Current learning rate: 0.00991 
2024-12-16 12:50:29.598113: train_loss 0.0106 
2024-12-16 12:50:29.604179: val_loss -0.002 
2024-12-16 12:50:29.606320: Pseudo dice [np.float32(0.0)] 
2024-12-16 12:50:29.609831: Epoch time: 41.16 s 
2024-12-16 12:50:30.098554:  
2024-12-16 12:50:30.103567: Epoch 2 
2024-12-16 12:50:30.106580: Current learning rate: 0.00982 
2024-12-16 12:51:11.273350: train_loss 0.0034 
2024-12-16 12:51:11.278911: val_loss 0.0011 
2024-12-16 12:51:11.280975: Pseudo dice [np.float32(0.0)] 
2024-12-16 12:51:11.283514: Epoch time: 41.18 s 
2024-12-16 12:51:11.801259:  
2024-12-16 12:51:11.806279: Epoch 3 
2024-12-16 12:51:11.809286: Current learning rate: 0.00973 
2024-12-16 12:51:52.952096: train_loss 0.0008 
2024-12-16 12:51:52.958130: val_loss 0.0068 
2024-12-16 12:51:52.960649: Pseudo dice [np.float32(0.0)] 
2024-12-16 12:51:52.964161: Epoch time: 41.15 s 
2024-12-16 12:51:53.459740:  
2024-12-16 12:51:53.464308: Epoch 4 
2024-12-16 12:51:53.468319: Current learning rate: 0.00964 
2024-12-16 12:52:34.615866: train_loss -0.0054 
2024-12-16 12:52:34.622881: val_loss -0.0154 
2024-12-16 12:52:34.625917: Pseudo dice [np.float32(0.0)] 
2024-12-16 12:52:34.628426: Epoch time: 41.16 s 
2024-12-16 12:52:35.252971:  
2024-12-16 12:52:35.257486: Epoch 5 
2024-12-16 12:52:35.260494: Current learning rate: 0.00955 
2024-12-16 12:53:16.398520: train_loss -0.0079 
2024-12-16 12:53:16.404085: val_loss -0.0159 
2024-12-16 12:53:16.406590: Pseudo dice [np.float32(0.0)] 
2024-12-16 12:53:16.410100: Epoch time: 41.15 s 
2024-12-16 12:53:16.898896:  
2024-12-16 12:53:16.903409: Epoch 6 
2024-12-16 12:53:16.906420: Current learning rate: 0.00946 
2024-12-16 12:53:58.049695: train_loss -0.0016 
2024-12-16 12:53:58.055276: val_loss -0.0114 
2024-12-16 12:53:58.058786: Pseudo dice [np.float32(0.0128)] 
2024-12-16 12:53:58.061291: Epoch time: 41.15 s 
2024-12-16 12:53:58.063796: Yayy! New best EMA pseudo Dice: 0.0013000000035390258 
2024-12-16 12:53:58.731025:  
2024-12-16 12:53:58.736039: Epoch 7 
2024-12-16 12:53:58.739048: Current learning rate: 0.00937 
2024-12-16 12:54:39.900578: train_loss -0.0112 
2024-12-16 12:54:39.906638: val_loss -0.0086 
2024-12-16 12:54:39.910141: Pseudo dice [np.float32(0.0404)] 
2024-12-16 12:54:39.913148: Epoch time: 41.17 s 
2024-12-16 12:54:39.915653: Yayy! New best EMA pseudo Dice: 0.005200000014156103 
2024-12-16 12:54:40.591681:  
2024-12-16 12:54:40.596718: Epoch 8 
2024-12-16 12:54:40.599248: Current learning rate: 0.00928 
2024-12-16 12:55:21.757642: train_loss -0.0186 
2024-12-16 12:55:21.763155: val_loss -0.0304 
2024-12-16 12:55:21.766664: Pseudo dice [np.float32(0.0875)] 
2024-12-16 12:55:21.769178: Epoch time: 41.17 s 
2024-12-16 12:55:21.771683: Yayy! New best EMA pseudo Dice: 0.013399999588727951 
2024-12-16 12:55:22.445955:  
2024-12-16 12:55:22.450481: Epoch 9 
2024-12-16 12:55:22.454494: Current learning rate: 0.00919 
2024-12-16 12:56:03.618965: train_loss -0.0174 
2024-12-16 12:56:03.624059: val_loss -0.0182 
2024-12-16 12:56:03.627082: Pseudo dice [np.float32(0.0813)] 
2024-12-16 12:56:03.629477: Epoch time: 41.17 s 
2024-12-16 12:56:03.632987: Yayy! New best EMA pseudo Dice: 0.02019999921321869 
2024-12-16 12:56:04.273509:  
2024-12-16 12:56:04.278055: Epoch 10 
2024-12-16 12:56:04.281081: Current learning rate: 0.0091 
2024-12-16 12:56:45.434985: train_loss -0.0347 
2024-12-16 12:56:45.440998: val_loss -0.033 
2024-12-16 12:56:45.444511: Pseudo dice [np.float32(0.1479)] 
2024-12-16 12:56:45.447520: Epoch time: 41.16 s 
2024-12-16 12:56:45.451030: Yayy! New best EMA pseudo Dice: 0.032999999821186066 
2024-12-16 12:56:46.100645:  
2024-12-16 12:56:46.105158: Epoch 11 
2024-12-16 12:56:46.108172: Current learning rate: 0.009 
2024-12-16 12:57:27.266176: train_loss -0.0321 
2024-12-16 12:57:27.273241: val_loss -0.0551 
2024-12-16 12:57:27.276251: Pseudo dice [np.float32(0.1797)] 
2024-12-16 12:57:27.278761: Epoch time: 41.17 s 
2024-12-16 12:57:27.281267: Yayy! New best EMA pseudo Dice: 0.04769999906420708 
2024-12-16 12:57:27.931662:  
2024-12-16 12:57:27.937272: Epoch 12 
2024-12-16 12:57:27.939816: Current learning rate: 0.00891 
2024-12-16 12:58:09.094806: train_loss -0.0519 
2024-12-16 12:58:09.099936: val_loss -0.0871 
2024-12-16 12:58:09.102968: Pseudo dice [np.float32(0.2196)] 
2024-12-16 12:58:09.105490: Epoch time: 41.16 s 
2024-12-16 12:58:09.108009: Yayy! New best EMA pseudo Dice: 0.06480000168085098 
2024-12-16 12:58:09.909255:  
2024-12-16 12:58:09.914285: Epoch 13 
2024-12-16 12:58:09.917343: Current learning rate: 0.00882 
2024-12-16 12:58:51.053077: train_loss -0.0392 
2024-12-16 12:58:51.059590: val_loss -0.0323 
2024-12-16 12:58:51.063102: Pseudo dice [np.float32(0.1561)] 
2024-12-16 12:58:51.065609: Epoch time: 41.14 s 
2024-12-16 12:58:51.068115: Yayy! New best EMA pseudo Dice: 0.07400000095367432 
2024-12-16 12:58:51.734543:  
2024-12-16 12:58:51.739052: Epoch 14 
2024-12-16 12:58:51.742063: Current learning rate: 0.00873 
2024-12-16 12:59:32.890201: train_loss -0.0345 
2024-12-16 12:59:32.896267: val_loss 0.0144 
2024-12-16 12:59:32.899775: Pseudo dice [np.float32(0.0)] 
2024-12-16 12:59:32.902786: Epoch time: 41.16 s 
2024-12-16 12:59:33.430598:  
2024-12-16 12:59:33.435612: Epoch 15 
2024-12-16 12:59:33.438623: Current learning rate: 0.00864 
2024-12-16 13:00:14.568750: train_loss 0.011 
2024-12-16 13:00:14.573799: val_loss 0.0016 
2024-12-16 13:00:14.576305: Pseudo dice [np.float32(0.0149)] 
2024-12-16 13:00:14.579813: Epoch time: 41.14 s 
2024-12-16 13:00:15.094784:  
2024-12-16 13:00:15.100330: Epoch 16 
2024-12-16 13:00:15.102872: Current learning rate: 0.00855 
2024-12-16 13:00:56.249045: train_loss -0.0136 
2024-12-16 13:00:56.254119: val_loss -0.0246 
2024-12-16 13:00:56.257172: Pseudo dice [np.float32(0.1102)] 
2024-12-16 13:00:56.259723: Epoch time: 41.15 s 
2024-12-16 13:00:56.795968:  
2024-12-16 13:00:56.801019: Epoch 17 
2024-12-16 13:00:56.804082: Current learning rate: 0.00846 
2024-12-16 13:01:37.955780: train_loss -0.0456 
2024-12-16 13:01:37.961869: val_loss -0.0211 
2024-12-16 13:01:37.964920: Pseudo dice [np.float32(0.0862)] 
2024-12-16 13:01:37.967967: Epoch time: 41.16 s 
2024-12-16 13:01:38.495108:  
2024-12-16 13:01:38.500118: Epoch 18 
2024-12-16 13:01:38.502623: Current learning rate: 0.00836 
2024-12-16 13:02:19.656025: train_loss -0.0409 
2024-12-16 13:02:19.661099: val_loss -0.0296 
2024-12-16 13:02:19.664117: Pseudo dice [np.float32(0.1067)] 
2024-12-16 13:02:19.666622: Epoch time: 41.16 s 
2024-12-16 13:02:20.187473:  
2024-12-16 13:02:20.193017: Epoch 19 
2024-12-16 13:02:20.196062: Current learning rate: 0.00827 
2024-12-16 13:03:01.360612: train_loss -0.0447 
2024-12-16 13:03:01.368128: val_loss -0.0332 
2024-12-16 13:03:01.370633: Pseudo dice [np.float32(0.1147)] 
2024-12-16 13:03:01.374643: Epoch time: 41.17 s 
2024-12-16 13:03:01.377008: Yayy! New best EMA pseudo Dice: 0.07639999687671661 
2024-12-16 13:03:02.180060:  
2024-12-16 13:03:02.187649: Epoch 20 
2024-12-16 13:03:02.190705: Current learning rate: 0.00818 
2024-12-16 13:03:43.354903: train_loss -0.054 
2024-12-16 13:03:43.362420: val_loss -0.039 
2024-12-16 13:03:43.365929: Pseudo dice [np.float32(0.1726)] 
2024-12-16 13:03:43.368438: Epoch time: 41.17 s 
2024-12-16 13:03:43.370945: Yayy! New best EMA pseudo Dice: 0.0860000029206276 
2024-12-16 13:03:44.046066:  
2024-12-16 13:03:44.051079: Epoch 21 
2024-12-16 13:03:44.054090: Current learning rate: 0.00809 
2024-12-16 13:04:25.232545: train_loss -0.0661 
2024-12-16 13:04:25.238569: val_loss -0.0592 
2024-12-16 13:04:25.240583: Pseudo dice [np.float32(0.162)] 
2024-12-16 13:04:25.244619: Epoch time: 41.19 s 
2024-12-16 13:04:25.249633: Yayy! New best EMA pseudo Dice: 0.09359999746084213 
2024-12-16 13:04:25.909814:  
2024-12-16 13:04:25.914825: Epoch 22 
2024-12-16 13:04:25.917332: Current learning rate: 0.008 
2024-12-16 13:05:07.098555: train_loss -0.0715 
2024-12-16 13:05:07.103643: val_loss -0.0673 
2024-12-16 13:05:07.107185: Pseudo dice [np.float32(0.1699)] 
2024-12-16 13:05:07.109216: Epoch time: 41.19 s 
2024-12-16 13:05:07.113751: Yayy! New best EMA pseudo Dice: 0.10119999945163727 
2024-12-16 13:05:07.774740:  
2024-12-16 13:05:07.779587: Epoch 23 
2024-12-16 13:05:07.782093: Current learning rate: 0.0079 
2024-12-16 13:05:48.947972: train_loss -0.0665 
2024-12-16 13:05:48.954526: val_loss -0.0444 
2024-12-16 13:05:48.958059: Pseudo dice [np.float32(0.1659)] 
2024-12-16 13:05:48.961096: Epoch time: 41.17 s 
2024-12-16 13:05:48.963621: Yayy! New best EMA pseudo Dice: 0.10769999772310257 
2024-12-16 13:05:49.618683:  
2024-12-16 13:05:49.623700: Epoch 24 
2024-12-16 13:05:49.626714: Current learning rate: 0.00781 
2024-12-16 13:06:30.803867: train_loss -0.0754 
2024-12-16 13:06:30.811020: val_loss -0.0601 
2024-12-16 13:06:30.813565: Pseudo dice [np.float32(0.1663)] 
2024-12-16 13:06:30.817115: Epoch time: 41.19 s 
2024-12-16 13:06:30.819203: Yayy! New best EMA pseudo Dice: 0.1136000007390976 
2024-12-16 13:06:31.495984:  
2024-12-16 13:06:31.501000: Epoch 25 
2024-12-16 13:06:31.504013: Current learning rate: 0.00772 
2024-12-16 13:07:12.676661: train_loss -0.0722 
2024-12-16 13:07:12.681862: val_loss -0.0776 
2024-12-16 13:07:12.684915: Pseudo dice [np.float32(0.2488)] 
2024-12-16 13:07:12.687974: Epoch time: 41.18 s 
2024-12-16 13:07:12.690495: Yayy! New best EMA pseudo Dice: 0.12710000574588776 
2024-12-16 13:07:13.351369:  
2024-12-16 13:07:13.357491: Epoch 26 
2024-12-16 13:07:13.360534: Current learning rate: 0.00763 
2024-12-16 13:07:54.522614: train_loss -0.0764 
2024-12-16 13:07:54.529163: val_loss -0.039 
2024-12-16 13:07:54.531670: Pseudo dice [np.float32(0.1764)] 
2024-12-16 13:07:54.535678: Epoch time: 41.17 s 
2024-12-16 13:07:54.538184: Yayy! New best EMA pseudo Dice: 0.13199999928474426 
2024-12-16 13:07:55.197689:  
2024-12-16 13:07:55.202749: Epoch 27 
2024-12-16 13:07:55.206271: Current learning rate: 0.00753 
2024-12-16 13:08:36.371936: train_loss -0.0714 
2024-12-16 13:08:36.378461: val_loss -0.0596 
2024-12-16 13:08:36.382972: Pseudo dice [np.float32(0.232)] 
2024-12-16 13:08:36.385983: Epoch time: 41.18 s 
2024-12-16 13:08:36.389500: Yayy! New best EMA pseudo Dice: 0.1420000046491623 
2024-12-16 13:08:37.186912:  
2024-12-16 13:08:37.194999: Epoch 28 
2024-12-16 13:08:37.198550: Current learning rate: 0.00744 
2024-12-16 13:09:24.298843: train_loss -0.0851 
2024-12-16 13:09:24.303892: val_loss -0.0928 
2024-12-16 13:09:24.308975: Pseudo dice [np.float32(0.1881)] 
2024-12-16 13:09:24.312501: Epoch time: 47.11 s 
2024-12-16 13:09:24.315532: Yayy! New best EMA pseudo Dice: 0.14659999310970306 
2024-12-16 13:09:24.991435:  
2024-12-16 13:09:24.997005: Epoch 29 
2024-12-16 13:09:24.999547: Current learning rate: 0.00735 
2024-12-16 13:10:07.335769: train_loss -0.0857 
2024-12-16 13:10:07.340783: val_loss -0.0385 
2024-12-16 13:10:07.344294: Pseudo dice [np.float32(0.1683)] 
2024-12-16 13:10:07.347803: Epoch time: 42.34 s 
2024-12-16 13:10:07.350812: Yayy! New best EMA pseudo Dice: 0.14880000054836273 
2024-12-16 13:10:08.029362:  
2024-12-16 13:10:08.034439: Epoch 30 
2024-12-16 13:10:08.037980: Current learning rate: 0.00725 
2024-12-16 13:10:50.939606: train_loss -0.0839 
2024-12-16 13:10:50.945622: val_loss -0.0902 
2024-12-16 13:10:50.948632: Pseudo dice [np.float32(0.214)] 
2024-12-16 13:10:50.952146: Epoch time: 42.91 s 
2024-12-16 13:10:50.956157: Yayy! New best EMA pseudo Dice: 0.15530000627040863 
2024-12-16 13:10:51.636304:  
2024-12-16 13:10:51.641432: Epoch 31 
2024-12-16 13:10:51.644633: Current learning rate: 0.00716 
2024-12-16 13:11:33.654719: train_loss -0.0895 
2024-12-16 13:11:33.661235: val_loss -0.0834 
2024-12-16 13:11:33.664745: Pseudo dice [np.float32(0.1867)] 
2024-12-16 13:11:33.668761: Epoch time: 42.02 s 
2024-12-16 13:11:33.672275: Yayy! New best EMA pseudo Dice: 0.15850000083446503 
2024-12-16 13:11:34.391058:  
2024-12-16 13:11:34.397124: Epoch 32 
2024-12-16 13:11:34.400685: Current learning rate: 0.00707 
2024-12-16 13:12:16.345150: train_loss -0.0741 
2024-12-16 13:12:16.352247: val_loss -0.0951 
2024-12-16 13:12:16.355295: Pseudo dice [np.float32(0.2283)] 
2024-12-16 13:12:16.357831: Epoch time: 41.95 s 
2024-12-16 13:12:16.361880: Yayy! New best EMA pseudo Dice: 0.16539999842643738 
2024-12-16 13:12:17.078044:  
2024-12-16 13:12:17.083373: Epoch 33 
2024-12-16 13:12:17.087406: Current learning rate: 0.00697 
2024-12-16 13:12:59.801401: train_loss -0.1093 
2024-12-16 13:12:59.807436: val_loss -0.079 
2024-12-16 13:12:59.810956: Pseudo dice [np.float32(0.2005)] 
2024-12-16 13:12:59.813977: Epoch time: 42.72 s 
2024-12-16 13:12:59.816492: Yayy! New best EMA pseudo Dice: 0.1688999980688095 
2024-12-16 13:13:00.533504:  
2024-12-16 13:13:00.540073: Epoch 34 
2024-12-16 13:13:00.543626: Current learning rate: 0.00688 
