
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-18 21:12:53.686892: do_dummy_2d_data_aug: True 
2025-03-18 21:12:53.688895: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset027_ACDC\splits_final.json 
2025-03-18 21:12:53.695962: The split file contains 5 splits. 
2025-03-18 21:12:53.699963: Desired fold for training: 0 
2025-03-18 21:12:53.702963: This split has 160 training and 40 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_1_spacing_batchsize_3_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [48, 192, 160], 'median_image_size_in_voxels': [90.0, 370.0, 325.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset027_ACDC', 'plans_name': 'nnUNetPlans_1_spacing_batchsize_3', 'original_median_spacing_after_transp': [10.0, 1.5625, 1.5625], 'original_median_shape_after_transp': [9, 256, 216], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1488.0, 'mean': 123.30044555664062, 'median': 99.0, 'min': 0.0, 'percentile_00_5': 24.0, 'percentile_99_5': 615.0, 'std': 92.96476745605469}}} 
 
2025-03-18 21:13:00.967193: unpacking dataset... 
2025-03-18 21:13:01.146967: unpacking done... 
2025-03-18 21:13:03.744758:  
2025-03-18 21:13:03.751280: Epoch 0 
2025-03-18 21:13:03.754795: Current learning rate: 0.01 
2025-03-18 21:13:46.564388: train_loss 0.1633 
2025-03-18 21:13:46.569962: val_loss -0.1485 
2025-03-18 21:13:46.573038: Pseudo dice [np.float32(0.38), np.float32(0.5795), np.float32(0.6899)] 
2025-03-18 21:13:46.577072: Epoch time: 42.82 s 
2025-03-18 21:13:46.580080: Yayy! New best EMA pseudo Dice: 0.5497999787330627 
2025-03-18 21:13:47.239223:  
2025-03-18 21:13:47.244736: Epoch 1 
2025-03-18 21:13:47.248245: Current learning rate: 0.00999 
2025-03-18 21:14:25.436109: train_loss -0.2193 
2025-03-18 21:14:25.442127: val_loss -0.2593 
2025-03-18 21:14:25.446134: Pseudo dice [np.float32(0.48), np.float32(0.6341), np.float32(0.7668)] 
2025-03-18 21:14:25.449173: Epoch time: 38.2 s 
2025-03-18 21:14:25.453206: Yayy! New best EMA pseudo Dice: 0.5575000047683716 
2025-03-18 21:14:26.188592:  
2025-03-18 21:14:26.195176: Epoch 2 
2025-03-18 21:14:26.199222: Current learning rate: 0.00998 
2025-03-18 21:15:04.127189: train_loss -0.3835 
2025-03-18 21:15:04.132703: val_loss -0.4531 
2025-03-18 21:15:04.136744: Pseudo dice [np.float32(0.6562), np.float32(0.6969), np.float32(0.8575)] 
2025-03-18 21:15:04.139781: Epoch time: 37.94 s 
2025-03-18 21:15:04.142807: Yayy! New best EMA pseudo Dice: 0.5755000114440918 
2025-03-18 21:15:04.920566:  
2025-03-18 21:15:04.925576: Epoch 3 
2025-03-18 21:15:04.929085: Current learning rate: 0.00997 
2025-03-18 21:15:43.055731: train_loss -0.4772 
2025-03-18 21:15:43.061336: val_loss -0.5578 
2025-03-18 21:15:43.065391: Pseudo dice [np.float32(0.7562), np.float32(0.7525), np.float32(0.9001)] 
2025-03-18 21:15:43.068418: Epoch time: 38.14 s 
2025-03-18 21:15:43.071928: Yayy! New best EMA pseudo Dice: 0.5982000231742859 
2025-03-18 21:15:43.956397:  
2025-03-18 21:15:43.962413: Epoch 4 
2025-03-18 21:15:43.964917: Current learning rate: 0.00996 
2025-03-18 21:16:22.438331: train_loss -0.5647 
2025-03-18 21:16:22.444887: val_loss -0.5559 
2025-03-18 21:16:22.448921: Pseudo dice [np.float32(0.7476), np.float32(0.7729), np.float32(0.8885)] 
2025-03-18 21:16:22.452450: Epoch time: 38.48 s 
2025-03-18 21:16:22.455973: Yayy! New best EMA pseudo Dice: 0.6187000274658203 
2025-03-18 21:16:23.221862:  
2025-03-18 21:16:23.227880: Epoch 5 
2025-03-18 21:16:23.232898: Current learning rate: 0.00995 
2025-03-18 21:17:01.507270: train_loss -0.5901 
2025-03-18 21:17:01.515301: val_loss -0.6038 
2025-03-18 21:17:01.518869: Pseudo dice [np.float32(0.8087), np.float32(0.7871), np.float32(0.9071)] 
2025-03-18 21:17:01.521422: Epoch time: 38.29 s 
2025-03-18 21:17:01.524931: Yayy! New best EMA pseudo Dice: 0.6402000188827515 
2025-03-18 21:17:02.279873:  
2025-03-18 21:17:02.286483: Epoch 6 
2025-03-18 21:17:02.289080: Current learning rate: 0.00995 
2025-03-18 21:17:40.221523: train_loss -0.5956 
2025-03-18 21:17:40.228153: val_loss -0.6327 
2025-03-18 21:17:40.231758: Pseudo dice [np.float32(0.805), np.float32(0.8099), np.float32(0.9195)] 
2025-03-18 21:17:40.234796: Epoch time: 37.94 s 
2025-03-18 21:17:40.238336: Yayy! New best EMA pseudo Dice: 0.6607000231742859 
2025-03-18 21:17:40.994716:  
2025-03-18 21:17:41.000288: Epoch 7 
2025-03-18 21:17:41.003907: Current learning rate: 0.00994 
2025-03-18 21:18:20.651905: train_loss -0.5981 
2025-03-18 21:18:20.662622: val_loss -0.6143 
2025-03-18 21:18:20.670316: Pseudo dice [np.float32(0.7854), np.float32(0.8054), np.float32(0.9223)] 
2025-03-18 21:18:20.677450: Epoch time: 39.66 s 
2025-03-18 21:18:20.689503: Yayy! New best EMA pseudo Dice: 0.6783999800682068 
2025-03-18 21:18:21.869832:  
2025-03-18 21:18:21.875410: Epoch 8 
2025-03-18 21:18:21.880524: Current learning rate: 0.00993 
2025-03-18 21:19:01.524879: train_loss -0.6243 
2025-03-18 21:19:01.530904: val_loss -0.5923 
2025-03-18 21:19:01.535929: Pseudo dice [np.float32(0.7834), np.float32(0.8026), np.float32(0.8956)] 
2025-03-18 21:19:01.539971: Epoch time: 39.66 s 
2025-03-18 21:19:01.544009: Yayy! New best EMA pseudo Dice: 0.6933000087738037 
2025-03-18 21:19:02.458450:  
2025-03-18 21:19:02.464000: Epoch 9 
2025-03-18 21:19:02.467565: Current learning rate: 0.00992 
2025-03-18 21:19:41.509988: train_loss -0.6399 
2025-03-18 21:19:41.517284: val_loss -0.6484 
2025-03-18 21:19:41.523309: Pseudo dice [np.float32(0.8154), np.float32(0.8389), np.float32(0.9239)] 
2025-03-18 21:19:41.529831: Epoch time: 39.05 s 
2025-03-18 21:19:41.534846: Yayy! New best EMA pseudo Dice: 0.7099000215530396 
2025-03-18 21:19:42.352045:  
2025-03-18 21:19:42.358571: Epoch 10 
2025-03-18 21:19:42.366097: Current learning rate: 0.00991 
2025-03-18 21:20:21.412898: train_loss -0.671 
2025-03-18 21:20:21.421472: val_loss -0.6453 
2025-03-18 21:20:21.429597: Pseudo dice [np.float32(0.8318), np.float32(0.8354), np.float32(0.9323)] 
2025-03-18 21:20:21.435683: Epoch time: 39.06 s 
2025-03-18 21:20:21.441234: Yayy! New best EMA pseudo Dice: 0.725600004196167 
2025-03-18 21:20:22.224073:  
2025-03-18 21:20:22.231601: Epoch 11 
2025-03-18 21:20:22.235115: Current learning rate: 0.0099 
