
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-12-08 16:17:57.020535: do_dummy_2d_data_aug: False 
2024-12-08 16:17:57.022534: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2024-12-08 16:17:57.031169: The split file contains 5 splits. 
2024-12-08 16:17:57.033168: Desired fold for training: 0 
2024-12-08 16:17:57.036168: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 366, 'patch_size': [56, 40], 'median_image_size_in_voxels': [50.0, 35.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2024-12-08 16:18:03.575867: unpacking dataset... 
2024-12-08 16:18:03.793340: unpacking done... 
2024-12-08 16:18:04.757689:  
2024-12-08 16:18:04.761696: Epoch 0 
2024-12-08 16:18:04.764202: Current learning rate: 0.01 
2024-12-08 16:18:14.520807: train_loss -0.2751 
2024-12-08 16:18:14.526353: val_loss -0.7495 
2024-12-08 16:18:14.529384: Pseudo dice [np.float32(0.8435), np.float32(0.8286)] 
2024-12-08 16:18:14.531923: Epoch time: 9.76 s 
2024-12-08 16:18:14.535008: Yayy! New best EMA pseudo Dice: 0.8360999822616577 
2024-12-08 16:18:15.176283:  
2024-12-08 16:18:15.181327: Epoch 1 
2024-12-08 16:18:15.184382: Current learning rate: 0.00991 
2024-12-08 16:18:24.198471: train_loss -0.812 
2024-12-08 16:18:24.204494: val_loss -0.7976 
2024-12-08 16:18:24.207003: Pseudo dice [np.float32(0.8694), np.float32(0.8578)] 
2024-12-08 16:18:24.210152: Epoch time: 9.02 s 
2024-12-08 16:18:24.213658: Yayy! New best EMA pseudo Dice: 0.8388000130653381 
2024-12-08 16:18:24.935809:  
2024-12-08 16:18:24.940823: Epoch 2 
2024-12-08 16:18:24.943332: Current learning rate: 0.00982 
2024-12-08 16:18:33.971557: train_loss -0.8516 
2024-12-08 16:18:33.978254: val_loss -0.8047 
2024-12-08 16:18:33.981791: Pseudo dice [np.float32(0.8767), np.float32(0.8627)] 
2024-12-08 16:18:33.984297: Epoch time: 9.04 s 
2024-12-08 16:18:33.987808: Yayy! New best EMA pseudo Dice: 0.8418999910354614 
2024-12-08 16:18:34.748028:  
2024-12-08 16:18:34.753136: Epoch 3 
2024-12-08 16:18:34.756181: Current learning rate: 0.00973 
2024-12-08 16:18:43.819232: train_loss -0.8731 
2024-12-08 16:18:43.824351: val_loss -0.7996 
2024-12-08 16:18:43.826899: Pseudo dice [np.float32(0.8765), np.float32(0.8577)] 
2024-12-08 16:18:43.831160: Epoch time: 9.07 s 
2024-12-08 16:18:43.834206: Yayy! New best EMA pseudo Dice: 0.8443999886512756 
2024-12-08 16:18:44.577904:  
2024-12-08 16:18:44.585975: Epoch 4 
2024-12-08 16:18:44.589516: Current learning rate: 0.00964 
2024-12-08 16:18:53.599963: train_loss -0.8885 
2024-12-08 16:18:53.606089: val_loss -0.7969 
2024-12-08 16:18:53.609263: Pseudo dice [np.float32(0.8766), np.float32(0.858)] 
2024-12-08 16:18:53.612302: Epoch time: 9.02 s 
2024-12-08 16:18:53.615389: Yayy! New best EMA pseudo Dice: 0.8467000126838684 
2024-12-08 16:18:54.355752:  
2024-12-08 16:18:54.361397: Epoch 5 
2024-12-08 16:18:54.364449: Current learning rate: 0.00955 
2024-12-08 16:19:03.442982: train_loss -0.8994 
2024-12-08 16:19:03.448121: val_loss -0.7933 
2024-12-08 16:19:03.450651: Pseudo dice [np.float32(0.876), np.float32(0.857)] 
2024-12-08 16:19:03.454057: Epoch time: 9.09 s 
2024-12-08 16:19:03.457563: Yayy! New best EMA pseudo Dice: 0.8486999869346619 
2024-12-08 16:19:04.253206:  
2024-12-08 16:19:04.258358: Epoch 6 
2024-12-08 16:19:04.261423: Current learning rate: 0.00946 
2024-12-08 16:19:13.227340: train_loss -0.9076 
2024-12-08 16:19:13.231890: val_loss -0.7892 
2024-12-08 16:19:13.236434: Pseudo dice [np.float32(0.8759), np.float32(0.8556)] 
2024-12-08 16:19:13.238944: Epoch time: 8.97 s 
2024-12-08 16:19:13.241960: Yayy! New best EMA pseudo Dice: 0.8503999710083008 
2024-12-08 16:19:13.956352:  
2024-12-08 16:19:13.961932: Epoch 7 
2024-12-08 16:19:13.965031: Current learning rate: 0.00937 
2024-12-08 16:19:22.986642: train_loss -0.9148 
2024-12-08 16:19:22.993364: val_loss -0.7876 
2024-12-08 16:19:22.995913: Pseudo dice [np.float32(0.8759), np.float32(0.8563)] 
2024-12-08 16:19:22.999476: Epoch time: 9.03 s 
2024-12-08 16:19:23.002132: Yayy! New best EMA pseudo Dice: 0.8519999980926514 
2024-12-08 16:19:23.741704:  
2024-12-08 16:19:23.746818: Epoch 8 
2024-12-08 16:19:23.749901: Current learning rate: 0.00928 
2024-12-08 16:19:32.771513: train_loss -0.9198 
2024-12-08 16:19:32.779100: val_loss -0.7842 
2024-12-08 16:19:32.782129: Pseudo dice [np.float32(0.8759), np.float32(0.8546)] 
2024-12-08 16:19:32.784642: Epoch time: 9.03 s 
2024-12-08 16:19:32.788226: Yayy! New best EMA pseudo Dice: 0.8532999753952026 
2024-12-08 16:19:33.537375:  
2024-12-08 16:19:33.542501: Epoch 9 
2024-12-08 16:19:33.545178: Current learning rate: 0.00919 
2024-12-08 16:19:42.640114: train_loss -0.9244 
2024-12-08 16:19:42.646130: val_loss -0.7829 
2024-12-08 16:19:42.649139: Pseudo dice [np.float32(0.8777), np.float32(0.8559)] 
2024-12-08 16:19:42.651646: Epoch time: 9.1 s 
2024-12-08 16:19:42.655160: Yayy! New best EMA pseudo Dice: 0.8546000123023987 
2024-12-08 16:19:43.369946:  
2024-12-08 16:19:43.374740: Epoch 10 
2024-12-08 16:19:43.378442: Current learning rate: 0.0091 
2024-12-08 16:19:52.382410: train_loss -0.9282 
2024-12-08 16:19:52.389942: val_loss -0.7793 
2024-12-08 16:19:52.392448: Pseudo dice [np.float32(0.8755), np.float32(0.8553)] 
2024-12-08 16:19:52.396540: Epoch time: 9.01 s 
2024-12-08 16:19:52.399047: Yayy! New best EMA pseudo Dice: 0.8557000160217285 
2024-12-08 16:19:53.126673:  
2024-12-08 16:19:53.131895: Epoch 11 
2024-12-08 16:19:53.135406: Current learning rate: 0.009 
2024-12-08 16:20:02.205575: train_loss -0.9317 
2024-12-08 16:20:02.211168: val_loss -0.7735 
2024-12-08 16:20:02.215242: Pseudo dice [np.float32(0.8735), np.float32(0.854)] 
2024-12-08 16:20:02.218412: Epoch time: 9.08 s 
2024-12-08 16:20:02.221428: Yayy! New best EMA pseudo Dice: 0.8565000295639038 
2024-12-08 16:20:02.944005:  
2024-12-08 16:20:02.949024: Epoch 12 
2024-12-08 16:20:02.951534: Current learning rate: 0.00891 
2024-12-08 16:20:11.974862: train_loss -0.9348 
2024-12-08 16:20:11.980882: val_loss -0.773 
2024-12-08 16:20:11.983778: Pseudo dice [np.float32(0.8738), np.float32(0.854)] 
2024-12-08 16:20:11.987839: Epoch time: 9.03 s 
2024-12-08 16:20:11.991409: Yayy! New best EMA pseudo Dice: 0.8572999835014343 
2024-12-08 16:20:12.839937:  
2024-12-08 16:20:12.845949: Epoch 13 
2024-12-08 16:20:12.848960: Current learning rate: 0.00882 
2024-12-08 16:20:21.969947: train_loss -0.9375 
2024-12-08 16:20:21.976550: val_loss -0.7707 
2024-12-08 16:20:21.979062: Pseudo dice [np.float32(0.8722), np.float32(0.8543)] 
2024-12-08 16:20:21.982682: Epoch time: 9.13 s 
2024-12-08 16:20:21.985221: Yayy! New best EMA pseudo Dice: 0.8579000234603882 
2024-12-08 16:20:22.712199:  
2024-12-08 16:20:22.718213: Epoch 14 
2024-12-08 16:20:22.721225: Current learning rate: 0.00873 
2024-12-08 16:20:32.365809: train_loss -0.9399 
2024-12-08 16:20:32.371842: val_loss -0.772 
2024-12-08 16:20:32.375454: Pseudo dice [np.float32(0.8743), np.float32(0.8565)] 
2024-12-08 16:20:32.378504: Epoch time: 9.65 s 
2024-12-08 16:20:32.381607: Yayy! New best EMA pseudo Dice: 0.8586000204086304 
2024-12-08 16:20:33.159112:  
2024-12-08 16:20:33.164709: Epoch 15 
2024-12-08 16:20:33.167308: Current learning rate: 0.00864 
2024-12-08 16:20:42.937308: train_loss -0.9423 
2024-12-08 16:20:42.942320: val_loss -0.7683 
2024-12-08 16:20:42.946479: Pseudo dice [np.float32(0.8727), np.float32(0.8552)] 
2024-12-08 16:20:42.948985: Epoch time: 9.78 s 
2024-12-08 16:20:42.952503: Yayy! New best EMA pseudo Dice: 0.8590999841690063 
2024-12-08 16:20:43.691918:  
2024-12-08 16:20:43.697072: Epoch 16 
2024-12-08 16:20:43.699581: Current learning rate: 0.00855 
2024-12-08 16:20:52.939104: train_loss -0.9446 
2024-12-08 16:20:52.945907: val_loss -0.7646 
2024-12-08 16:20:52.948821: Pseudo dice [np.float32(0.8718), np.float32(0.8539)] 
2024-12-08 16:20:52.952345: Epoch time: 9.25 s 
2024-12-08 16:20:52.955886: Yayy! New best EMA pseudo Dice: 0.859499990940094 
2024-12-08 16:20:53.711329:  
2024-12-08 16:20:53.716745: Epoch 17 
2024-12-08 16:20:53.720345: Current learning rate: 0.00846 
2024-12-08 16:21:02.785078: train_loss -0.9466 
2024-12-08 16:21:02.791447: val_loss -0.7634 
2024-12-08 16:21:02.794731: Pseudo dice [np.float32(0.8717), np.float32(0.8551)] 
2024-12-08 16:21:02.797270: Epoch time: 9.07 s 
2024-12-08 16:21:02.801207: Yayy! New best EMA pseudo Dice: 0.8598999977111816 
2024-12-08 16:21:03.541498:  
2024-12-08 16:21:03.548131: Epoch 18 
2024-12-08 16:21:03.550693: Current learning rate: 0.00836 
2024-12-08 16:21:12.663647: train_loss -0.9481 
2024-12-08 16:21:12.668736: val_loss -0.7619 
2024-12-08 16:21:12.673807: Pseudo dice [np.float32(0.8712), np.float32(0.853)] 
2024-12-08 16:21:12.677387: Epoch time: 9.12 s 
2024-12-08 16:21:12.679925: Yayy! New best EMA pseudo Dice: 0.8600999712944031 
2024-12-08 16:21:13.409443:  
2024-12-08 16:21:13.414991: Epoch 19 
2024-12-08 16:21:13.418563: Current learning rate: 0.00827 
2024-12-08 16:21:22.584424: train_loss -0.9497 
2024-12-08 16:21:22.590462: val_loss -0.7628 
2024-12-08 16:21:22.593350: Pseudo dice [np.float32(0.8726), np.float32(0.856)] 
2024-12-08 16:21:22.596714: Epoch time: 9.17 s 
2024-12-08 16:21:22.599330: Yayy! New best EMA pseudo Dice: 0.8604999780654907 
2024-12-08 16:21:23.334100:  
2024-12-08 16:21:23.339168: Epoch 20 
2024-12-08 16:21:23.342028: Current learning rate: 0.00818 
2024-12-08 16:21:32.421135: train_loss -0.9512 
2024-12-08 16:21:32.426668: val_loss -0.7618 
2024-12-08 16:21:32.430013: Pseudo dice [np.float32(0.8722), np.float32(0.8554)] 
2024-12-08 16:21:32.433540: Epoch time: 9.09 s 
2024-12-08 16:21:32.436594: Yayy! New best EMA pseudo Dice: 0.8608999848365784 
2024-12-08 16:21:33.320536:  
2024-12-08 16:21:33.326436: Epoch 21 
2024-12-08 16:21:33.329477: Current learning rate: 0.00809 
2024-12-08 16:21:42.340770: train_loss -0.9527 
2024-12-08 16:21:42.346320: val_loss -0.763 
2024-12-08 16:21:42.350149: Pseudo dice [np.float32(0.873), np.float32(0.8552)] 
2024-12-08 16:21:42.352980: Epoch time: 9.02 s 
2024-12-08 16:21:42.356100: Yayy! New best EMA pseudo Dice: 0.8611999750137329 
2024-12-08 16:21:43.065378:  
2024-12-08 16:21:43.070656: Epoch 22 
2024-12-08 16:21:43.073222: Current learning rate: 0.008 
2024-12-08 16:21:52.001901: train_loss -0.9542 
2024-12-08 16:21:52.009536: val_loss -0.7576 
2024-12-08 16:21:52.013436: Pseudo dice [np.float32(0.8717), np.float32(0.8544)] 
2024-12-08 16:21:52.016000: Epoch time: 8.94 s 
2024-12-08 16:21:52.019606: Yayy! New best EMA pseudo Dice: 0.8614000082015991 
2024-12-08 16:21:52.724857:  
2024-12-08 16:21:52.730483: Epoch 23 
2024-12-08 16:21:52.733554: Current learning rate: 0.0079 
2024-12-08 16:22:01.882420: train_loss -0.9555 
2024-12-08 16:22:01.888150: val_loss -0.7582 
2024-12-08 16:22:01.892910: Pseudo dice [np.float32(0.8721), np.float32(0.8545)] 
2024-12-08 16:22:01.896267: Epoch time: 9.16 s 
2024-12-08 16:22:01.899362: Yayy! New best EMA pseudo Dice: 0.8615999817848206 
2024-12-08 16:22:02.609446:  
2024-12-08 16:22:02.614554: Epoch 24 
2024-12-08 16:22:02.617727: Current learning rate: 0.00781 
2024-12-08 16:22:11.740872: train_loss -0.9563 
2024-12-08 16:22:11.747157: val_loss -0.7564 
2024-12-08 16:22:11.750474: Pseudo dice [np.float32(0.8716), np.float32(0.8544)] 
2024-12-08 16:22:11.753391: Epoch time: 9.13 s 
2024-12-08 16:22:11.756326: Yayy! New best EMA pseudo Dice: 0.8616999983787537 
2024-12-08 16:22:12.477505:  
2024-12-08 16:22:12.483448: Epoch 25 
2024-12-08 16:22:12.486516: Current learning rate: 0.00772 
2024-12-08 16:22:21.557949: train_loss -0.9578 
2024-12-08 16:22:21.564414: val_loss -0.7564 
2024-12-08 16:22:21.567932: Pseudo dice [np.float32(0.8728), np.float32(0.8546)] 
2024-12-08 16:22:21.570405: Epoch time: 9.08 s 
2024-12-08 16:22:21.574452: Yayy! New best EMA pseudo Dice: 0.8618999719619751 
2024-12-08 16:22:22.281686:  
2024-12-08 16:22:22.287162: Epoch 26 
2024-12-08 16:22:22.290678: Current learning rate: 0.00763 
2024-12-08 16:22:31.336224: train_loss -0.9588 
2024-12-08 16:22:31.342060: val_loss -0.7535 
2024-12-08 16:22:31.345938: Pseudo dice [np.float32(0.8714), np.float32(0.8532)] 
2024-12-08 16:22:31.350245: Epoch time: 9.06 s 
2024-12-08 16:22:31.353345: Yayy! New best EMA pseudo Dice: 0.8619999885559082 
2024-12-08 16:22:32.064518:  
2024-12-08 16:22:32.069576: Epoch 27 
2024-12-08 16:22:32.072888: Current learning rate: 0.00753 
2024-12-08 16:22:41.112984: train_loss -0.9597 
2024-12-08 16:22:41.118589: val_loss -0.7543 
2024-12-08 16:22:41.122820: Pseudo dice [np.float32(0.8711), np.float32(0.854)] 
2024-12-08 16:22:41.125929: Epoch time: 9.05 s 
2024-12-08 16:22:41.129968: Yayy! New best EMA pseudo Dice: 0.8619999885559082 
2024-12-08 16:22:41.854302:  
2024-12-08 16:22:41.859931: Epoch 28 
2024-12-08 16:22:41.863816: Current learning rate: 0.00744 
2024-12-08 16:22:50.925804: train_loss -0.9607 
2024-12-08 16:22:50.933150: val_loss -0.7539 
2024-12-08 16:22:50.936697: Pseudo dice [np.float32(0.8716), np.float32(0.854)] 
2024-12-08 16:22:50.940571: Epoch time: 9.07 s 
2024-12-08 16:22:50.944111: Yayy! New best EMA pseudo Dice: 0.8621000051498413 
2024-12-08 16:22:51.809948:  
2024-12-08 16:22:51.815085: Epoch 29 
2024-12-08 16:22:51.818650: Current learning rate: 0.00735 
2024-12-08 16:23:00.813752: train_loss -0.9617 
2024-12-08 16:23:00.820200: val_loss -0.754 
2024-12-08 16:23:00.823714: Pseudo dice [np.float32(0.872), np.float32(0.8531)] 
2024-12-08 16:23:00.827720: Epoch time: 9.0 s 
2024-12-08 16:23:00.831055: Yayy! New best EMA pseudo Dice: 0.8621000051498413 
2024-12-08 16:23:01.544844:  
2024-12-08 16:23:01.550325: Epoch 30 
2024-12-08 16:23:01.554136: Current learning rate: 0.00725 
2024-12-08 16:23:10.573341: train_loss -0.9625 
2024-12-08 16:23:10.579730: val_loss -0.7507 
2024-12-08 16:23:10.582815: Pseudo dice [np.float32(0.8714), np.float32(0.853)] 
2024-12-08 16:23:10.586267: Epoch time: 9.03 s 
2024-12-08 16:23:10.589282: Yayy! New best EMA pseudo Dice: 0.8621000051498413 
2024-12-08 16:23:11.318289:  
2024-12-08 16:23:11.323748: Epoch 31 
2024-12-08 16:23:11.326776: Current learning rate: 0.00716 
2024-12-08 16:23:20.315267: train_loss -0.9634 
2024-12-08 16:23:20.321040: val_loss -0.7518 
2024-12-08 16:23:20.324095: Pseudo dice [np.float32(0.8712), np.float32(0.8532)] 
2024-12-08 16:23:20.326638: Epoch time: 9.0 s 
2024-12-08 16:23:20.330419: Yayy! New best EMA pseudo Dice: 0.8622000217437744 
2024-12-08 16:23:21.050948:  
2024-12-08 16:23:21.056715: Epoch 32 
2024-12-08 16:23:21.059081: Current learning rate: 0.00707 
2024-12-08 16:23:30.116191: train_loss -0.9641 
2024-12-08 16:23:30.122411: val_loss -0.7521 
2024-12-08 16:23:30.125710: Pseudo dice [np.float32(0.8713), np.float32(0.854)] 
2024-12-08 16:23:30.128261: Epoch time: 9.07 s 
2024-12-08 16:23:30.131221: Yayy! New best EMA pseudo Dice: 0.8622000217437744 
2024-12-08 16:23:30.863555:  
2024-12-08 16:23:30.869601: Epoch 33 
2024-12-08 16:23:30.873210: Current learning rate: 0.00697 
2024-12-08 16:23:40.020981: train_loss -0.9646 
2024-12-08 16:23:40.026062: val_loss -0.7508 
2024-12-08 16:23:40.029620: Pseudo dice [np.float32(0.8709), np.float32(0.8535)] 
2024-12-08 16:23:40.031151: Epoch time: 9.16 s 
2024-12-08 16:23:40.035706: Yayy! New best EMA pseudo Dice: 0.8622000217437744 
2024-12-08 16:23:40.822815:  
2024-12-08 16:23:40.828449: Epoch 34 
2024-12-08 16:23:40.832036: Current learning rate: 0.00688 
2024-12-08 16:23:50.003805: train_loss -0.9653 
2024-12-08 16:23:50.010258: val_loss -0.7511 
2024-12-08 16:23:50.013810: Pseudo dice [np.float32(0.8705), np.float32(0.8533)] 
2024-12-08 16:23:50.016365: Epoch time: 9.18 s 
2024-12-08 16:23:50.766171:  
2024-12-08 16:23:50.771247: Epoch 35 
2024-12-08 16:23:50.773840: Current learning rate: 0.00679 
2024-12-08 16:23:59.759810: train_loss -0.966 
2024-12-08 16:23:59.765952: val_loss -0.7507 
2024-12-08 16:23:59.769515: Pseudo dice [np.float32(0.8714), np.float32(0.8531)] 
2024-12-08 16:23:59.772071: Epoch time: 8.99 s 
2024-12-08 16:24:00.633917:  
2024-12-08 16:24:00.638962: Epoch 36 
2024-12-08 16:24:00.642498: Current learning rate: 0.00669 
2024-12-08 16:24:09.804794: train_loss -0.9666 
2024-12-08 16:24:09.809684: val_loss -0.747 
2024-12-08 16:24:09.812270: Pseudo dice [np.float32(0.8694), np.float32(0.8527)] 
2024-12-08 16:24:09.816754: Epoch time: 9.17 s 
2024-12-08 16:24:10.543437:  
2024-12-08 16:24:10.549201: Epoch 37 
2024-12-08 16:24:10.551635: Current learning rate: 0.0066 
2024-12-08 16:24:19.607143: train_loss -0.9671 
2024-12-08 16:24:19.612060: val_loss -0.7487 
2024-12-08 16:24:19.615781: Pseudo dice [np.float32(0.8707), np.float32(0.853)] 
2024-12-08 16:24:19.619089: Epoch time: 9.06 s 
2024-12-08 16:24:20.337208:  
2024-12-08 16:24:20.343159: Epoch 38 
2024-12-08 16:24:20.347039: Current learning rate: 0.0065 
2024-12-08 16:24:29.351822: train_loss -0.9677 
2024-12-08 16:24:29.357262: val_loss -0.7491 
2024-12-08 16:24:29.361195: Pseudo dice [np.float32(0.8714), np.float32(0.8532)] 
2024-12-08 16:24:29.364232: Epoch time: 9.01 s 
2024-12-08 16:24:30.086952:  
2024-12-08 16:24:30.092937: Epoch 39 
2024-12-08 16:24:30.095294: Current learning rate: 0.00641 
2024-12-08 16:24:39.144063: train_loss -0.9683 
2024-12-08 16:24:39.149541: val_loss -0.7489 
2024-12-08 16:24:39.152089: Pseudo dice [np.float32(0.8722), np.float32(0.8528)] 
2024-12-08 16:24:39.156545: Epoch time: 9.06 s 
2024-12-08 16:24:39.902481:  
2024-12-08 16:24:39.908163: Epoch 40 
2024-12-08 16:24:39.912097: Current learning rate: 0.00631 
2024-12-08 16:24:49.064413: train_loss -0.9687 
2024-12-08 16:24:49.071919: val_loss -0.747 
2024-12-08 16:24:49.077003: Pseudo dice [np.float32(0.8695), np.float32(0.853)] 
2024-12-08 16:24:49.079379: Epoch time: 9.16 s 
2024-12-08 16:24:49.792727:  
2024-12-08 16:24:49.798138: Epoch 41 
2024-12-08 16:24:49.801491: Current learning rate: 0.00622 
2024-12-08 16:24:58.850369: train_loss -0.9694 
2024-12-08 16:24:58.857529: val_loss -0.7472 
2024-12-08 16:24:58.862360: Pseudo dice [np.float32(0.8713), np.float32(0.8519)] 
2024-12-08 16:24:58.865885: Epoch time: 9.06 s 
2024-12-08 16:24:59.566423:  
2024-12-08 16:24:59.572068: Epoch 42 
2024-12-08 16:24:59.575015: Current learning rate: 0.00612 
2024-12-08 16:25:08.662616: train_loss -0.97 
2024-12-08 16:25:08.668565: val_loss -0.7442 
2024-12-08 16:25:08.671092: Pseudo dice [np.float32(0.8708), np.float32(0.8518)] 
2024-12-08 16:25:08.674477: Epoch time: 9.1 s 
2024-12-08 16:25:09.373803:  
2024-12-08 16:25:09.379557: Epoch 43 
2024-12-08 16:25:09.382666: Current learning rate: 0.00603 
2024-12-08 16:25:18.347674: train_loss -0.9703 
2024-12-08 16:25:18.354193: val_loss -0.7451 
2024-12-08 16:25:18.357677: Pseudo dice [np.float32(0.8704), np.float32(0.8526)] 
2024-12-08 16:25:18.360188: Epoch time: 8.97 s 
2024-12-08 16:25:19.214893:  
2024-12-08 16:25:19.219833: Epoch 44 
2024-12-08 16:25:19.222183: Current learning rate: 0.00593 
2024-12-08 16:25:28.272343: train_loss -0.9708 
2024-12-08 16:25:28.278190: val_loss -0.7437 
2024-12-08 16:25:28.280528: Pseudo dice [np.float32(0.8696), np.float32(0.8532)] 
2024-12-08 16:25:28.284084: Epoch time: 9.06 s 
2024-12-08 16:25:28.982350:  
2024-12-08 16:25:28.987923: Epoch 45 
2024-12-08 16:25:28.990807: Current learning rate: 0.00584 
2024-12-08 16:25:38.000363: train_loss -0.9712 
2024-12-08 16:25:38.005267: val_loss -0.7475 
2024-12-08 16:25:38.009327: Pseudo dice [np.float32(0.8719), np.float32(0.8537)] 
2024-12-08 16:25:38.011779: Epoch time: 9.02 s 
2024-12-08 16:25:38.694362:  
2024-12-08 16:25:38.698438: Epoch 46 
2024-12-08 16:25:38.702747: Current learning rate: 0.00574 
2024-12-08 16:25:47.718852: train_loss -0.9718 
2024-12-08 16:25:47.723609: val_loss -0.7463 
2024-12-08 16:25:47.727062: Pseudo dice [np.float32(0.8709), np.float32(0.8529)] 
2024-12-08 16:25:47.730717: Epoch time: 9.02 s 
2024-12-08 16:25:48.402746:  
2024-12-08 16:25:48.408245: Epoch 47 
2024-12-08 16:25:48.410799: Current learning rate: 0.00565 
2024-12-08 16:25:57.417228: train_loss -0.9722 
2024-12-08 16:25:57.424591: val_loss -0.7424 
2024-12-08 16:25:57.427127: Pseudo dice [np.float32(0.8697), np.float32(0.8518)] 
2024-12-08 16:25:57.429671: Epoch time: 9.02 s 
2024-12-08 16:25:58.127654:  
2024-12-08 16:25:58.133071: Epoch 48 
2024-12-08 16:25:58.135854: Current learning rate: 0.00555 
2024-12-08 16:26:07.292911: train_loss -0.9727 
2024-12-08 16:26:07.298428: val_loss -0.7442 
2024-12-08 16:26:07.302024: Pseudo dice [np.float32(0.8707), np.float32(0.8538)] 
2024-12-08 16:26:07.304351: Epoch time: 9.17 s 
2024-12-08 16:26:08.018711:  
2024-12-08 16:26:08.023520: Epoch 49 
2024-12-08 16:26:08.026519: Current learning rate: 0.00546 
2024-12-08 16:26:17.039847: train_loss -0.9729 
2024-12-08 16:26:17.045913: val_loss -0.7439 
2024-12-08 16:26:17.049319: Pseudo dice [np.float32(0.8717), np.float32(0.8517)] 
2024-12-08 16:26:17.051777: Epoch time: 9.02 s 
2024-12-08 16:26:17.777265:  
2024-12-08 16:26:17.782788: Epoch 50 
2024-12-08 16:26:17.785810: Current learning rate: 0.00536 
2024-12-08 16:26:26.728373: train_loss -0.973 
2024-12-08 16:26:26.734089: val_loss -0.7437 
2024-12-08 16:26:26.737058: Pseudo dice [np.float32(0.8718), np.float32(0.8512)] 
2024-12-08 16:26:26.739606: Epoch time: 8.95 s 
2024-12-08 16:26:27.441598:  
2024-12-08 16:26:27.446651: Epoch 51 
2024-12-08 16:26:27.449274: Current learning rate: 0.00526 
2024-12-08 16:26:36.437225: train_loss -0.9736 
2024-12-08 16:26:36.442241: val_loss -0.7435 
2024-12-08 16:26:36.446808: Pseudo dice [np.float32(0.8697), np.float32(0.8528)] 
2024-12-08 16:26:36.449684: Epoch time: 9.0 s 
2024-12-08 16:26:37.313133:  
2024-12-08 16:26:37.318021: Epoch 52 
2024-12-08 16:26:37.320282: Current learning rate: 0.00517 
2024-12-08 16:26:46.327838: train_loss -0.9738 
2024-12-08 16:26:46.334464: val_loss -0.7457 
2024-12-08 16:26:46.338024: Pseudo dice [np.float32(0.8709), np.float32(0.8538)] 
2024-12-08 16:26:46.341389: Epoch time: 9.02 s 
2024-12-08 16:26:47.055961:  
2024-12-08 16:26:47.060652: Epoch 53 
2024-12-08 16:26:47.063630: Current learning rate: 0.00507 
2024-12-08 16:26:56.054535: train_loss -0.9741 
2024-12-08 16:26:56.060544: val_loss -0.7395 
2024-12-08 16:26:56.064469: Pseudo dice [np.float32(0.8691), np.float32(0.8511)] 
2024-12-08 16:26:56.067902: Epoch time: 9.0 s 
2024-12-08 16:26:56.764789:  
2024-12-08 16:26:56.769852: Epoch 54 
2024-12-08 16:26:56.773382: Current learning rate: 0.00497 
2024-12-08 16:27:05.821610: train_loss -0.9744 
2024-12-08 16:27:05.827169: val_loss -0.7417 
2024-12-08 16:27:05.830746: Pseudo dice [np.float32(0.8709), np.float32(0.8519)] 
2024-12-08 16:27:05.833794: Epoch time: 9.06 s 
2024-12-08 16:27:06.546742:  
2024-12-08 16:27:06.552742: Epoch 55 
2024-12-08 16:27:06.555797: Current learning rate: 0.00487 
2024-12-08 16:27:15.656183: train_loss -0.9748 
2024-12-08 16:27:15.661477: val_loss -0.7408 
2024-12-08 16:27:15.664205: Pseudo dice [np.float32(0.8705), np.float32(0.8526)] 
2024-12-08 16:27:15.666903: Epoch time: 9.11 s 
2024-12-08 16:27:16.355596:  
2024-12-08 16:27:16.361039: Epoch 56 
2024-12-08 16:27:16.364067: Current learning rate: 0.00478 
2024-12-08 16:27:25.345044: train_loss -0.9752 
2024-12-08 16:27:25.350516: val_loss -0.7409 
2024-12-08 16:27:25.353580: Pseudo dice [np.float32(0.8702), np.float32(0.8514)] 
2024-12-08 16:27:25.356738: Epoch time: 8.99 s 
2024-12-08 16:27:26.074761:  
2024-12-08 16:27:26.081205: Epoch 57 
2024-12-08 16:27:26.084095: Current learning rate: 0.00468 
2024-12-08 16:27:35.051558: train_loss -0.9754 
2024-12-08 16:27:35.058008: val_loss -0.7407 
2024-12-08 16:27:35.061549: Pseudo dice [np.float32(0.8711), np.float32(0.8525)] 
2024-12-08 16:27:35.065038: Epoch time: 8.98 s 
2024-12-08 16:27:35.774721:  
2024-12-08 16:27:35.780330: Epoch 58 
2024-12-08 16:27:35.783467: Current learning rate: 0.00458 
2024-12-08 16:27:44.950080: train_loss -0.9758 
2024-12-08 16:27:44.955537: val_loss -0.7388 
2024-12-08 16:27:44.958580: Pseudo dice [np.float32(0.8693), np.float32(0.8522)] 
2024-12-08 16:27:44.961812: Epoch time: 9.18 s 
2024-12-08 16:27:45.677884:  
2024-12-08 16:27:45.681935: Epoch 59 
2024-12-08 16:27:45.685525: Current learning rate: 0.00448 
2024-12-08 16:27:54.699776: train_loss -0.9758 
2024-12-08 16:27:54.706941: val_loss -0.738 
2024-12-08 16:27:54.709961: Pseudo dice [np.float32(0.8696), np.float32(0.8516)] 
2024-12-08 16:27:54.713316: Epoch time: 9.02 s 
2024-12-08 16:27:55.590378:  
2024-12-08 16:27:55.595747: Epoch 60 
2024-12-08 16:27:55.598831: Current learning rate: 0.00438 
2024-12-08 16:28:04.596483: train_loss -0.9761 
2024-12-08 16:28:04.602341: val_loss -0.7401 
2024-12-08 16:28:04.605407: Pseudo dice [np.float32(0.8705), np.float32(0.8534)] 
2024-12-08 16:28:04.608636: Epoch time: 9.01 s 
2024-12-08 16:28:05.301541:  
2024-12-08 16:28:05.306756: Epoch 61 
2024-12-08 16:28:05.309821: Current learning rate: 0.00429 
2024-12-08 16:28:14.316072: train_loss -0.9767 
2024-12-08 16:28:14.321734: val_loss -0.7392 
2024-12-08 16:28:14.324239: Pseudo dice [np.float32(0.87), np.float32(0.852)] 
2024-12-08 16:28:14.327332: Epoch time: 9.02 s 
2024-12-08 16:28:15.038067:  
2024-12-08 16:28:15.042979: Epoch 62 
2024-12-08 16:28:15.046053: Current learning rate: 0.00419 
2024-12-08 16:28:24.147734: train_loss -0.9768 
2024-12-08 16:28:24.154770: val_loss -0.7386 
2024-12-08 16:28:24.158284: Pseudo dice [np.float32(0.8692), np.float32(0.8526)] 
2024-12-08 16:28:24.160798: Epoch time: 9.11 s 
2024-12-08 16:28:24.853917:  
2024-12-08 16:28:24.859047: Epoch 63 
2024-12-08 16:28:24.861579: Current learning rate: 0.00409 
2024-12-08 16:28:33.923123: train_loss -0.9768 
2024-12-08 16:28:33.928576: val_loss -0.7414 
2024-12-08 16:28:33.931699: Pseudo dice [np.float32(0.8705), np.float32(0.8529)] 
2024-12-08 16:28:33.934743: Epoch time: 9.07 s 
2024-12-08 16:28:34.630440:  
2024-12-08 16:28:34.635540: Epoch 64 
2024-12-08 16:28:34.638135: Current learning rate: 0.00399 
2024-12-08 16:28:43.706577: train_loss -0.9773 
2024-12-08 16:28:43.711657: val_loss -0.7384 
2024-12-08 16:28:43.715220: Pseudo dice [np.float32(0.8688), np.float32(0.8521)] 
2024-12-08 16:28:43.718773: Epoch time: 9.08 s 
2024-12-08 16:28:44.427274:  
2024-12-08 16:28:44.433175: Epoch 65 
2024-12-08 16:28:44.435639: Current learning rate: 0.00389 
2024-12-08 16:28:53.456512: train_loss -0.9774 
2024-12-08 16:28:53.464164: val_loss -0.7395 
2024-12-08 16:28:53.467761: Pseudo dice [np.float32(0.8714), np.float32(0.8519)] 
2024-12-08 16:28:53.470622: Epoch time: 9.03 s 
2024-12-08 16:28:54.164304:  
2024-12-08 16:28:54.169011: Epoch 66 
2024-12-08 16:28:54.172535: Current learning rate: 0.00379 
2024-12-08 16:29:03.189449: train_loss -0.9777 
2024-12-08 16:29:03.194530: val_loss -0.7376 
2024-12-08 16:29:03.199126: Pseudo dice [np.float32(0.8682), np.float32(0.8525)] 
2024-12-08 16:29:03.202193: Epoch time: 9.03 s 
2024-12-08 16:29:03.917093:  
2024-12-08 16:29:03.922745: Epoch 67 
2024-12-08 16:29:03.925838: Current learning rate: 0.00369 
2024-12-08 16:29:12.910380: train_loss -0.9777 
2024-12-08 16:29:12.916018: val_loss -0.7394 
2024-12-08 16:29:12.919570: Pseudo dice [np.float32(0.8707), np.float32(0.8522)] 
2024-12-08 16:29:12.922132: Epoch time: 8.99 s 
2024-12-08 16:29:13.790877:  
2024-12-08 16:29:13.796511: Epoch 68 
2024-12-08 16:29:13.799072: Current learning rate: 0.00359 
2024-12-08 16:29:22.762349: train_loss -0.9781 
2024-12-08 16:29:22.767489: val_loss -0.7388 
2024-12-08 16:29:22.771657: Pseudo dice [np.float32(0.87), np.float32(0.8523)] 
2024-12-08 16:29:22.774720: Epoch time: 8.97 s 
2024-12-08 16:29:23.500803:  
2024-12-08 16:29:23.506437: Epoch 69 
2024-12-08 16:29:23.509004: Current learning rate: 0.00349 
2024-12-08 16:29:32.413894: train_loss -0.9784 
2024-12-08 16:29:32.418975: val_loss -0.7362 
2024-12-08 16:29:32.423540: Pseudo dice [np.float32(0.8694), np.float32(0.8513)] 
2024-12-08 16:29:32.426610: Epoch time: 8.91 s 
2024-12-08 16:29:33.124597:  
2024-12-08 16:29:33.129753: Epoch 70 
2024-12-08 16:29:33.132304: Current learning rate: 0.00338 
2024-12-08 16:29:42.139895: train_loss -0.9786 
2024-12-08 16:29:42.145034: val_loss -0.7374 
2024-12-08 16:29:42.147579: Pseudo dice [np.float32(0.87), np.float32(0.8509)] 
2024-12-08 16:29:42.150124: Epoch time: 9.02 s 
2024-12-08 16:29:42.867539:  
2024-12-08 16:29:42.872176: Epoch 71 
2024-12-08 16:29:42.875271: Current learning rate: 0.00328 
2024-12-08 16:29:51.796009: train_loss -0.9785 
2024-12-08 16:29:51.803150: val_loss -0.7372 
2024-12-08 16:29:51.807721: Pseudo dice [np.float32(0.87), np.float32(0.8505)] 
2024-12-08 16:29:51.809761: Epoch time: 8.93 s 
2024-12-08 16:29:52.520013:  
2024-12-08 16:29:52.525102: Epoch 72 
2024-12-08 16:29:52.528214: Current learning rate: 0.00318 
2024-12-08 16:30:01.449083: train_loss -0.9787 
2024-12-08 16:30:01.454168: val_loss -0.7391 
2024-12-08 16:30:01.457747: Pseudo dice [np.float32(0.8702), np.float32(0.8528)] 
2024-12-08 16:30:01.460342: Epoch time: 8.93 s 
2024-12-08 16:30:02.170038:  
2024-12-08 16:30:02.176242: Epoch 73 
2024-12-08 16:30:02.179816: Current learning rate: 0.00308 
2024-12-08 16:30:11.137118: train_loss -0.9792 
2024-12-08 16:30:11.142236: val_loss -0.735 
2024-12-08 16:30:11.144777: Pseudo dice [np.float32(0.8684), np.float32(0.8529)] 
2024-12-08 16:30:11.147321: Epoch time: 8.97 s 
2024-12-08 16:30:11.876070:  
2024-12-08 16:30:11.881154: Epoch 74 
2024-12-08 16:30:11.884227: Current learning rate: 0.00297 
2024-12-08 16:30:20.769135: train_loss -0.9793 
2024-12-08 16:30:20.774195: val_loss -0.7322 
2024-12-08 16:30:20.779276: Pseudo dice [np.float32(0.8677), np.float32(0.8514)] 
2024-12-08 16:30:20.781814: Epoch time: 8.89 s 
2024-12-08 16:30:21.519020:  
2024-12-08 16:30:21.525672: Epoch 75 
2024-12-08 16:30:21.529247: Current learning rate: 0.00287 
2024-12-08 16:30:30.512049: train_loss -0.9794 
2024-12-08 16:30:30.517670: val_loss -0.732 
2024-12-08 16:30:30.520222: Pseudo dice [np.float32(0.8679), np.float32(0.8504)] 
2024-12-08 16:30:30.522767: Epoch time: 8.99 s 
2024-12-08 16:30:31.392215:  
2024-12-08 16:30:31.399420: Epoch 76 
2024-12-08 16:30:31.402488: Current learning rate: 0.00277 
2024-12-08 16:30:40.342002: train_loss -0.9797 
2024-12-08 16:30:40.347645: val_loss -0.7364 
2024-12-08 16:30:40.350702: Pseudo dice [np.float32(0.8696), np.float32(0.8535)] 
2024-12-08 16:30:40.353248: Epoch time: 8.95 s 
2024-12-08 16:30:41.058903:  
2024-12-08 16:30:41.063985: Epoch 77 
2024-12-08 16:30:41.067080: Current learning rate: 0.00266 
2024-12-08 16:30:49.990219: train_loss -0.9801 
2024-12-08 16:30:49.997824: val_loss -0.7333 
2024-12-08 16:30:50.001386: Pseudo dice [np.float32(0.8689), np.float32(0.8502)] 
2024-12-08 16:30:50.005532: Epoch time: 8.93 s 
2024-12-08 16:30:50.730040:  
2024-12-08 16:30:50.736692: Epoch 78 
2024-12-08 16:30:50.740246: Current learning rate: 0.00256 
2024-12-08 16:30:59.717708: train_loss -0.9798 
2024-12-08 16:30:59.725343: val_loss -0.7363 
2024-12-08 16:30:59.727916: Pseudo dice [np.float32(0.8696), np.float32(0.8511)] 
2024-12-08 16:30:59.731980: Epoch time: 8.99 s 
2024-12-08 16:31:00.469631:  
2024-12-08 16:31:00.477327: Epoch 79 
2024-12-08 16:31:00.482464: Current learning rate: 0.00245 
2024-12-08 16:31:09.375842: train_loss -0.9801 
2024-12-08 16:31:09.380944: val_loss -0.7349 
2024-12-08 16:31:09.385093: Pseudo dice [np.float32(0.8696), np.float32(0.8516)] 
2024-12-08 16:31:09.388147: Epoch time: 8.91 s 
2024-12-08 16:31:10.093394:  
2024-12-08 16:31:10.099036: Epoch 80 
2024-12-08 16:31:10.102620: Current learning rate: 0.00235 
2024-12-08 16:31:19.023742: train_loss -0.98 
2024-12-08 16:31:19.029858: val_loss -0.7324 
2024-12-08 16:31:19.033481: Pseudo dice [np.float32(0.8683), np.float32(0.8511)] 
2024-12-08 16:31:19.037552: Epoch time: 8.93 s 
2024-12-08 16:31:19.781029:  
2024-12-08 16:31:19.787207: Epoch 81 
2024-12-08 16:31:19.791285: Current learning rate: 0.00224 
2024-12-08 16:31:28.778472: train_loss -0.9806 
2024-12-08 16:31:28.785624: val_loss -0.7365 
2024-12-08 16:31:28.788706: Pseudo dice [np.float32(0.8699), np.float32(0.8524)] 
2024-12-08 16:31:28.793286: Epoch time: 9.0 s 
2024-12-08 16:31:29.538879:  
2024-12-08 16:31:29.545047: Epoch 82 
2024-12-08 16:31:29.550167: Current learning rate: 0.00214 
2024-12-08 16:31:38.510654: train_loss -0.9806 
2024-12-08 16:31:38.518288: val_loss -0.7313 
2024-12-08 16:31:38.520833: Pseudo dice [np.float32(0.8676), np.float32(0.8506)] 
2024-12-08 16:31:38.525978: Epoch time: 8.97 s 
2024-12-08 16:31:39.362711:  
2024-12-08 16:31:39.367819: Epoch 83 
2024-12-08 16:31:39.370371: Current learning rate: 0.00203 
2024-12-08 16:31:48.329739: train_loss -0.9808 
2024-12-08 16:31:48.336374: val_loss -0.7361 
2024-12-08 16:31:48.340446: Pseudo dice [np.float32(0.8697), np.float32(0.8514)] 
2024-12-08 16:31:48.344507: Epoch time: 8.97 s 
2024-12-08 16:31:49.043567:  
2024-12-08 16:31:49.049230: Epoch 84 
2024-12-08 16:31:49.054337: Current learning rate: 0.00192 
2024-12-08 16:31:58.005437: train_loss -0.9807 
2024-12-08 16:31:58.012790: val_loss -0.7329 
2024-12-08 16:31:58.018210: Pseudo dice [np.float32(0.8686), np.float32(0.8512)] 
2024-12-08 16:31:58.021032: Epoch time: 8.96 s 
2024-12-08 16:31:58.712264:  
2024-12-08 16:31:58.717845: Epoch 85 
2024-12-08 16:31:58.721877: Current learning rate: 0.00181 
2024-12-08 16:32:07.748262: train_loss -0.9808 
2024-12-08 16:32:07.755862: val_loss -0.7352 
2024-12-08 16:32:07.758395: Pseudo dice [np.float32(0.8685), np.float32(0.8524)] 
2024-12-08 16:32:07.763501: Epoch time: 9.04 s 
2024-12-08 16:32:08.450966:  
2024-12-08 16:32:08.456360: Epoch 86 
2024-12-08 16:32:08.461371: Current learning rate: 0.0017 
2024-12-08 16:32:17.416436: train_loss -0.981 
2024-12-08 16:32:17.423007: val_loss -0.7331 
2024-12-08 16:32:17.428363: Pseudo dice [np.float32(0.8698), np.float32(0.8504)] 
2024-12-08 16:32:17.431884: Epoch time: 8.97 s 
2024-12-08 16:32:18.118673:  
2024-12-08 16:32:18.124810: Epoch 87 
2024-12-08 16:32:18.129895: Current learning rate: 0.00159 
2024-12-08 16:32:27.185721: train_loss -0.9811 
2024-12-08 16:32:27.193315: val_loss -0.7343 
2024-12-08 16:32:27.197374: Pseudo dice [np.float32(0.8688), np.float32(0.8522)] 
2024-12-08 16:32:27.200981: Epoch time: 9.07 s 
2024-12-08 16:32:27.874942:  
2024-12-08 16:32:27.881113: Epoch 88 
2024-12-08 16:32:27.883667: Current learning rate: 0.00148 
2024-12-08 16:32:36.830014: train_loss -0.9811 
2024-12-08 16:32:36.835636: val_loss -0.7323 
2024-12-08 16:32:36.839679: Pseudo dice [np.float32(0.8683), np.float32(0.8509)] 
2024-12-08 16:32:36.843762: Epoch time: 8.96 s 
2024-12-08 16:32:37.528197:  
2024-12-08 16:32:37.533557: Epoch 89 
2024-12-08 16:32:37.536635: Current learning rate: 0.00137 
2024-12-08 16:32:46.628709: train_loss -0.9813 
2024-12-08 16:32:46.633754: val_loss -0.7321 
2024-12-08 16:32:46.637431: Pseudo dice [np.float32(0.868), np.float32(0.8511)] 
2024-12-08 16:32:46.640499: Epoch time: 9.1 s 
2024-12-08 16:32:47.330353:  
2024-12-08 16:32:47.335760: Epoch 90 
2024-12-08 16:32:47.338815: Current learning rate: 0.00126 
2024-12-08 16:32:56.541321: train_loss -0.9816 
2024-12-08 16:32:56.547792: val_loss -0.7311 
2024-12-08 16:32:56.551409: Pseudo dice [np.float32(0.8678), np.float32(0.8516)] 
2024-12-08 16:32:56.554770: Epoch time: 9.21 s 
2024-12-08 16:32:57.247128:  
2024-12-08 16:32:57.252174: Epoch 91 
2024-12-08 16:32:57.255220: Current learning rate: 0.00115 
2024-12-08 16:33:06.188275: train_loss -0.9815 
2024-12-08 16:33:06.194367: val_loss -0.7337 
2024-12-08 16:33:06.197429: Pseudo dice [np.float32(0.8704), np.float32(0.8508)] 
2024-12-08 16:33:06.199971: Epoch time: 8.94 s 
2024-12-08 16:33:07.038742:  
2024-12-08 16:33:07.045880: Epoch 92 
2024-12-08 16:33:07.048949: Current learning rate: 0.00103 
2024-12-08 16:33:15.972119: train_loss -0.9818 
2024-12-08 16:33:15.977721: val_loss -0.732 
2024-12-08 16:33:15.980781: Pseudo dice [np.float32(0.8684), np.float32(0.8512)] 
2024-12-08 16:33:15.983326: Epoch time: 8.93 s 
2024-12-08 16:33:16.678641:  
2024-12-08 16:33:16.684288: Epoch 93 
2024-12-08 16:33:16.687351: Current learning rate: 0.00091 
2024-12-08 16:33:25.654204: train_loss -0.9819 
2024-12-08 16:33:25.659637: val_loss -0.7327 
2024-12-08 16:33:25.663285: Pseudo dice [np.float32(0.8693), np.float32(0.8517)] 
2024-12-08 16:33:25.666595: Epoch time: 8.98 s 
2024-12-08 16:33:26.355341:  
2024-12-08 16:33:26.360647: Epoch 94 
2024-12-08 16:33:26.363961: Current learning rate: 0.00079 
2024-12-08 16:33:35.327509: train_loss -0.982 
2024-12-08 16:33:35.333107: val_loss -0.7338 
2024-12-08 16:33:35.336170: Pseudo dice [np.float32(0.8695), np.float32(0.8517)] 
2024-12-08 16:33:35.338755: Epoch time: 8.97 s 
2024-12-08 16:33:36.034256:  
2024-12-08 16:33:36.040875: Epoch 95 
2024-12-08 16:33:36.043967: Current learning rate: 0.00067 
2024-12-08 16:33:44.946888: train_loss -0.9818 
2024-12-08 16:33:44.952996: val_loss -0.7328 
2024-12-08 16:33:44.956541: Pseudo dice [np.float32(0.87), np.float32(0.8502)] 
2024-12-08 16:33:44.959090: Epoch time: 8.91 s 
2024-12-08 16:33:45.616412:  
2024-12-08 16:33:45.623068: Epoch 96 
2024-12-08 16:33:45.626135: Current learning rate: 0.00055 
2024-12-08 16:33:54.586471: train_loss -0.982 
2024-12-08 16:33:54.594113: val_loss -0.7306 
2024-12-08 16:33:54.598716: Pseudo dice [np.float32(0.8681), np.float32(0.8495)] 
2024-12-08 16:33:54.601780: Epoch time: 8.97 s 
2024-12-08 16:33:55.314265:  
2024-12-08 16:33:55.317876: Epoch 97 
2024-12-08 16:33:55.321182: Current learning rate: 0.00043 
2024-12-08 16:34:04.296473: train_loss -0.9819 
2024-12-08 16:34:04.303034: val_loss -0.7308 
2024-12-08 16:34:04.306638: Pseudo dice [np.float32(0.8693), np.float32(0.8497)] 
2024-12-08 16:34:04.309027: Epoch time: 8.98 s 
2024-12-08 16:34:05.008335:  
2024-12-08 16:34:05.014345: Epoch 98 
2024-12-08 16:34:05.017612: Current learning rate: 0.0003 
2024-12-08 16:34:14.007891: train_loss -0.982 
2024-12-08 16:34:14.013365: val_loss -0.7318 
2024-12-08 16:34:14.016415: Pseudo dice [np.float32(0.8683), np.float32(0.8506)] 
2024-12-08 16:34:14.019643: Epoch time: 9.0 s 
2024-12-08 16:34:14.723393:  
2024-12-08 16:34:14.728011: Epoch 99 
2024-12-08 16:34:14.731082: Current learning rate: 0.00016 
2024-12-08 16:34:23.742943: train_loss -0.9821 
2024-12-08 16:34:23.748016: val_loss -0.7295 
2024-12-08 16:34:23.750592: Pseudo dice [np.float32(0.8678), np.float32(0.8505)] 
2024-12-08 16:34:23.755432: Epoch time: 9.02 s 
2024-12-08 16:34:24.633398: Training done. 
2024-12-08 16:34:24.669200: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2024-12-08 16:34:24.677263: The split file contains 5 splits. 
2024-12-08 16:34:24.685280: Desired fold for training: 0 
2024-12-08 16:34:24.685280: This split has 208 training and 52 validation cases. 
2024-12-08 16:34:24.693492: predicting hippocampus_017 
2024-12-08 16:34:24.693492: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2024-12-08 16:34:24.846354: predicting hippocampus_019 
2024-12-08 16:34:24.852086: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2024-12-08 16:34:24.948629: predicting hippocampus_033 
2024-12-08 16:34:24.954483: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2024-12-08 16:34:25.034897: predicting hippocampus_035 
2024-12-08 16:34:25.040905: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2024-12-08 16:34:25.131026: predicting hippocampus_037 
2024-12-08 16:34:25.137033: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2024-12-08 16:34:25.207384: predicting hippocampus_049 
2024-12-08 16:34:25.215418: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2024-12-08 16:34:25.296932: predicting hippocampus_052 
2024-12-08 16:34:25.296932: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2024-12-08 16:34:25.380904: predicting hippocampus_065 
2024-12-08 16:34:25.386904: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2024-12-08 16:34:25.464098: predicting hippocampus_083 
2024-12-08 16:34:25.470113: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2024-12-08 16:34:25.542780: predicting hippocampus_088 
2024-12-08 16:34:25.550971: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2024-12-08 16:34:28.542896: predicting hippocampus_090 
2024-12-08 16:34:28.557282: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2024-12-08 16:34:28.643103: predicting hippocampus_092 
2024-12-08 16:34:28.650999: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2024-12-08 16:34:28.742610: predicting hippocampus_095 
2024-12-08 16:34:28.762889: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2024-12-08 16:34:28.859491: predicting hippocampus_107 
2024-12-08 16:34:28.867926: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2024-12-08 16:34:28.980053: predicting hippocampus_108 
2024-12-08 16:34:28.986517: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2024-12-08 16:34:29.091555: predicting hippocampus_123 
2024-12-08 16:34:29.099521: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2024-12-08 16:34:29.200110: predicting hippocampus_125 
2024-12-08 16:34:29.207976: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2024-12-08 16:34:29.368945: predicting hippocampus_157 
2024-12-08 16:34:29.372949: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2024-12-08 16:34:29.444725: predicting hippocampus_164 
2024-12-08 16:34:29.451020: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2024-12-08 16:34:29.622646: predicting hippocampus_169 
2024-12-08 16:34:29.628656: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2024-12-08 16:34:29.708981: predicting hippocampus_175 
2024-12-08 16:34:29.716990: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2024-12-08 16:34:29.794917: predicting hippocampus_185 
2024-12-08 16:34:29.798914: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2024-12-08 16:34:29.876925: predicting hippocampus_190 
2024-12-08 16:34:29.886014: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2024-12-08 16:34:29.954840: predicting hippocampus_194 
2024-12-08 16:34:29.965800: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2024-12-08 16:34:30.025331: predicting hippocampus_204 
2024-12-08 16:34:30.034763: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2024-12-08 16:34:30.121219: predicting hippocampus_205 
2024-12-08 16:34:30.128536: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2024-12-08 16:34:30.198608: predicting hippocampus_210 
2024-12-08 16:34:30.208620: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2024-12-08 16:34:30.289530: predicting hippocampus_217 
2024-12-08 16:34:30.295530: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2024-12-08 16:34:30.357409: predicting hippocampus_219 
2024-12-08 16:34:30.362878: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2024-12-08 16:34:30.439859: predicting hippocampus_229 
2024-12-08 16:34:30.452637: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2024-12-08 16:34:30.530533: predicting hippocampus_244 
2024-12-08 16:34:30.536718: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2024-12-08 16:34:30.598791: predicting hippocampus_261 
2024-12-08 16:34:30.602796: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2024-12-08 16:34:30.728340: predicting hippocampus_264 
2024-12-08 16:34:30.738554: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2024-12-08 16:34:30.818271: predicting hippocampus_277 
2024-12-08 16:34:30.824095: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2024-12-08 16:34:30.948991: predicting hippocampus_280 
2024-12-08 16:34:30.955461: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2024-12-08 16:34:31.016823: predicting hippocampus_286 
2024-12-08 16:34:31.024163: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2024-12-08 16:34:31.111762: predicting hippocampus_288 
2024-12-08 16:34:31.117772: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2024-12-08 16:34:31.202573: predicting hippocampus_289 
2024-12-08 16:34:31.216011: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2024-12-08 16:34:31.291691: predicting hippocampus_296 
2024-12-08 16:34:31.300186: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2024-12-08 16:34:31.382750: predicting hippocampus_305 
2024-12-08 16:34:31.389232: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2024-12-08 16:34:31.452299: predicting hippocampus_308 
2024-12-08 16:34:31.457694: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2024-12-08 16:34:31.539601: predicting hippocampus_317 
2024-12-08 16:34:31.545409: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2024-12-08 16:34:31.619808: predicting hippocampus_327 
2024-12-08 16:34:31.626823: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2024-12-08 16:34:31.695096: predicting hippocampus_330 
2024-12-08 16:34:31.702481: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2024-12-08 16:34:31.780642: predicting hippocampus_332 
2024-12-08 16:34:31.793661: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2024-12-08 16:34:31.870956: predicting hippocampus_338 
2024-12-08 16:34:31.880509: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2024-12-08 16:34:31.984927: predicting hippocampus_349 
2024-12-08 16:34:31.990546: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2024-12-08 16:34:32.070487: predicting hippocampus_350 
2024-12-08 16:34:32.074768: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2024-12-08 16:34:32.157798: predicting hippocampus_356 
2024-12-08 16:34:32.163205: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2024-12-08 16:34:32.250031: predicting hippocampus_358 
2024-12-08 16:34:32.255484: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2024-12-08 16:34:32.325510: predicting hippocampus_374 
2024-12-08 16:34:32.330003: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2024-12-08 16:34:32.422120: predicting hippocampus_394 
2024-12-08 16:34:32.427982: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2024-12-08 16:34:36.136170: Validation complete 
2024-12-08 16:34:36.144197: Mean Validation Dice:  0.8543816843526029 
