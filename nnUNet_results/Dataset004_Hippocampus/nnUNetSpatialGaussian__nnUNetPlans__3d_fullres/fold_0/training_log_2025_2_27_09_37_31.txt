
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-02-27 09:37:31.532993: do_dummy_2d_data_aug: False 
2025-02-27 09:37:31.550002: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-02-27 09:37:31.557003: The split file contains 5 splits. 
2025-02-27 09:37:31.560003: Desired fold for training: 0 
2025-02-27 09:37:31.562705: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-02-27 09:37:37.881283: unpacking dataset... 
2025-02-27 09:37:38.714052: unpacking done... 
2025-02-27 09:37:40.902358:  
2025-02-27 09:37:40.906379: Epoch 0 
2025-02-27 09:37:40.909883: Current learning rate: 0.01 
2025-02-27 09:37:48.485355: train_loss -0.5331 
2025-02-27 09:37:48.490960: val_loss -0.8124 
2025-02-27 09:37:48.494674: Pseudo dice [np.float32(0.8649), np.float32(0.8572)] 
2025-02-27 09:37:48.498711: Epoch time: 7.58 s 
2025-02-27 09:37:48.502284: Yayy! New best EMA pseudo Dice: 0.8610000014305115 
2025-02-27 09:37:49.019760:  
2025-02-27 09:37:49.025331: Epoch 1 
2025-02-27 09:37:49.028883: Current learning rate: 0.00991 
2025-02-27 09:37:55.741903: train_loss -0.8225 
2025-02-27 09:37:55.748086: val_loss -0.8358 
2025-02-27 09:37:55.751137: Pseudo dice [np.float32(0.8864), np.float32(0.8702)] 
2025-02-27 09:37:55.754187: Epoch time: 6.72 s 
2025-02-27 09:37:55.757739: Yayy! New best EMA pseudo Dice: 0.8628000020980835 
2025-02-27 09:37:56.340292:  
2025-02-27 09:37:56.345307: Epoch 2 
2025-02-27 09:37:56.348818: Current learning rate: 0.00982 
2025-02-27 09:38:03.053746: train_loss -0.8419 
2025-02-27 09:38:03.058862: val_loss -0.8385 
2025-02-27 09:38:03.063452: Pseudo dice [np.float32(0.8898), np.float32(0.8709)] 
2025-02-27 09:38:03.067158: Epoch time: 6.71 s 
2025-02-27 09:38:03.070231: Yayy! New best EMA pseudo Dice: 0.8644999861717224 
2025-02-27 09:38:03.747456:  
2025-02-27 09:38:03.752971: Epoch 3 
2025-02-27 09:38:03.756482: Current learning rate: 0.00973 
2025-02-27 09:38:10.445246: train_loss -0.8533 
2025-02-27 09:38:10.451813: val_loss -0.849 
2025-02-27 09:38:10.455351: Pseudo dice [np.float32(0.8964), np.float32(0.8798)] 
2025-02-27 09:38:10.458890: Epoch time: 6.7 s 
2025-02-27 09:38:10.462930: Yayy! New best EMA pseudo Dice: 0.8669000267982483 
2025-02-27 09:38:11.092951:  
2025-02-27 09:38:11.096964: Epoch 4 
2025-02-27 09:38:11.100472: Current learning rate: 0.00964 
2025-02-27 09:38:17.787117: train_loss -0.8604 
2025-02-27 09:38:17.793775: val_loss -0.8481 
2025-02-27 09:38:17.796816: Pseudo dice [np.float32(0.8949), np.float32(0.8788)] 
2025-02-27 09:38:17.800381: Epoch time: 6.7 s 
2025-02-27 09:38:17.803447: Yayy! New best EMA pseudo Dice: 0.8689000010490417 
2025-02-27 09:38:18.539769:  
2025-02-27 09:38:18.545343: Epoch 5 
2025-02-27 09:38:18.547898: Current learning rate: 0.00955 
2025-02-27 09:38:25.232317: train_loss -0.8673 
2025-02-27 09:38:25.238427: val_loss -0.8487 
2025-02-27 09:38:25.240954: Pseudo dice [np.float32(0.8973), np.float32(0.8798)] 
2025-02-27 09:38:25.244991: Epoch time: 6.69 s 
2025-02-27 09:38:25.248548: Yayy! New best EMA pseudo Dice: 0.8708000183105469 
2025-02-27 09:38:25.830009:  
2025-02-27 09:38:25.836111: Epoch 6 
2025-02-27 09:38:25.839201: Current learning rate: 0.00946 
2025-02-27 09:38:32.527204: train_loss -0.8724 
2025-02-27 09:38:32.532750: val_loss -0.8474 
2025-02-27 09:38:32.536488: Pseudo dice [np.float32(0.8961), np.float32(0.8795)] 
2025-02-27 09:38:32.540545: Epoch time: 6.7 s 
2025-02-27 09:38:32.544105: Yayy! New best EMA pseudo Dice: 0.8725000023841858 
2025-02-27 09:38:33.132796:  
2025-02-27 09:38:33.138325: Epoch 7 
2025-02-27 09:38:33.141835: Current learning rate: 0.00937 
2025-02-27 09:38:39.834270: train_loss -0.8793 
2025-02-27 09:38:39.839813: val_loss -0.8509 
2025-02-27 09:38:39.845049: Pseudo dice [np.float32(0.8996), np.float32(0.8805)] 
2025-02-27 09:38:39.848586: Epoch time: 6.7 s 
2025-02-27 09:38:39.852610: Yayy! New best EMA pseudo Dice: 0.8743000030517578 
2025-02-27 09:38:40.471083:  
2025-02-27 09:38:40.477640: Epoch 8 
2025-02-27 09:38:40.481153: Current learning rate: 0.00928 
2025-02-27 09:38:47.162428: train_loss -0.8814 
2025-02-27 09:38:47.168518: val_loss -0.8513 
2025-02-27 09:38:47.172576: Pseudo dice [np.float32(0.9001), np.float32(0.8806)] 
2025-02-27 09:38:47.176128: Epoch time: 6.69 s 
2025-02-27 09:38:47.178652: Yayy! New best EMA pseudo Dice: 0.8758999705314636 
2025-02-27 09:38:47.793179:  
2025-02-27 09:38:47.799217: Epoch 9 
2025-02-27 09:38:47.802732: Current learning rate: 0.00919 
2025-02-27 09:38:54.480251: train_loss -0.8873 
2025-02-27 09:38:54.486415: val_loss -0.8491 
2025-02-27 09:38:54.489444: Pseudo dice [np.float32(0.9), np.float32(0.8798)] 
2025-02-27 09:38:54.492996: Epoch time: 6.69 s 
2025-02-27 09:38:54.495508: Yayy! New best EMA pseudo Dice: 0.8773000240325928 
2025-02-27 09:38:55.072609:  
2025-02-27 09:38:55.078207: Epoch 10 
2025-02-27 09:38:55.081789: Current learning rate: 0.0091 
2025-02-27 09:39:01.762723: train_loss -0.8898 
2025-02-27 09:39:01.769436: val_loss -0.8446 
2025-02-27 09:39:01.771946: Pseudo dice [np.float32(0.8962), np.float32(0.8763)] 
2025-02-27 09:39:01.774996: Epoch time: 6.69 s 
2025-02-27 09:39:01.779026: Yayy! New best EMA pseudo Dice: 0.8781999945640564 
2025-02-27 09:39:02.378247:  
2025-02-27 09:39:02.384841: Epoch 11 
2025-02-27 09:39:02.387406: Current learning rate: 0.009 
2025-02-27 09:39:09.049269: train_loss -0.8947 
2025-02-27 09:39:09.055832: val_loss -0.847 
2025-02-27 09:39:09.059484: Pseudo dice [np.float32(0.8984), np.float32(0.8789)] 
2025-02-27 09:39:09.062537: Epoch time: 6.67 s 
2025-02-27 09:39:09.065590: Yayy! New best EMA pseudo Dice: 0.8791999816894531 
2025-02-27 09:39:09.653468:  
2025-02-27 09:39:09.658628: Epoch 12 
2025-02-27 09:39:09.661667: Current learning rate: 0.00891 
2025-02-27 09:39:16.346665: train_loss -0.8963 
2025-02-27 09:39:16.352747: val_loss -0.8448 
2025-02-27 09:39:16.356304: Pseudo dice [np.float32(0.8977), np.float32(0.8767)] 
2025-02-27 09:39:16.358849: Epoch time: 6.69 s 
2025-02-27 09:39:16.362370: Yayy! New best EMA pseudo Dice: 0.8799999952316284 
2025-02-27 09:39:17.102417:  
2025-02-27 09:39:17.108007: Epoch 13 
2025-02-27 09:39:17.110557: Current learning rate: 0.00882 
2025-02-27 09:39:23.779903: train_loss -0.8979 
2025-02-27 09:39:23.786436: val_loss -0.8459 
2025-02-27 09:39:23.790032: Pseudo dice [np.float32(0.8972), np.float32(0.8782)] 
2025-02-27 09:39:23.793577: Epoch time: 6.68 s 
2025-02-27 09:39:23.795615: Yayy! New best EMA pseudo Dice: 0.8808000087738037 
2025-02-27 09:39:24.393501:  
2025-02-27 09:39:24.398512: Epoch 14 
2025-02-27 09:39:24.402024: Current learning rate: 0.00873 
2025-02-27 09:39:31.068355: train_loss -0.9004 
2025-02-27 09:39:31.074922: val_loss -0.8462 
2025-02-27 09:39:31.077964: Pseudo dice [np.float32(0.8978), np.float32(0.8795)] 
2025-02-27 09:39:31.081586: Epoch time: 6.68 s 
2025-02-27 09:39:31.085620: Yayy! New best EMA pseudo Dice: 0.881600022315979 
2025-02-27 09:39:31.707856:  
2025-02-27 09:39:31.712396: Epoch 15 
2025-02-27 09:39:31.715418: Current learning rate: 0.00864 
2025-02-27 09:39:38.386590: train_loss -0.9027 
2025-02-27 09:39:38.392222: val_loss -0.8424 
2025-02-27 09:39:38.395761: Pseudo dice [np.float32(0.8969), np.float32(0.8767)] 
2025-02-27 09:39:38.399329: Epoch time: 6.68 s 
2025-02-27 09:39:38.401876: Yayy! New best EMA pseudo Dice: 0.882099986076355 
2025-02-27 09:39:39.010911:  
2025-02-27 09:39:39.017034: Epoch 16 
2025-02-27 09:39:39.020086: Current learning rate: 0.00855 
2025-02-27 09:39:45.707706: train_loss -0.9035 
2025-02-27 09:39:45.713234: val_loss -0.8425 
2025-02-27 09:39:45.716744: Pseudo dice [np.float32(0.8956), np.float32(0.8778)] 
2025-02-27 09:39:45.720820: Epoch time: 6.7 s 
2025-02-27 09:39:45.724424: Yayy! New best EMA pseudo Dice: 0.8826000094413757 
2025-02-27 09:39:46.338978:  
2025-02-27 09:39:46.344530: Epoch 17 
2025-02-27 09:39:46.348077: Current learning rate: 0.00846 
2025-02-27 09:39:53.032629: train_loss -0.9065 
2025-02-27 09:39:53.038729: val_loss -0.8429 
2025-02-27 09:39:53.042802: Pseudo dice [np.float32(0.8976), np.float32(0.8766)] 
2025-02-27 09:39:53.045959: Epoch time: 6.69 s 
2025-02-27 09:39:53.049030: Yayy! New best EMA pseudo Dice: 0.8830000162124634 
2025-02-27 09:39:53.675380:  
2025-02-27 09:39:53.681008: Epoch 18 
2025-02-27 09:39:53.685050: Current learning rate: 0.00836 
2025-02-27 09:40:00.358742: train_loss -0.9077 
2025-02-27 09:40:00.364817: val_loss -0.8423 
2025-02-27 09:40:00.368344: Pseudo dice [np.float32(0.8981), np.float32(0.8769)] 
2025-02-27 09:40:00.370633: Epoch time: 6.68 s 
2025-02-27 09:40:00.374937: Yayy! New best EMA pseudo Dice: 0.8834999799728394 
2025-02-27 09:40:00.986226:  
2025-02-27 09:40:00.991763: Epoch 19 
2025-02-27 09:40:00.996400: Current learning rate: 0.00827 
2025-02-27 09:40:07.698601: train_loss -0.9089 
2025-02-27 09:40:07.704747: val_loss -0.8443 
2025-02-27 09:40:07.707842: Pseudo dice [np.float32(0.8976), np.float32(0.8786)] 
2025-02-27 09:40:07.711411: Epoch time: 6.71 s 
2025-02-27 09:40:07.714973: Yayy! New best EMA pseudo Dice: 0.883899986743927 
2025-02-27 09:40:08.481512:  
2025-02-27 09:40:08.487656: Epoch 20 
2025-02-27 09:40:08.491198: Current learning rate: 0.00818 
2025-02-27 09:40:15.159349: train_loss -0.9099 
2025-02-27 09:40:15.165442: val_loss -0.8392 
2025-02-27 09:40:15.169476: Pseudo dice [np.float32(0.8952), np.float32(0.8743)] 
2025-02-27 09:40:15.173041: Epoch time: 6.68 s 
2025-02-27 09:40:15.175057: Yayy! New best EMA pseudo Dice: 0.8840000033378601 
2025-02-27 09:40:15.790997:  
2025-02-27 09:40:15.797086: Epoch 21 
2025-02-27 09:40:15.800152: Current learning rate: 0.00809 
2025-02-27 09:40:22.474004: train_loss -0.9116 
2025-02-27 09:40:22.480606: val_loss -0.8402 
2025-02-27 09:40:22.484176: Pseudo dice [np.float32(0.8967), np.float32(0.8747)] 
2025-02-27 09:40:22.487741: Epoch time: 6.68 s 
2025-02-27 09:40:22.490773: Yayy! New best EMA pseudo Dice: 0.8841999769210815 
2025-02-27 09:40:23.076473:  
2025-02-27 09:40:23.083047: Epoch 22 
2025-02-27 09:40:23.086604: Current learning rate: 0.008 
2025-02-27 09:40:29.767772: train_loss -0.9118 
2025-02-27 09:40:29.773873: val_loss -0.841 
2025-02-27 09:40:29.777413: Pseudo dice [np.float32(0.8956), np.float32(0.8751)] 
2025-02-27 09:40:29.780968: Epoch time: 6.69 s 
2025-02-27 09:40:29.784009: Yayy! New best EMA pseudo Dice: 0.8842999935150146 
2025-02-27 09:40:30.368437:  
2025-02-27 09:40:30.374668: Epoch 23 
2025-02-27 09:40:30.378680: Current learning rate: 0.0079 
2025-02-27 09:40:37.068921: train_loss -0.9138 
2025-02-27 09:40:37.074005: val_loss -0.8424 
2025-02-27 09:40:37.078966: Pseudo dice [np.float32(0.8977), np.float32(0.8775)] 
2025-02-27 09:40:37.082002: Epoch time: 6.7 s 
2025-02-27 09:40:37.086031: Yayy! New best EMA pseudo Dice: 0.8845999836921692 
2025-02-27 09:40:37.660051:  
2025-02-27 09:40:37.666094: Epoch 24 
2025-02-27 09:40:37.669763: Current learning rate: 0.00781 
2025-02-27 09:40:44.342619: train_loss -0.9152 
2025-02-27 09:40:44.350168: val_loss -0.8398 
2025-02-27 09:40:44.354196: Pseudo dice [np.float32(0.8963), np.float32(0.8762)] 
2025-02-27 09:40:44.358076: Epoch time: 6.68 s 
2025-02-27 09:40:44.361593: Yayy! New best EMA pseudo Dice: 0.8848000168800354 
2025-02-27 09:40:44.950165:  
2025-02-27 09:40:44.956183: Epoch 25 
2025-02-27 09:40:44.959688: Current learning rate: 0.00772 
2025-02-27 09:40:51.635398: train_loss -0.9158 
2025-02-27 09:40:51.641501: val_loss -0.8418 
2025-02-27 09:40:51.645015: Pseudo dice [np.float32(0.8971), np.float32(0.8787)] 
2025-02-27 09:40:51.649029: Epoch time: 6.69 s 
2025-02-27 09:40:51.652111: Yayy! New best EMA pseudo Dice: 0.8851000070571899 
2025-02-27 09:40:52.238755:  
2025-02-27 09:40:52.243768: Epoch 26 
2025-02-27 09:40:52.247781: Current learning rate: 0.00763 
2025-02-27 09:40:58.921129: train_loss -0.9157 
2025-02-27 09:40:58.927806: val_loss -0.8414 
2025-02-27 09:40:58.931589: Pseudo dice [np.float32(0.8965), np.float32(0.8786)] 
2025-02-27 09:40:58.935104: Epoch time: 6.68 s 
2025-02-27 09:40:58.938256: Yayy! New best EMA pseudo Dice: 0.8853999972343445 
2025-02-27 09:40:59.526761:  
2025-02-27 09:40:59.532843: Epoch 27 
2025-02-27 09:40:59.535962: Current learning rate: 0.00753 
2025-02-27 09:41:06.247638: train_loss -0.9181 
2025-02-27 09:41:06.254193: val_loss -0.8441 
2025-02-27 09:41:06.257734: Pseudo dice [np.float32(0.8995), np.float32(0.8794)] 
2025-02-27 09:41:06.261306: Epoch time: 6.72 s 
2025-02-27 09:41:06.264828: Yayy! New best EMA pseudo Dice: 0.8858000040054321 
2025-02-27 09:41:07.004697:  
2025-02-27 09:41:07.010214: Epoch 28 
2025-02-27 09:41:07.013723: Current learning rate: 0.00744 
2025-02-27 09:41:13.688134: train_loss -0.9171 
2025-02-27 09:41:13.694854: val_loss -0.8419 
2025-02-27 09:41:13.698203: Pseudo dice [np.float32(0.8983), np.float32(0.8795)] 
2025-02-27 09:41:13.702322: Epoch time: 6.68 s 
2025-02-27 09:41:13.705869: Yayy! New best EMA pseudo Dice: 0.8860999941825867 
2025-02-27 09:41:14.291483:  
2025-02-27 09:41:14.297062: Epoch 29 
2025-02-27 09:41:14.301125: Current learning rate: 0.00735 
2025-02-27 09:41:20.970542: train_loss -0.919 
2025-02-27 09:41:20.976630: val_loss -0.8363 
2025-02-27 09:41:20.980198: Pseudo dice [np.float32(0.8948), np.float32(0.8753)] 
2025-02-27 09:41:20.984229: Epoch time: 6.68 s 
2025-02-27 09:41:21.551647:  
2025-02-27 09:41:21.557166: Epoch 30 
2025-02-27 09:41:21.560678: Current learning rate: 0.00725 
2025-02-27 09:41:28.254780: train_loss -0.9203 
2025-02-27 09:41:28.261412: val_loss -0.8329 
2025-02-27 09:41:28.264436: Pseudo dice [np.float32(0.8937), np.float32(0.8726)] 
2025-02-27 09:41:28.267995: Epoch time: 6.7 s 
2025-02-27 09:41:28.828293:  
2025-02-27 09:41:28.833810: Epoch 31 
2025-02-27 09:41:28.837325: Current learning rate: 0.00716 
2025-02-27 09:41:35.514208: train_loss -0.9209 
2025-02-27 09:41:35.521729: val_loss -0.8471 
2025-02-27 09:41:35.525242: Pseudo dice [np.float32(0.9004), np.float32(0.8826)] 
2025-02-27 09:41:35.527401: Epoch time: 6.69 s 
2025-02-27 09:41:35.531946: Yayy! New best EMA pseudo Dice: 0.8863000273704529 
2025-02-27 09:41:36.130561:  
2025-02-27 09:41:36.136642: Epoch 32 
2025-02-27 09:41:36.140235: Current learning rate: 0.00707 
2025-02-27 09:41:42.823697: train_loss -0.9215 
2025-02-27 09:41:42.830731: val_loss -0.8438 
2025-02-27 09:41:42.833752: Pseudo dice [np.float32(0.9002), np.float32(0.8804)] 
2025-02-27 09:41:42.837384: Epoch time: 6.69 s 
2025-02-27 09:41:42.840714: Yayy! New best EMA pseudo Dice: 0.8866999745368958 
2025-02-27 09:41:43.439530:  
2025-02-27 09:41:43.445571: Epoch 33 
2025-02-27 09:41:43.448279: Current learning rate: 0.00697 
2025-02-27 09:41:50.131001: train_loss -0.9223 
2025-02-27 09:41:50.137579: val_loss -0.8389 
2025-02-27 09:41:50.140591: Pseudo dice [np.float32(0.8964), np.float32(0.8781)] 
2025-02-27 09:41:50.144104: Epoch time: 6.69 s 
2025-02-27 09:41:50.148115: Yayy! New best EMA pseudo Dice: 0.8866999745368958 
2025-02-27 09:41:50.745341:  
2025-02-27 09:41:50.750857: Epoch 34 
2025-02-27 09:41:50.754369: Current learning rate: 0.00688 
2025-02-27 09:41:57.437786: train_loss -0.9212 
2025-02-27 09:41:57.444381: val_loss -0.8416 
2025-02-27 09:41:57.448411: Pseudo dice [np.float32(0.8984), np.float32(0.8785)] 
2025-02-27 09:41:57.451934: Epoch time: 6.69 s 
2025-02-27 09:41:57.454882: Yayy! New best EMA pseudo Dice: 0.886900007724762 
2025-02-27 09:41:58.062004:  
2025-02-27 09:41:58.067581: Epoch 35 
2025-02-27 09:41:58.072212: Current learning rate: 0.00679 
2025-02-27 09:42:04.748895: train_loss -0.9232 
2025-02-27 09:42:04.755993: val_loss -0.8397 
2025-02-27 09:42:04.759070: Pseudo dice [np.float32(0.8977), np.float32(0.8785)] 
2025-02-27 09:42:04.763655: Epoch time: 6.69 s 
2025-02-27 09:42:04.767208: Yayy! New best EMA pseudo Dice: 0.8870000243186951 
2025-02-27 09:42:05.555748:  
2025-02-27 09:42:05.562356: Epoch 36 
2025-02-27 09:42:05.564911: Current learning rate: 0.00669 
2025-02-27 09:42:12.226627: train_loss -0.9243 
2025-02-27 09:42:12.232706: val_loss -0.8382 
2025-02-27 09:42:12.236263: Pseudo dice [np.float32(0.8969), np.float32(0.8781)] 
2025-02-27 09:42:12.239420: Epoch time: 6.67 s 
2025-02-27 09:42:12.241947: Yayy! New best EMA pseudo Dice: 0.8870999813079834 
2025-02-27 09:42:12.850206:  
2025-02-27 09:42:12.855741: Epoch 37 
2025-02-27 09:42:12.859300: Current learning rate: 0.0066 
2025-02-27 09:42:19.529983: train_loss -0.9244 
2025-02-27 09:42:19.535587: val_loss -0.8392 
2025-02-27 09:42:19.538632: Pseudo dice [np.float32(0.899), np.float32(0.877)] 
2025-02-27 09:42:19.542186: Epoch time: 6.68 s 
2025-02-27 09:42:19.545811: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-02-27 09:42:20.153173:  
2025-02-27 09:42:20.158712: Epoch 38 
2025-02-27 09:42:20.162270: Current learning rate: 0.0065 
2025-02-27 09:42:26.847852: train_loss -0.9272 
2025-02-27 09:42:26.853423: val_loss -0.8339 
2025-02-27 09:42:26.856957: Pseudo dice [np.float32(0.8958), np.float32(0.876)] 
2025-02-27 09:42:26.859995: Epoch time: 6.7 s 
2025-02-27 09:42:27.433486:  
2025-02-27 09:42:27.439057: Epoch 39 
2025-02-27 09:42:27.441602: Current learning rate: 0.00641 
2025-02-27 09:42:34.124113: train_loss -0.9267 
2025-02-27 09:42:34.129721: val_loss -0.8356 
2025-02-27 09:42:34.132780: Pseudo dice [np.float32(0.8958), np.float32(0.8761)] 
2025-02-27 09:42:34.135880: Epoch time: 6.69 s 
2025-02-27 09:42:34.715336:  
2025-02-27 09:42:34.720903: Epoch 40 
2025-02-27 09:42:34.724453: Current learning rate: 0.00631 
2025-02-27 09:42:41.399801: train_loss -0.9285 
2025-02-27 09:42:41.406321: val_loss -0.8371 
2025-02-27 09:42:41.409837: Pseudo dice [np.float32(0.8964), np.float32(0.8796)] 
2025-02-27 09:42:41.412346: Epoch time: 6.68 s 
2025-02-27 09:42:41.992754:  
2025-02-27 09:42:41.996792: Epoch 41 
2025-02-27 09:42:41.999322: Current learning rate: 0.00622 
2025-02-27 09:42:48.685839: train_loss -0.9272 
2025-02-27 09:42:48.691895: val_loss -0.8366 
2025-02-27 09:42:48.694280: Pseudo dice [np.float32(0.8967), np.float32(0.8775)] 
2025-02-27 09:42:48.698933: Epoch time: 6.69 s 
2025-02-27 09:42:49.244421:  
2025-02-27 09:42:49.250507: Epoch 42 
2025-02-27 09:42:49.253636: Current learning rate: 0.00612 
2025-02-27 09:42:55.920290: train_loss -0.9288 
2025-02-27 09:42:55.925947: val_loss -0.8354 
2025-02-27 09:42:55.929538: Pseudo dice [np.float32(0.8963), np.float32(0.8765)] 
2025-02-27 09:42:55.932100: Epoch time: 6.68 s 
2025-02-27 09:42:56.632277:  
2025-02-27 09:42:56.637870: Epoch 43 
2025-02-27 09:42:56.640972: Current learning rate: 0.00603 
2025-02-27 09:43:03.324780: train_loss -0.93 
2025-02-27 09:43:03.330388: val_loss -0.8306 
2025-02-27 09:43:03.333433: Pseudo dice [np.float32(0.8942), np.float32(0.8748)] 
2025-02-27 09:43:03.336486: Epoch time: 6.69 s 
2025-02-27 09:43:03.950398:  
2025-02-27 09:43:03.955414: Epoch 44 
2025-02-27 09:43:03.958927: Current learning rate: 0.00593 
2025-02-27 09:43:10.634065: train_loss -0.9295 
2025-02-27 09:43:10.638644: val_loss -0.8355 
2025-02-27 09:43:10.643307: Pseudo dice [np.float32(0.8969), np.float32(0.8773)] 
2025-02-27 09:43:10.646348: Epoch time: 6.68 s 
2025-02-27 09:43:11.193132:  
2025-02-27 09:43:11.198680: Epoch 45 
2025-02-27 09:43:11.201710: Current learning rate: 0.00584 
2025-02-27 09:43:17.870552: train_loss -0.9312 
2025-02-27 09:43:17.876263: val_loss -0.8389 
2025-02-27 09:43:17.879831: Pseudo dice [np.float32(0.8973), np.float32(0.8802)] 
2025-02-27 09:43:17.882431: Epoch time: 6.68 s 
2025-02-27 09:43:18.428195:  
2025-02-27 09:43:18.433739: Epoch 46 
2025-02-27 09:43:18.436749: Current learning rate: 0.00574 
2025-02-27 09:43:25.111785: train_loss -0.9308 
2025-02-27 09:43:25.117880: val_loss -0.8333 
2025-02-27 09:43:25.120924: Pseudo dice [np.float32(0.8951), np.float32(0.8774)] 
2025-02-27 09:43:25.124026: Epoch time: 6.68 s 
2025-02-27 09:43:25.667262:  
2025-02-27 09:43:25.672396: Epoch 47 
2025-02-27 09:43:25.676471: Current learning rate: 0.00565 
2025-02-27 09:43:32.354646: train_loss -0.9313 
2025-02-27 09:43:32.360296: val_loss -0.8359 
2025-02-27 09:43:32.363495: Pseudo dice [np.float32(0.8961), np.float32(0.8783)] 
2025-02-27 09:43:32.366344: Epoch time: 6.69 s 
2025-02-27 09:43:32.918192:  
2025-02-27 09:43:32.923862: Epoch 48 
2025-02-27 09:43:32.927431: Current learning rate: 0.00555 
2025-02-27 09:43:39.605740: train_loss -0.9329 
2025-02-27 09:43:39.612057: val_loss -0.8348 
2025-02-27 09:43:39.615571: Pseudo dice [np.float32(0.8969), np.float32(0.8777)] 
2025-02-27 09:43:39.618728: Epoch time: 6.69 s 
2025-02-27 09:43:40.173455:  
2025-02-27 09:43:40.178975: Epoch 49 
2025-02-27 09:43:40.181481: Current learning rate: 0.00546 
2025-02-27 09:43:46.862643: train_loss -0.9331 
2025-02-27 09:43:46.869315: val_loss -0.8381 
2025-02-27 09:43:46.872347: Pseudo dice [np.float32(0.8983), np.float32(0.88)] 
2025-02-27 09:43:46.875369: Epoch time: 6.69 s 
2025-02-27 09:43:46.917694: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-02-27 09:43:47.504861:  
2025-02-27 09:43:47.510460: Epoch 50 
2025-02-27 09:43:47.514099: Current learning rate: 0.00536 
2025-02-27 09:43:54.188967: train_loss -0.9316 
2025-02-27 09:43:54.193561: val_loss -0.8287 
2025-02-27 09:43:54.198202: Pseudo dice [np.float32(0.8934), np.float32(0.8748)] 
2025-02-27 09:43:54.201764: Epoch time: 6.68 s 
2025-02-27 09:43:54.912045:  
2025-02-27 09:43:54.917066: Epoch 51 
2025-02-27 09:43:54.920579: Current learning rate: 0.00526 
2025-02-27 09:44:01.582484: train_loss -0.9318 
2025-02-27 09:44:01.589069: val_loss -0.8352 
2025-02-27 09:44:01.592618: Pseudo dice [np.float32(0.8969), np.float32(0.8796)] 
2025-02-27 09:44:01.595688: Epoch time: 6.67 s 
2025-02-27 09:44:02.146199:  
2025-02-27 09:44:02.151804: Epoch 52 
2025-02-27 09:44:02.154354: Current learning rate: 0.00517 
2025-02-27 09:44:08.854343: train_loss -0.9342 
2025-02-27 09:44:08.860752: val_loss -0.8349 
2025-02-27 09:44:08.864277: Pseudo dice [np.float32(0.8978), np.float32(0.8801)] 
2025-02-27 09:44:08.867604: Epoch time: 6.71 s 
2025-02-27 09:44:08.870713: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-02-27 09:44:09.466031:  
2025-02-27 09:44:09.471697: Epoch 53 
2025-02-27 09:44:09.474263: Current learning rate: 0.00507 
2025-02-27 09:44:16.144284: train_loss -0.933 
2025-02-27 09:44:16.149967: val_loss -0.8311 
2025-02-27 09:44:16.154023: Pseudo dice [np.float32(0.895), np.float32(0.8766)] 
2025-02-27 09:44:16.157102: Epoch time: 6.68 s 
2025-02-27 09:44:16.713732:  
2025-02-27 09:44:16.719289: Epoch 54 
2025-02-27 09:44:16.722337: Current learning rate: 0.00497 
2025-02-27 09:44:23.389437: train_loss -0.9347 
2025-02-27 09:44:23.395463: val_loss -0.8318 
2025-02-27 09:44:23.398568: Pseudo dice [np.float32(0.8961), np.float32(0.8767)] 
2025-02-27 09:44:23.401649: Epoch time: 6.68 s 
2025-02-27 09:44:23.955033:  
2025-02-27 09:44:23.960628: Epoch 55 
2025-02-27 09:44:23.964209: Current learning rate: 0.00487 
2025-02-27 09:44:30.634475: train_loss -0.9348 
2025-02-27 09:44:30.641119: val_loss -0.8327 
2025-02-27 09:44:30.644130: Pseudo dice [np.float32(0.8978), np.float32(0.8772)] 
2025-02-27 09:44:30.647643: Epoch time: 6.68 s 
2025-02-27 09:44:31.203907:  
2025-02-27 09:44:31.209425: Epoch 56 
2025-02-27 09:44:31.212937: Current learning rate: 0.00478 
2025-02-27 09:44:37.902257: train_loss -0.9345 
2025-02-27 09:44:37.909778: val_loss -0.8309 
2025-02-27 09:44:37.913833: Pseudo dice [np.float32(0.895), np.float32(0.8783)] 
2025-02-27 09:44:37.916389: Epoch time: 6.7 s 
2025-02-27 09:44:38.510630:  
2025-02-27 09:44:38.516148: Epoch 57 
2025-02-27 09:44:38.518655: Current learning rate: 0.00468 
2025-02-27 09:44:45.210905: train_loss -0.9342 
2025-02-27 09:44:45.216026: val_loss -0.8291 
2025-02-27 09:44:45.220121: Pseudo dice [np.float32(0.8955), np.float32(0.8754)] 
2025-02-27 09:44:45.223763: Epoch time: 6.7 s 
2025-02-27 09:44:45.779231:  
2025-02-27 09:44:45.784752: Epoch 58 
2025-02-27 09:44:45.787262: Current learning rate: 0.00458 
2025-02-27 09:44:52.464868: train_loss -0.935 
2025-02-27 09:44:52.471078: val_loss -0.8316 
2025-02-27 09:44:52.474144: Pseudo dice [np.float32(0.8963), np.float32(0.8771)] 
2025-02-27 09:44:52.477705: Epoch time: 6.69 s 
2025-02-27 09:44:53.194740:  
2025-02-27 09:44:53.200311: Epoch 59 
2025-02-27 09:44:53.202918: Current learning rate: 0.00448 
2025-02-27 09:44:59.872344: train_loss -0.9366 
2025-02-27 09:44:59.877925: val_loss -0.8279 
2025-02-27 09:44:59.881443: Pseudo dice [np.float32(0.8942), np.float32(0.8771)] 
2025-02-27 09:44:59.884531: Epoch time: 6.68 s 
2025-02-27 09:45:00.454925:  
2025-02-27 09:45:00.460989: Epoch 60 
2025-02-27 09:45:00.463867: Current learning rate: 0.00438 
2025-02-27 09:45:07.153409: train_loss -0.9368 
2025-02-27 09:45:07.159549: val_loss -0.8327 
2025-02-27 09:45:07.161573: Pseudo dice [np.float32(0.8965), np.float32(0.8789)] 
2025-02-27 09:45:07.165638: Epoch time: 6.7 s 
2025-02-27 09:45:07.732125:  
2025-02-27 09:45:07.738249: Epoch 61 
2025-02-27 09:45:07.740811: Current learning rate: 0.00429 
2025-02-27 09:45:14.415135: train_loss -0.9371 
2025-02-27 09:45:14.420250: val_loss -0.8309 
2025-02-27 09:45:14.423320: Pseudo dice [np.float32(0.8955), np.float32(0.8789)] 
2025-02-27 09:45:14.425350: Epoch time: 6.68 s 
2025-02-27 09:45:14.992787:  
2025-02-27 09:45:14.998310: Epoch 62 
2025-02-27 09:45:15.000816: Current learning rate: 0.00419 
2025-02-27 09:45:21.660404: train_loss -0.9381 
2025-02-27 09:45:21.666081: val_loss -0.8248 
2025-02-27 09:45:21.669245: Pseudo dice [np.float32(0.8933), np.float32(0.8741)] 
2025-02-27 09:45:21.672841: Epoch time: 6.67 s 
2025-02-27 09:45:22.239281:  
2025-02-27 09:45:22.244302: Epoch 63 
2025-02-27 09:45:22.247814: Current learning rate: 0.00409 
2025-02-27 09:45:28.925664: train_loss -0.9384 
2025-02-27 09:45:28.930779: val_loss -0.8283 
2025-02-27 09:45:28.934817: Pseudo dice [np.float32(0.8942), np.float32(0.877)] 
2025-02-27 09:45:28.937861: Epoch time: 6.69 s 
2025-02-27 09:45:29.528812:  
2025-02-27 09:45:29.532844: Epoch 64 
2025-02-27 09:45:29.535379: Current learning rate: 0.00399 
2025-02-27 09:45:36.219618: train_loss -0.9389 
2025-02-27 09:45:36.224846: val_loss -0.8291 
2025-02-27 09:45:36.228857: Pseudo dice [np.float32(0.895), np.float32(0.8767)] 
2025-02-27 09:45:36.232374: Epoch time: 6.69 s 
2025-02-27 09:45:36.793729:  
2025-02-27 09:45:36.798819: Epoch 65 
2025-02-27 09:45:36.802614: Current learning rate: 0.00389 
2025-02-27 09:45:43.477396: train_loss -0.9384 
2025-02-27 09:45:43.483517: val_loss -0.8291 
2025-02-27 09:45:43.486608: Pseudo dice [np.float32(0.8957), np.float32(0.8766)] 
2025-02-27 09:45:43.490227: Epoch time: 6.68 s 
2025-02-27 09:45:44.069448:  
2025-02-27 09:45:44.075033: Epoch 66 
2025-02-27 09:45:44.078544: Current learning rate: 0.00379 
2025-02-27 09:45:50.756091: train_loss -0.9388 
2025-02-27 09:45:50.762620: val_loss -0.8278 
2025-02-27 09:45:50.766130: Pseudo dice [np.float32(0.8945), np.float32(0.8765)] 
2025-02-27 09:45:50.768164: Epoch time: 6.69 s 
2025-02-27 09:45:51.518390:  
2025-02-27 09:45:51.523991: Epoch 67 
2025-02-27 09:45:51.527047: Current learning rate: 0.00369 
2025-02-27 09:45:58.202680: train_loss -0.9385 
2025-02-27 09:45:58.209321: val_loss -0.8307 
2025-02-27 09:45:58.212913: Pseudo dice [np.float32(0.8962), np.float32(0.8782)] 
2025-02-27 09:45:58.216021: Epoch time: 6.68 s 
2025-02-27 09:45:58.790004:  
2025-02-27 09:45:58.795527: Epoch 68 
2025-02-27 09:45:58.798038: Current learning rate: 0.00359 
2025-02-27 09:46:05.467433: train_loss -0.939 
2025-02-27 09:46:05.472491: val_loss -0.8223 
2025-02-27 09:46:05.476293: Pseudo dice [np.float32(0.892), np.float32(0.8733)] 
2025-02-27 09:46:05.478805: Epoch time: 6.68 s 
2025-02-27 09:46:06.057864:  
2025-02-27 09:46:06.062878: Epoch 69 
2025-02-27 09:46:06.066390: Current learning rate: 0.00349 
2025-02-27 09:46:12.746923: train_loss -0.9398 
2025-02-27 09:46:12.753069: val_loss -0.8294 
2025-02-27 09:46:12.756209: Pseudo dice [np.float32(0.8964), np.float32(0.8772)] 
2025-02-27 09:46:12.758746: Epoch time: 6.69 s 
2025-02-27 09:46:13.335203:  
2025-02-27 09:46:13.340806: Epoch 70 
2025-02-27 09:46:13.343411: Current learning rate: 0.00338 
2025-02-27 09:46:20.018969: train_loss -0.9407 
2025-02-27 09:46:20.024555: val_loss -0.8303 
2025-02-27 09:46:20.028119: Pseudo dice [np.float32(0.8959), np.float32(0.8786)] 
2025-02-27 09:46:20.031174: Epoch time: 6.68 s 
2025-02-27 09:46:20.618225:  
2025-02-27 09:46:20.623764: Epoch 71 
2025-02-27 09:46:20.627333: Current learning rate: 0.00328 
2025-02-27 09:46:27.311455: train_loss -0.9393 
2025-02-27 09:46:27.316865: val_loss -0.8254 
2025-02-27 09:46:27.320377: Pseudo dice [np.float32(0.894), np.float32(0.8758)] 
2025-02-27 09:46:27.323474: Epoch time: 6.69 s 
2025-02-27 09:46:27.898195:  
2025-02-27 09:46:27.903253: Epoch 72 
2025-02-27 09:46:27.906872: Current learning rate: 0.00318 
2025-02-27 09:46:34.586137: train_loss -0.9406 
2025-02-27 09:46:34.591753: val_loss -0.827 
2025-02-27 09:46:34.595301: Pseudo dice [np.float32(0.8949), np.float32(0.8759)] 
2025-02-27 09:46:34.598818: Epoch time: 6.69 s 
2025-02-27 09:46:35.185013:  
2025-02-27 09:46:35.189068: Epoch 73 
2025-02-27 09:46:35.191685: Current learning rate: 0.00308 
2025-02-27 09:46:41.865474: train_loss -0.942 
2025-02-27 09:46:41.870532: val_loss -0.8275 
2025-02-27 09:46:41.874200: Pseudo dice [np.float32(0.8944), np.float32(0.8768)] 
2025-02-27 09:46:41.877727: Epoch time: 6.68 s 
2025-02-27 09:46:42.637391:  
2025-02-27 09:46:42.642938: Epoch 74 
2025-02-27 09:46:42.645976: Current learning rate: 0.00297 
2025-02-27 09:46:49.307673: train_loss -0.9403 
2025-02-27 09:46:49.313760: val_loss -0.8291 
2025-02-27 09:46:49.317294: Pseudo dice [np.float32(0.8959), np.float32(0.8779)] 
2025-02-27 09:46:49.320370: Epoch time: 6.67 s 
2025-02-27 09:46:49.900443:  
2025-02-27 09:46:49.906017: Epoch 75 
2025-02-27 09:46:49.908568: Current learning rate: 0.00287 
2025-02-27 09:46:56.572489: train_loss -0.9408 
2025-02-27 09:46:56.577504: val_loss -0.8261 
2025-02-27 09:46:56.581576: Pseudo dice [np.float32(0.895), np.float32(0.8763)] 
2025-02-27 09:46:56.585196: Epoch time: 6.67 s 
2025-02-27 09:46:57.157573:  
2025-02-27 09:46:57.163150: Epoch 76 
2025-02-27 09:46:57.165695: Current learning rate: 0.00277 
2025-02-27 09:47:03.850292: train_loss -0.9414 
2025-02-27 09:47:03.855942: val_loss -0.8265 
2025-02-27 09:47:03.859052: Pseudo dice [np.float32(0.8944), np.float32(0.8761)] 
2025-02-27 09:47:03.862668: Epoch time: 6.69 s 
2025-02-27 09:47:04.453744:  
2025-02-27 09:47:04.458764: Epoch 77 
2025-02-27 09:47:04.462282: Current learning rate: 0.00266 
2025-02-27 09:47:11.139641: train_loss -0.9425 
2025-02-27 09:47:11.145887: val_loss -0.821 
2025-02-27 09:47:11.149927: Pseudo dice [np.float32(0.8918), np.float32(0.8718)] 
2025-02-27 09:47:11.153012: Epoch time: 6.69 s 
2025-02-27 09:47:11.743650:  
2025-02-27 09:47:11.749298: Epoch 78 
2025-02-27 09:47:11.752877: Current learning rate: 0.00256 
2025-02-27 09:47:18.423213: train_loss -0.9419 
2025-02-27 09:47:18.429235: val_loss -0.8303 
2025-02-27 09:47:18.432250: Pseudo dice [np.float32(0.8963), np.float32(0.8791)] 
2025-02-27 09:47:18.435761: Epoch time: 6.68 s 
2025-02-27 09:47:19.102681:  
2025-02-27 09:47:19.108848: Epoch 79 
2025-02-27 09:47:19.111361: Current learning rate: 0.00245 
2025-02-27 09:47:25.807371: train_loss -0.9425 
2025-02-27 09:47:25.813996: val_loss -0.8308 
2025-02-27 09:47:25.817085: Pseudo dice [np.float32(0.8971), np.float32(0.8793)] 
2025-02-27 09:47:25.820767: Epoch time: 6.7 s 
2025-02-27 09:47:26.400167:  
2025-02-27 09:47:26.405763: Epoch 80 
2025-02-27 09:47:26.408374: Current learning rate: 0.00235 
2025-02-27 09:47:33.083123: train_loss -0.9421 
2025-02-27 09:47:33.088799: val_loss -0.8261 
2025-02-27 09:47:33.092674: Pseudo dice [np.float32(0.8945), np.float32(0.8776)] 
2025-02-27 09:47:33.095691: Epoch time: 6.68 s 
2025-02-27 09:47:33.686791:  
2025-02-27 09:47:33.690457: Epoch 81 
2025-02-27 09:47:33.693496: Current learning rate: 0.00224 
2025-02-27 09:47:40.362736: train_loss -0.9403 
2025-02-27 09:47:40.368273: val_loss -0.8317 
2025-02-27 09:47:40.371830: Pseudo dice [np.float32(0.8979), np.float32(0.879)] 
2025-02-27 09:47:40.374882: Epoch time: 6.68 s 
2025-02-27 09:47:41.118740:  
2025-02-27 09:47:41.124307: Epoch 82 
2025-02-27 09:47:41.127903: Current learning rate: 0.00214 
2025-02-27 09:47:47.809451: train_loss -0.9424 
2025-02-27 09:47:47.815479: val_loss -0.8235 
2025-02-27 09:47:47.818108: Pseudo dice [np.float32(0.8928), np.float32(0.8746)] 
2025-02-27 09:47:47.822233: Epoch time: 6.69 s 
2025-02-27 09:47:48.369191:  
2025-02-27 09:47:48.374243: Epoch 83 
2025-02-27 09:47:48.377757: Current learning rate: 0.00203 
2025-02-27 09:47:55.049624: train_loss -0.943 
2025-02-27 09:47:55.054681: val_loss -0.8261 
2025-02-27 09:47:55.059252: Pseudo dice [np.float32(0.8962), np.float32(0.8767)] 
2025-02-27 09:47:55.061802: Epoch time: 6.68 s 
2025-02-27 09:47:55.606648:  
2025-02-27 09:47:55.612221: Epoch 84 
2025-02-27 09:47:55.614771: Current learning rate: 0.00192 
2025-02-27 09:48:02.290573: train_loss -0.9428 
2025-02-27 09:48:02.296784: val_loss -0.8265 
2025-02-27 09:48:02.299927: Pseudo dice [np.float32(0.8953), np.float32(0.8768)] 
2025-02-27 09:48:02.303939: Epoch time: 6.68 s 
2025-02-27 09:48:02.852384:  
2025-02-27 09:48:02.858424: Epoch 85 
2025-02-27 09:48:02.861348: Current learning rate: 0.00181 
2025-02-27 09:48:09.558132: train_loss -0.9449 
2025-02-27 09:48:09.563467: val_loss -0.8235 
2025-02-27 09:48:09.566982: Pseudo dice [np.float32(0.8931), np.float32(0.8751)] 
2025-02-27 09:48:09.570696: Epoch time: 6.71 s 
2025-02-27 09:48:10.109396:  
2025-02-27 09:48:10.113434: Epoch 86 
2025-02-27 09:48:10.115984: Current learning rate: 0.0017 
2025-02-27 09:48:16.806830: train_loss -0.945 
2025-02-27 09:48:16.812890: val_loss -0.8268 
2025-02-27 09:48:16.816485: Pseudo dice [np.float32(0.8948), np.float32(0.8763)] 
2025-02-27 09:48:16.819283: Epoch time: 6.7 s 
2025-02-27 09:48:17.358828:  
2025-02-27 09:48:17.363841: Epoch 87 
2025-02-27 09:48:17.367353: Current learning rate: 0.00159 
2025-02-27 09:48:24.033187: train_loss -0.9452 
2025-02-27 09:48:24.039806: val_loss -0.8183 
2025-02-27 09:48:24.042863: Pseudo dice [np.float32(0.8916), np.float32(0.8715)] 
2025-02-27 09:48:24.045914: Epoch time: 6.67 s 
2025-02-27 09:48:24.597616:  
2025-02-27 09:48:24.603133: Epoch 88 
2025-02-27 09:48:24.605639: Current learning rate: 0.00148 
2025-02-27 09:48:31.277203: train_loss -0.945 
2025-02-27 09:48:31.283752: val_loss -0.8305 
2025-02-27 09:48:31.286319: Pseudo dice [np.float32(0.8964), np.float32(0.8798)] 
2025-02-27 09:48:31.289949: Epoch time: 6.68 s 
2025-02-27 09:48:31.827062:  
2025-02-27 09:48:31.832690: Epoch 89 
2025-02-27 09:48:31.835721: Current learning rate: 0.00137 
2025-02-27 09:48:38.527346: train_loss -0.9444 
2025-02-27 09:48:38.534949: val_loss -0.8234 
2025-02-27 09:48:38.537997: Pseudo dice [np.float32(0.8943), np.float32(0.8758)] 
2025-02-27 09:48:38.541038: Epoch time: 6.7 s 
2025-02-27 09:48:39.234789:  
2025-02-27 09:48:39.240402: Epoch 90 
2025-02-27 09:48:39.244447: Current learning rate: 0.00126 
2025-02-27 09:48:45.920362: train_loss -0.9473 
2025-02-27 09:48:45.925424: val_loss -0.8302 
2025-02-27 09:48:45.928795: Pseudo dice [np.float32(0.8969), np.float32(0.8794)] 
2025-02-27 09:48:45.931815: Epoch time: 6.69 s 
2025-02-27 09:48:46.473445:  
2025-02-27 09:48:46.478961: Epoch 91 
2025-02-27 09:48:46.482473: Current learning rate: 0.00115 
2025-02-27 09:48:53.157749: train_loss -0.9448 
2025-02-27 09:48:53.163333: val_loss -0.8287 
2025-02-27 09:48:53.166868: Pseudo dice [np.float32(0.8968), np.float32(0.8781)] 
2025-02-27 09:48:53.169943: Epoch time: 6.69 s 
2025-02-27 09:48:53.727621:  
2025-02-27 09:48:53.732664: Epoch 92 
2025-02-27 09:48:53.735434: Current learning rate: 0.00103 
2025-02-27 09:49:00.423127: train_loss -0.9463 
2025-02-27 09:49:00.428737: val_loss -0.8295 
2025-02-27 09:49:00.432265: Pseudo dice [np.float32(0.8968), np.float32(0.8785)] 
2025-02-27 09:49:00.434967: Epoch time: 6.7 s 
2025-02-27 09:49:00.976694:  
2025-02-27 09:49:00.982826: Epoch 93 
2025-02-27 09:49:00.985889: Current learning rate: 0.00091 
2025-02-27 09:49:07.664819: train_loss -0.9459 
2025-02-27 09:49:07.669975: val_loss -0.8295 
2025-02-27 09:49:07.673559: Pseudo dice [np.float32(0.8977), np.float32(0.8793)] 
2025-02-27 09:49:07.676639: Epoch time: 6.69 s 
2025-02-27 09:49:08.215187:  
2025-02-27 09:49:08.221267: Epoch 94 
2025-02-27 09:49:08.224349: Current learning rate: 0.00079 
2025-02-27 09:49:14.911216: train_loss -0.9461 
2025-02-27 09:49:14.918346: val_loss -0.8244 
2025-02-27 09:49:14.922505: Pseudo dice [np.float32(0.8941), np.float32(0.8765)] 
2025-02-27 09:49:14.925523: Epoch time: 6.7 s 
2025-02-27 09:49:15.484526:  
2025-02-27 09:49:15.490613: Epoch 95 
2025-02-27 09:49:15.493685: Current learning rate: 0.00067 
2025-02-27 09:49:22.167247: train_loss -0.9457 
2025-02-27 09:49:22.172780: val_loss -0.8251 
2025-02-27 09:49:22.177320: Pseudo dice [np.float32(0.8938), np.float32(0.8768)] 
2025-02-27 09:49:22.179871: Epoch time: 6.68 s 
2025-02-27 09:49:22.716291:  
2025-02-27 09:49:22.721825: Epoch 96 
2025-02-27 09:49:22.725575: Current learning rate: 0.00055 
2025-02-27 09:49:29.407260: train_loss -0.9458 
2025-02-27 09:49:29.413283: val_loss -0.8215 
2025-02-27 09:49:29.416363: Pseudo dice [np.float32(0.8926), np.float32(0.8743)] 
2025-02-27 09:49:29.419407: Epoch time: 6.69 s 
2025-02-27 09:49:29.977517:  
2025-02-27 09:49:29.983616: Epoch 97 
2025-02-27 09:49:29.986265: Current learning rate: 0.00043 
2025-02-27 09:49:36.667551: train_loss -0.9481 
2025-02-27 09:49:36.674199: val_loss -0.8252 
2025-02-27 09:49:36.677230: Pseudo dice [np.float32(0.8955), np.float32(0.8757)] 
2025-02-27 09:49:36.680748: Epoch time: 6.69 s 
2025-02-27 09:49:37.232687:  
2025-02-27 09:49:37.238264: Epoch 98 
2025-02-27 09:49:37.242318: Current learning rate: 0.0003 
2025-02-27 09:49:43.924750: train_loss -0.9471 
2025-02-27 09:49:43.930799: val_loss -0.822 
2025-02-27 09:49:43.934322: Pseudo dice [np.float32(0.8931), np.float32(0.8736)] 
2025-02-27 09:49:43.937565: Epoch time: 6.69 s 
2025-02-27 09:49:44.657349:  
2025-02-27 09:49:44.662885: Epoch 99 
2025-02-27 09:49:44.666451: Current learning rate: 0.00016 
2025-02-27 09:49:51.344094: train_loss -0.9469 
2025-02-27 09:49:51.349746: val_loss -0.8316 
2025-02-27 09:49:51.353280: Pseudo dice [np.float32(0.8972), np.float32(0.8811)] 
2025-02-27 09:49:51.356838: Epoch time: 6.69 s 
2025-02-27 09:49:51.957677: Training done. 
2025-02-27 09:49:51.993186: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-02-27 09:49:52.001186: The split file contains 5 splits. 
2025-02-27 09:49:52.007188: Desired fold for training: 0 
2025-02-27 09:49:52.013187: This split has 208 training and 52 validation cases. 
2025-02-27 09:49:52.019187: predicting hippocampus_017 
2025-02-27 09:49:52.025187: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-02-27 09:49:52.117696: predicting hippocampus_019 
2025-02-27 09:49:52.124696: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-02-27 09:49:52.160696: predicting hippocampus_033 
2025-02-27 09:49:52.166696: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-02-27 09:49:52.189019: predicting hippocampus_035 
2025-02-27 09:49:52.195019: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-02-27 09:49:52.218019: predicting hippocampus_037 
2025-02-27 09:49:52.224017: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-02-27 09:49:52.247017: predicting hippocampus_049 
2025-02-27 09:49:52.253018: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-02-27 09:49:52.277020: predicting hippocampus_052 
2025-02-27 09:49:52.284020: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-02-27 09:49:52.307525: predicting hippocampus_065 
2025-02-27 09:49:52.314525: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-02-27 09:49:52.339525: predicting hippocampus_083 
2025-02-27 09:49:52.346525: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-02-27 09:49:52.371528: predicting hippocampus_088 
2025-02-27 09:49:52.377528: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-02-27 09:49:55.821410: predicting hippocampus_090 
2025-02-27 09:49:55.827415: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-02-27 09:49:55.863415: predicting hippocampus_092 
2025-02-27 09:49:55.877420: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-02-27 09:49:55.923934: predicting hippocampus_095 
2025-02-27 09:49:55.932931: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-02-27 09:49:55.977931: predicting hippocampus_107 
2025-02-27 09:49:55.985932: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-02-27 09:49:56.026439: predicting hippocampus_108 
2025-02-27 09:49:56.034437: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-02-27 09:49:56.065436: predicting hippocampus_123 
2025-02-27 09:49:56.072438: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-02-27 09:49:56.110950: predicting hippocampus_125 
2025-02-27 09:49:56.120951: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-02-27 09:49:56.190461: predicting hippocampus_157 
2025-02-27 09:49:56.226461: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-02-27 09:49:56.264460: predicting hippocampus_164 
2025-02-27 09:49:56.271459: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-02-27 09:49:56.360968: predicting hippocampus_169 
2025-02-27 09:49:56.371970: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-02-27 09:49:56.402480: predicting hippocampus_175 
2025-02-27 09:49:56.409479: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-02-27 09:49:56.439480: predicting hippocampus_185 
2025-02-27 09:49:56.446481: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-02-27 09:49:56.474481: predicting hippocampus_190 
2025-02-27 09:49:56.481483: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-02-27 09:49:56.510987: predicting hippocampus_194 
2025-02-27 09:49:56.516989: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-02-27 09:49:56.546988: predicting hippocampus_204 
2025-02-27 09:49:56.553989: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-02-27 09:49:56.581990: predicting hippocampus_205 
2025-02-27 09:49:56.590498: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-02-27 09:49:56.618496: predicting hippocampus_210 
2025-02-27 09:49:56.625498: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-02-27 09:49:56.653497: predicting hippocampus_217 
2025-02-27 09:49:56.660496: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-02-27 09:49:56.691006: predicting hippocampus_219 
2025-02-27 09:49:56.698008: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-02-27 09:49:56.727007: predicting hippocampus_229 
2025-02-27 09:49:56.734008: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-02-27 09:49:56.762007: predicting hippocampus_244 
2025-02-27 09:49:56.770007: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-02-27 09:49:56.805519: predicting hippocampus_261 
2025-02-27 09:49:56.812521: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-02-27 09:49:56.862519: predicting hippocampus_264 
2025-02-27 09:49:56.869521: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-02-27 09:49:56.901030: predicting hippocampus_277 
2025-02-27 09:49:56.907029: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-02-27 09:49:56.956029: predicting hippocampus_280 
2025-02-27 09:49:56.963031: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-02-27 09:49:56.992538: predicting hippocampus_286 
2025-02-27 09:49:57.000539: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-02-27 09:49:57.047538: predicting hippocampus_288 
2025-02-27 09:49:57.055538: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-02-27 09:49:57.104045: predicting hippocampus_289 
2025-02-27 09:49:57.111045: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-02-27 09:49:57.141047: predicting hippocampus_296 
2025-02-27 09:49:57.147045: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-02-27 09:49:57.177049: predicting hippocampus_305 
2025-02-27 09:49:57.184049: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-02-27 09:49:57.211556: predicting hippocampus_308 
2025-02-27 09:49:57.219556: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-02-27 09:49:57.250555: predicting hippocampus_317 
2025-02-27 09:49:57.258558: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-02-27 09:49:57.286558: predicting hippocampus_327 
2025-02-27 09:49:57.293065: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-02-27 09:49:57.323063: predicting hippocampus_330 
2025-02-27 09:49:57.329064: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-02-27 09:49:57.358063: predicting hippocampus_332 
2025-02-27 09:49:57.366065: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-02-27 09:49:57.394570: predicting hippocampus_338 
2025-02-27 09:49:57.401572: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-02-27 09:49:57.447570: predicting hippocampus_349 
2025-02-27 09:49:57.454572: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-02-27 09:49:57.483572: predicting hippocampus_350 
2025-02-27 09:49:57.491078: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-02-27 09:49:57.521078: predicting hippocampus_356 
2025-02-27 09:49:57.527080: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-02-27 09:49:57.557079: predicting hippocampus_358 
2025-02-27 09:49:57.564081: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-02-27 09:49:57.592587: predicting hippocampus_374 
2025-02-27 09:49:57.600588: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-02-27 09:49:57.628587: predicting hippocampus_394 
2025-02-27 09:49:57.635588: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-02-27 09:50:01.259288: Validation complete 
2025-02-27 09:50:01.265290: Mean Validation Dice:  0.247928053187395 
