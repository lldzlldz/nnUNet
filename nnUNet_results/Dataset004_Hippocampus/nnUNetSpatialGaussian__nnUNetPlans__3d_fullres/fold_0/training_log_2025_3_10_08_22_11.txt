
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-10 08:22:11.350919: do_dummy_2d_data_aug: False 
2025-03-10 08:22:11.374069: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-10 08:22:11.382080: The split file contains 5 splits. 
2025-03-10 08:22:11.385081: Desired fold for training: 0 
2025-03-10 08:22:11.388080: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-10 08:22:17.824223: unpacking dataset... 
2025-03-10 08:22:18.195482: unpacking done... 
2025-03-10 08:22:20.309373:  
2025-03-10 08:22:20.313389: Epoch 0 
2025-03-10 08:22:20.317400: Current learning rate: 0.01 
2025-03-10 08:22:27.799995: train_loss -0.5398 
2025-03-10 08:22:27.806087: val_loss -0.8101 
2025-03-10 08:22:27.809631: Pseudo dice [np.float32(0.865), np.float32(0.8511)] 
2025-03-10 08:22:27.813160: Epoch time: 7.49 s 
2025-03-10 08:22:27.816277: Yayy! New best EMA pseudo Dice: 0.8580999970436096 
2025-03-10 08:22:28.324105:  
2025-03-10 08:22:28.329619: Epoch 1 
2025-03-10 08:22:28.332125: Current learning rate: 0.00991 
2025-03-10 08:22:34.931204: train_loss -0.8225 
2025-03-10 08:22:34.937341: val_loss -0.8375 
2025-03-10 08:22:34.940875: Pseudo dice [np.float32(0.8855), np.float32(0.8712)] 
2025-03-10 08:22:34.944432: Epoch time: 6.61 s 
2025-03-10 08:22:34.948057: Yayy! New best EMA pseudo Dice: 0.8600999712944031 
2025-03-10 08:22:35.518249:  
2025-03-10 08:22:35.523767: Epoch 2 
2025-03-10 08:22:35.527278: Current learning rate: 0.00982 
2025-03-10 08:22:42.131903: train_loss -0.8447 
2025-03-10 08:22:42.139409: val_loss -0.8416 
2025-03-10 08:22:42.142460: Pseudo dice [np.float32(0.8918), np.float32(0.8742)] 
2025-03-10 08:22:42.145508: Epoch time: 6.61 s 
2025-03-10 08:22:42.149018: Yayy! New best EMA pseudo Dice: 0.8623999953269958 
2025-03-10 08:22:42.743779:  
2025-03-10 08:22:42.749816: Epoch 3 
2025-03-10 08:22:42.753330: Current learning rate: 0.00973 
2025-03-10 08:22:49.345630: train_loss -0.8542 
2025-03-10 08:22:49.351162: val_loss -0.8414 
2025-03-10 08:22:49.354674: Pseudo dice [np.float32(0.8913), np.float32(0.8718)] 
2025-03-10 08:22:49.358181: Epoch time: 6.6 s 
2025-03-10 08:22:49.361192: Yayy! New best EMA pseudo Dice: 0.864300012588501 
2025-03-10 08:22:49.933854:  
2025-03-10 08:22:49.939387: Epoch 4 
2025-03-10 08:22:49.942956: Current learning rate: 0.00964 
2025-03-10 08:22:56.528288: train_loss -0.8628 
2025-03-10 08:22:56.534828: val_loss -0.8422 
2025-03-10 08:22:56.538904: Pseudo dice [np.float32(0.8925), np.float32(0.8743)] 
2025-03-10 08:22:56.541958: Epoch time: 6.6 s 
2025-03-10 08:22:56.545042: Yayy! New best EMA pseudo Dice: 0.8661999702453613 
2025-03-10 08:22:57.261225:  
2025-03-10 08:22:57.266237: Epoch 5 
2025-03-10 08:22:57.269751: Current learning rate: 0.00955 
2025-03-10 08:23:03.863475: train_loss -0.8688 
2025-03-10 08:23:03.869699: val_loss -0.8517 
2025-03-10 08:23:03.872741: Pseudo dice [np.float32(0.8984), np.float32(0.8816)] 
2025-03-10 08:23:03.875818: Epoch time: 6.6 s 
2025-03-10 08:23:03.878915: Yayy! New best EMA pseudo Dice: 0.8686000108718872 
2025-03-10 08:23:04.447020:  
2025-03-10 08:23:04.453539: Epoch 6 
2025-03-10 08:23:04.456043: Current learning rate: 0.00946 
2025-03-10 08:23:11.046143: train_loss -0.8734 
2025-03-10 08:23:11.051269: val_loss -0.8487 
2025-03-10 08:23:11.053808: Pseudo dice [np.float32(0.897), np.float32(0.8805)] 
2025-03-10 08:23:11.057852: Epoch time: 6.6 s 
2025-03-10 08:23:11.060912: Yayy! New best EMA pseudo Dice: 0.8705999851226807 
2025-03-10 08:23:11.636255:  
2025-03-10 08:23:11.641841: Epoch 7 
2025-03-10 08:23:11.645902: Current learning rate: 0.00937 
2025-03-10 08:23:18.225297: train_loss -0.8789 
2025-03-10 08:23:18.231514: val_loss -0.8413 
2025-03-10 08:23:18.235109: Pseudo dice [np.float32(0.8948), np.float32(0.8709)] 
2025-03-10 08:23:18.238136: Epoch time: 6.59 s 
2025-03-10 08:23:18.241782: Yayy! New best EMA pseudo Dice: 0.8718000054359436 
2025-03-10 08:23:18.830430:  
2025-03-10 08:23:18.836508: Epoch 8 
2025-03-10 08:23:18.839014: Current learning rate: 0.00928 
2025-03-10 08:23:25.432571: train_loss -0.884 
2025-03-10 08:23:25.439193: val_loss -0.8453 
2025-03-10 08:23:25.442860: Pseudo dice [np.float32(0.8971), np.float32(0.8764)] 
2025-03-10 08:23:25.445925: Epoch time: 6.6 s 
2025-03-10 08:23:25.448999: Yayy! New best EMA pseudo Dice: 0.8733000159263611 
2025-03-10 08:23:26.039411:  
2025-03-10 08:23:26.044974: Epoch 9 
2025-03-10 08:23:26.049032: Current learning rate: 0.00919 
2025-03-10 08:23:32.626903: train_loss -0.8884 
2025-03-10 08:23:32.632500: val_loss -0.8498 
2025-03-10 08:23:32.636537: Pseudo dice [np.float32(0.8989), np.float32(0.8806)] 
2025-03-10 08:23:32.639613: Epoch time: 6.59 s 
2025-03-10 08:23:32.642654: Yayy! New best EMA pseudo Dice: 0.875 
2025-03-10 08:23:33.208955:  
2025-03-10 08:23:33.213969: Epoch 10 
2025-03-10 08:23:33.217477: Current learning rate: 0.0091 
2025-03-10 08:23:39.786106: train_loss -0.8913 
2025-03-10 08:23:39.792252: val_loss -0.8466 
2025-03-10 08:23:39.795326: Pseudo dice [np.float32(0.8971), np.float32(0.8775)] 
2025-03-10 08:23:39.798914: Epoch time: 6.58 s 
2025-03-10 08:23:39.801484: Yayy! New best EMA pseudo Dice: 0.8762000203132629 
2025-03-10 08:23:40.375221:  
2025-03-10 08:23:40.380776: Epoch 11 
2025-03-10 08:23:40.384396: Current learning rate: 0.009 
2025-03-10 08:23:46.972238: train_loss -0.8939 
2025-03-10 08:23:46.978914: val_loss -0.8461 
2025-03-10 08:23:46.981471: Pseudo dice [np.float32(0.8981), np.float32(0.8784)] 
2025-03-10 08:23:46.987653: Epoch time: 6.6 s 
2025-03-10 08:23:46.991316: Yayy! New best EMA pseudo Dice: 0.8773999810218811 
2025-03-10 08:23:47.570200:  
2025-03-10 08:23:47.575741: Epoch 12 
2025-03-10 08:23:47.578781: Current learning rate: 0.00891 
2025-03-10 08:23:54.170341: train_loss -0.8955 
2025-03-10 08:23:54.176889: val_loss -0.8381 
2025-03-10 08:23:54.180483: Pseudo dice [np.float32(0.8918), np.float32(0.8722)] 
2025-03-10 08:23:54.183624: Epoch time: 6.6 s 
2025-03-10 08:23:54.186666: Yayy! New best EMA pseudo Dice: 0.8779000043869019 
2025-03-10 08:23:54.900245:  
2025-03-10 08:23:54.905258: Epoch 13 
2025-03-10 08:23:54.908271: Current learning rate: 0.00882 
2025-03-10 08:24:01.495330: train_loss -0.899 
2025-03-10 08:24:01.500884: val_loss -0.8461 
2025-03-10 08:24:01.503952: Pseudo dice [np.float32(0.8956), np.float32(0.8793)] 
2025-03-10 08:24:01.507509: Epoch time: 6.6 s 
2025-03-10 08:24:01.511069: Yayy! New best EMA pseudo Dice: 0.8787999749183655 
2025-03-10 08:24:02.089844:  
2025-03-10 08:24:02.095854: Epoch 14 
2025-03-10 08:24:02.099868: Current learning rate: 0.00873 
2025-03-10 08:24:08.673712: train_loss -0.9008 
2025-03-10 08:24:08.679799: val_loss -0.8459 
2025-03-10 08:24:08.683329: Pseudo dice [np.float32(0.8982), np.float32(0.8768)] 
2025-03-10 08:24:08.686982: Epoch time: 6.58 s 
2025-03-10 08:24:08.690084: Yayy! New best EMA pseudo Dice: 0.8797000050544739 
2025-03-10 08:24:09.274551:  
2025-03-10 08:24:09.280678: Epoch 15 
2025-03-10 08:24:09.283716: Current learning rate: 0.00864 
2025-03-10 08:24:15.856173: train_loss -0.9024 
2025-03-10 08:24:15.862241: val_loss -0.8474 
2025-03-10 08:24:15.865301: Pseudo dice [np.float32(0.8988), np.float32(0.878)] 
2025-03-10 08:24:15.868926: Epoch time: 6.58 s 
2025-03-10 08:24:15.871982: Yayy! New best EMA pseudo Dice: 0.8805999755859375 
2025-03-10 08:24:16.462915:  
2025-03-10 08:24:16.468459: Epoch 16 
2025-03-10 08:24:16.471489: Current learning rate: 0.00855 
2025-03-10 08:24:23.046328: train_loss -0.9042 
2025-03-10 08:24:23.052904: val_loss -0.8391 
2025-03-10 08:24:23.055927: Pseudo dice [np.float32(0.8942), np.float32(0.8739)] 
2025-03-10 08:24:23.058972: Epoch time: 6.58 s 
2025-03-10 08:24:23.063599: Yayy! New best EMA pseudo Dice: 0.8809000253677368 
2025-03-10 08:24:23.661405:  
2025-03-10 08:24:23.666416: Epoch 17 
2025-03-10 08:24:23.669930: Current learning rate: 0.00846 
2025-03-10 08:24:30.239581: train_loss -0.9064 
2025-03-10 08:24:30.245700: val_loss -0.8418 
2025-03-10 08:24:30.248789: Pseudo dice [np.float32(0.8963), np.float32(0.8744)] 
2025-03-10 08:24:30.252301: Epoch time: 6.58 s 
2025-03-10 08:24:30.255372: Yayy! New best EMA pseudo Dice: 0.8813999891281128 
2025-03-10 08:24:30.849979:  
2025-03-10 08:24:30.854992: Epoch 18 
2025-03-10 08:24:30.858565: Current learning rate: 0.00836 
2025-03-10 08:24:37.429008: train_loss -0.9078 
2025-03-10 08:24:37.435092: val_loss -0.8433 
2025-03-10 08:24:37.437634: Pseudo dice [np.float32(0.8985), np.float32(0.8772)] 
2025-03-10 08:24:37.441684: Epoch time: 6.58 s 
2025-03-10 08:24:37.444788: Yayy! New best EMA pseudo Dice: 0.8820000290870667 
2025-03-10 08:24:38.038344:  
2025-03-10 08:24:38.043088: Epoch 19 
2025-03-10 08:24:38.046600: Current learning rate: 0.00827 
2025-03-10 08:24:44.628077: train_loss -0.91 
2025-03-10 08:24:44.635797: val_loss -0.8471 
2025-03-10 08:24:44.638318: Pseudo dice [np.float32(0.8996), np.float32(0.8794)] 
2025-03-10 08:24:44.642008: Epoch time: 6.59 s 
2025-03-10 08:24:44.645541: Yayy! New best EMA pseudo Dice: 0.8827999830245972 
2025-03-10 08:24:45.380636:  
2025-03-10 08:24:45.386177: Epoch 20 
2025-03-10 08:24:45.389736: Current learning rate: 0.00818 
2025-03-10 08:24:51.960191: train_loss -0.9097 
2025-03-10 08:24:51.965280: val_loss -0.8456 
2025-03-10 08:24:51.969352: Pseudo dice [np.float32(0.8999), np.float32(0.878)] 
2025-03-10 08:24:51.972480: Epoch time: 6.58 s 
2025-03-10 08:24:51.976046: Yayy! New best EMA pseudo Dice: 0.883400022983551 
2025-03-10 08:24:52.571206:  
2025-03-10 08:24:52.576757: Epoch 21 
2025-03-10 08:24:52.579789: Current learning rate: 0.00809 
2025-03-10 08:24:59.142945: train_loss -0.9122 
2025-03-10 08:24:59.148599: val_loss -0.8358 
2025-03-10 08:24:59.152109: Pseudo dice [np.float32(0.8926), np.float32(0.8737)] 
2025-03-10 08:24:59.155613: Epoch time: 6.57 s 
2025-03-10 08:24:59.691981:  
2025-03-10 08:24:59.697552: Epoch 22 
2025-03-10 08:24:59.700612: Current learning rate: 0.008 
2025-03-10 08:25:06.276039: train_loss -0.9128 
2025-03-10 08:25:06.282220: val_loss -0.838 
2025-03-10 08:25:06.286316: Pseudo dice [np.float32(0.896), np.float32(0.874)] 
2025-03-10 08:25:06.289394: Epoch time: 6.58 s 
2025-03-10 08:25:06.292540: Yayy! New best EMA pseudo Dice: 0.8834999799728394 
2025-03-10 08:25:06.857569:  
2025-03-10 08:25:06.862587: Epoch 23 
2025-03-10 08:25:06.866096: Current learning rate: 0.0079 
2025-03-10 08:25:13.447847: train_loss -0.9149 
2025-03-10 08:25:13.453996: val_loss -0.8374 
2025-03-10 08:25:13.457104: Pseudo dice [np.float32(0.8946), np.float32(0.8743)] 
2025-03-10 08:25:13.460658: Epoch time: 6.59 s 
2025-03-10 08:25:13.463185: Yayy! New best EMA pseudo Dice: 0.8835999965667725 
2025-03-10 08:25:14.021629:  
2025-03-10 08:25:14.027211: Epoch 24 
2025-03-10 08:25:14.029766: Current learning rate: 0.00781 
2025-03-10 08:25:20.605747: train_loss -0.9164 
2025-03-10 08:25:20.612901: val_loss -0.8415 
2025-03-10 08:25:20.616412: Pseudo dice [np.float32(0.8985), np.float32(0.8778)] 
2025-03-10 08:25:20.620474: Epoch time: 6.58 s 
2025-03-10 08:25:20.623000: Yayy! New best EMA pseudo Dice: 0.8841000199317932 
2025-03-10 08:25:21.194596:  
2025-03-10 08:25:21.199610: Epoch 25 
2025-03-10 08:25:21.203126: Current learning rate: 0.00772 
2025-03-10 08:25:27.788767: train_loss -0.9163 
2025-03-10 08:25:27.794895: val_loss -0.8399 
2025-03-10 08:25:27.798668: Pseudo dice [np.float32(0.898), np.float32(0.8763)] 
2025-03-10 08:25:27.802182: Epoch time: 6.59 s 
2025-03-10 08:25:27.804349: Yayy! New best EMA pseudo Dice: 0.8844000101089478 
2025-03-10 08:25:28.376873:  
2025-03-10 08:25:28.383001: Epoch 26 
2025-03-10 08:25:28.386045: Current learning rate: 0.00763 
2025-03-10 08:25:34.963289: train_loss -0.9171 
2025-03-10 08:25:34.967314: val_loss -0.8404 
2025-03-10 08:25:34.970831: Pseudo dice [np.float32(0.8973), np.float32(0.8774)] 
2025-03-10 08:25:34.974852: Epoch time: 6.59 s 
2025-03-10 08:25:34.977953: Yayy! New best EMA pseudo Dice: 0.8847000002861023 
2025-03-10 08:25:35.553922:  
2025-03-10 08:25:35.559489: Epoch 27 
2025-03-10 08:25:35.563053: Current learning rate: 0.00753 
2025-03-10 08:25:42.140229: train_loss -0.9181 
2025-03-10 08:25:42.146333: val_loss -0.8451 
2025-03-10 08:25:42.148893: Pseudo dice [np.float32(0.9013), np.float32(0.88)] 
2025-03-10 08:25:42.152910: Epoch time: 6.59 s 
2025-03-10 08:25:42.155924: Yayy! New best EMA pseudo Dice: 0.8852999806404114 
2025-03-10 08:25:42.878157:  
2025-03-10 08:25:42.885195: Epoch 28 
2025-03-10 08:25:42.889208: Current learning rate: 0.00744 
2025-03-10 08:25:49.450117: train_loss -0.9192 
2025-03-10 08:25:49.455679: val_loss -0.8342 
2025-03-10 08:25:49.459719: Pseudo dice [np.float32(0.8951), np.float32(0.873)] 
2025-03-10 08:25:49.462757: Epoch time: 6.57 s 
2025-03-10 08:25:50.001749:  
2025-03-10 08:25:50.007770: Epoch 29 
2025-03-10 08:25:50.011277: Current learning rate: 0.00735 
2025-03-10 08:25:56.595257: train_loss -0.9191 
2025-03-10 08:25:56.601359: val_loss -0.8379 
2025-03-10 08:25:56.604936: Pseudo dice [np.float32(0.8974), np.float32(0.8751)] 
2025-03-10 08:25:56.607990: Epoch time: 6.59 s 
2025-03-10 08:25:57.155305:  
2025-03-10 08:25:57.162905: Epoch 30 
2025-03-10 08:25:57.166473: Current learning rate: 0.00725 
2025-03-10 08:26:03.729533: train_loss -0.9225 
2025-03-10 08:26:03.734659: val_loss -0.8328 
2025-03-10 08:26:03.738710: Pseudo dice [np.float32(0.8944), np.float32(0.873)] 
2025-03-10 08:26:03.741752: Epoch time: 6.57 s 
2025-03-10 08:26:04.287577:  
2025-03-10 08:26:04.292591: Epoch 31 
2025-03-10 08:26:04.296101: Current learning rate: 0.00716 
2025-03-10 08:26:10.858432: train_loss -0.9214 
2025-03-10 08:26:10.864031: val_loss -0.8422 
2025-03-10 08:26:10.866593: Pseudo dice [np.float32(0.9002), np.float32(0.8798)] 
2025-03-10 08:26:10.869102: Epoch time: 6.57 s 
2025-03-10 08:26:10.873670: Yayy! New best EMA pseudo Dice: 0.8855999708175659 
2025-03-10 08:26:11.459075:  
2025-03-10 08:26:11.464645: Epoch 32 
2025-03-10 08:26:11.467185: Current learning rate: 0.00707 
2025-03-10 08:26:18.046050: train_loss -0.9221 
2025-03-10 08:26:18.051638: val_loss -0.8396 
2025-03-10 08:26:18.055727: Pseudo dice [np.float32(0.8983), np.float32(0.8771)] 
2025-03-10 08:26:18.058004: Epoch time: 6.59 s 
2025-03-10 08:26:18.061588: Yayy! New best EMA pseudo Dice: 0.8858000040054321 
2025-03-10 08:26:18.644213:  
2025-03-10 08:26:18.649753: Epoch 33 
2025-03-10 08:26:18.652789: Current learning rate: 0.00697 
2025-03-10 08:26:25.230120: train_loss -0.9228 
2025-03-10 08:26:25.236236: val_loss -0.8392 
2025-03-10 08:26:25.240409: Pseudo dice [np.float32(0.8972), np.float32(0.8775)] 
2025-03-10 08:26:25.243454: Epoch time: 6.59 s 
2025-03-10 08:26:25.246034: Yayy! New best EMA pseudo Dice: 0.8859999775886536 
2025-03-10 08:26:25.826670:  
2025-03-10 08:26:25.832188: Epoch 34 
2025-03-10 08:26:25.834694: Current learning rate: 0.00688 
2025-03-10 08:26:32.419014: train_loss -0.9241 
2025-03-10 08:26:32.425635: val_loss -0.8464 
2025-03-10 08:26:32.428661: Pseudo dice [np.float32(0.9032), np.float32(0.8838)] 
2025-03-10 08:26:32.431692: Epoch time: 6.59 s 
2025-03-10 08:26:32.435341: Yayy! New best EMA pseudo Dice: 0.8866999745368958 
2025-03-10 08:26:33.028503:  
2025-03-10 08:26:33.033687: Epoch 35 
2025-03-10 08:26:33.037199: Current learning rate: 0.00679 
2025-03-10 08:26:39.625746: train_loss -0.9248 
2025-03-10 08:26:39.632270: val_loss -0.8383 
2025-03-10 08:26:39.635783: Pseudo dice [np.float32(0.8983), np.float32(0.878)] 
2025-03-10 08:26:39.638289: Epoch time: 6.6 s 
2025-03-10 08:26:39.642304: Yayy! New best EMA pseudo Dice: 0.886900007724762 
2025-03-10 08:26:40.372754:  
2025-03-10 08:26:40.378825: Epoch 36 
2025-03-10 08:26:40.381917: Current learning rate: 0.00669 
2025-03-10 08:26:46.967814: train_loss -0.9248 
2025-03-10 08:26:46.974330: val_loss -0.831 
2025-03-10 08:26:46.977363: Pseudo dice [np.float32(0.8948), np.float32(0.8726)] 
2025-03-10 08:26:46.980880: Epoch time: 6.6 s 
2025-03-10 08:26:47.540516:  
2025-03-10 08:26:47.543541: Epoch 37 
2025-03-10 08:26:47.547603: Current learning rate: 0.0066 
2025-03-10 08:26:54.130481: train_loss -0.9278 
2025-03-10 08:26:54.136531: val_loss -0.84 
2025-03-10 08:26:54.140240: Pseudo dice [np.float32(0.899), np.float32(0.88)] 
2025-03-10 08:26:54.142767: Epoch time: 6.59 s 
2025-03-10 08:26:54.699216:  
2025-03-10 08:26:54.704229: Epoch 38 
2025-03-10 08:26:54.707743: Current learning rate: 0.0065 
2025-03-10 08:27:01.270119: train_loss -0.9283 
2025-03-10 08:27:01.276238: val_loss -0.8349 
2025-03-10 08:27:01.279806: Pseudo dice [np.float32(0.8981), np.float32(0.876)] 
2025-03-10 08:27:01.283337: Epoch time: 6.57 s 
2025-03-10 08:27:01.286402: Yayy! New best EMA pseudo Dice: 0.886900007724762 
2025-03-10 08:27:01.882342:  
2025-03-10 08:27:01.887900: Epoch 39 
2025-03-10 08:27:01.890454: Current learning rate: 0.00641 
2025-03-10 08:27:08.460107: train_loss -0.927 
2025-03-10 08:27:08.467084: val_loss -0.837 
2025-03-10 08:27:08.470145: Pseudo dice [np.float32(0.8988), np.float32(0.8779)] 
2025-03-10 08:27:08.474159: Epoch time: 6.58 s 
2025-03-10 08:27:08.477206: Yayy! New best EMA pseudo Dice: 0.8870000243186951 
2025-03-10 08:27:09.080303:  
2025-03-10 08:27:09.085872: Epoch 40 
2025-03-10 08:27:09.089438: Current learning rate: 0.00631 
2025-03-10 08:27:15.686501: train_loss -0.9283 
2025-03-10 08:27:15.692122: val_loss -0.8332 
2025-03-10 08:27:15.696198: Pseudo dice [np.float32(0.896), np.float32(0.8755)] 
2025-03-10 08:27:15.699322: Epoch time: 6.61 s 
2025-03-10 08:27:16.271071:  
2025-03-10 08:27:16.277170: Epoch 41 
2025-03-10 08:27:16.280746: Current learning rate: 0.00622 
2025-03-10 08:27:22.855365: train_loss -0.9271 
2025-03-10 08:27:22.861452: val_loss -0.8337 
2025-03-10 08:27:22.864000: Pseudo dice [np.float32(0.8963), np.float32(0.8769)] 
2025-03-10 08:27:22.868078: Epoch time: 6.58 s 
2025-03-10 08:27:23.402318:  
2025-03-10 08:27:23.407876: Epoch 42 
2025-03-10 08:27:23.410916: Current learning rate: 0.00612 
2025-03-10 08:27:29.992182: train_loss -0.9288 
2025-03-10 08:27:29.998359: val_loss -0.8331 
2025-03-10 08:27:30.001961: Pseudo dice [np.float32(0.8953), np.float32(0.8775)] 
2025-03-10 08:27:30.005037: Epoch time: 6.59 s 
2025-03-10 08:27:30.681411:  
2025-03-10 08:27:30.688930: Epoch 43 
2025-03-10 08:27:30.691941: Current learning rate: 0.00603 
2025-03-10 08:27:37.265828: train_loss -0.928 
2025-03-10 08:27:37.271114: val_loss -0.8315 
2025-03-10 08:27:37.277492: Pseudo dice [np.float32(0.8955), np.float32(0.875)] 
2025-03-10 08:27:37.281222: Epoch time: 6.59 s 
2025-03-10 08:27:37.817148:  
2025-03-10 08:27:37.822187: Epoch 44 
2025-03-10 08:27:37.825766: Current learning rate: 0.00593 
2025-03-10 08:27:44.400010: train_loss -0.9309 
2025-03-10 08:27:44.407274: val_loss -0.8366 
2025-03-10 08:27:44.410894: Pseudo dice [np.float32(0.8977), np.float32(0.8779)] 
2025-03-10 08:27:44.414469: Epoch time: 6.58 s 
2025-03-10 08:27:44.948359:  
2025-03-10 08:27:44.954010: Epoch 45 
2025-03-10 08:27:44.957047: Current learning rate: 0.00584 
2025-03-10 08:27:51.532585: train_loss -0.9301 
2025-03-10 08:27:51.538644: val_loss -0.8363 
2025-03-10 08:27:51.542353: Pseudo dice [np.float32(0.8973), np.float32(0.8779)] 
2025-03-10 08:27:51.544924: Epoch time: 6.58 s 
2025-03-10 08:27:52.079313:  
2025-03-10 08:27:52.085831: Epoch 46 
2025-03-10 08:27:52.089350: Current learning rate: 0.00574 
2025-03-10 08:27:58.661604: train_loss -0.9315 
2025-03-10 08:27:58.667668: val_loss -0.8291 
2025-03-10 08:27:58.670705: Pseudo dice [np.float32(0.8942), np.float32(0.8744)] 
2025-03-10 08:27:58.672752: Epoch time: 6.58 s 
2025-03-10 08:27:59.204457:  
2025-03-10 08:27:59.209976: Epoch 47 
2025-03-10 08:27:59.213487: Current learning rate: 0.00565 
2025-03-10 08:28:05.798928: train_loss -0.9304 
2025-03-10 08:28:05.804519: val_loss -0.8329 
2025-03-10 08:28:05.807213: Pseudo dice [np.float32(0.8971), np.float32(0.876)] 
2025-03-10 08:28:05.811257: Epoch time: 6.6 s 
2025-03-10 08:28:06.351429:  
2025-03-10 08:28:06.357506: Epoch 48 
2025-03-10 08:28:06.360589: Current learning rate: 0.00555 
2025-03-10 08:28:12.940553: train_loss -0.9326 
2025-03-10 08:28:12.946342: val_loss -0.8269 
2025-03-10 08:28:12.949855: Pseudo dice [np.float32(0.8926), np.float32(0.8735)] 
2025-03-10 08:28:12.951870: Epoch time: 6.59 s 
2025-03-10 08:28:13.493226:  
2025-03-10 08:28:13.498238: Epoch 49 
2025-03-10 08:28:13.501751: Current learning rate: 0.00546 
2025-03-10 08:28:20.081217: train_loss -0.9321 
2025-03-10 08:28:20.087231: val_loss -0.8279 
2025-03-10 08:28:20.090293: Pseudo dice [np.float32(0.8939), np.float32(0.8746)] 
2025-03-10 08:28:20.093805: Epoch time: 6.59 s 
2025-03-10 08:28:20.679131:  
2025-03-10 08:28:20.684144: Epoch 50 
2025-03-10 08:28:20.687657: Current learning rate: 0.00536 
2025-03-10 08:28:27.268770: train_loss -0.9322 
2025-03-10 08:28:27.274968: val_loss -0.833 
2025-03-10 08:28:27.278044: Pseudo dice [np.float32(0.8966), np.float32(0.8763)] 
2025-03-10 08:28:27.281123: Epoch time: 6.59 s 
2025-03-10 08:28:27.965357:  
2025-03-10 08:28:27.970996: Epoch 51 
2025-03-10 08:28:27.974051: Current learning rate: 0.00526 
2025-03-10 08:28:34.542063: train_loss -0.9341 
2025-03-10 08:28:34.548642: val_loss -0.8358 
2025-03-10 08:28:34.552655: Pseudo dice [np.float32(0.8998), np.float32(0.8781)] 
2025-03-10 08:28:34.555162: Epoch time: 6.58 s 
2025-03-10 08:28:35.100380:  
2025-03-10 08:28:35.106400: Epoch 52 
2025-03-10 08:28:35.108911: Current learning rate: 0.00517 
2025-03-10 08:28:41.681883: train_loss -0.9325 
2025-03-10 08:28:41.688074: val_loss -0.8339 
2025-03-10 08:28:41.691589: Pseudo dice [np.float32(0.897), np.float32(0.8774)] 
2025-03-10 08:28:41.695096: Epoch time: 6.58 s 
2025-03-10 08:28:42.245465:  
2025-03-10 08:28:42.251011: Epoch 53 
2025-03-10 08:28:42.254043: Current learning rate: 0.00507 
2025-03-10 08:28:48.821659: train_loss -0.9318 
2025-03-10 08:28:48.828299: val_loss -0.8347 
2025-03-10 08:28:48.831327: Pseudo dice [np.float32(0.8981), np.float32(0.8777)] 
2025-03-10 08:28:48.834850: Epoch time: 6.58 s 
2025-03-10 08:28:49.372270:  
2025-03-10 08:28:49.377785: Epoch 54 
2025-03-10 08:28:49.381300: Current learning rate: 0.00497 
2025-03-10 08:28:55.968212: train_loss -0.9341 
2025-03-10 08:28:55.975303: val_loss -0.8307 
2025-03-10 08:28:55.977811: Pseudo dice [np.float32(0.8955), np.float32(0.8756)] 
2025-03-10 08:28:55.981322: Epoch time: 6.6 s 
2025-03-10 08:28:56.527188:  
2025-03-10 08:28:56.532204: Epoch 55 
2025-03-10 08:28:56.535715: Current learning rate: 0.00487 
2025-03-10 08:29:03.114656: train_loss -0.9348 
2025-03-10 08:29:03.120826: val_loss -0.827 
2025-03-10 08:29:03.123899: Pseudo dice [np.float32(0.8939), np.float32(0.8726)] 
2025-03-10 08:29:03.127443: Epoch time: 6.59 s 
2025-03-10 08:29:03.672802:  
2025-03-10 08:29:03.677834: Epoch 56 
2025-03-10 08:29:03.681936: Current learning rate: 0.00478 
2025-03-10 08:29:10.260261: train_loss -0.9361 
2025-03-10 08:29:10.266831: val_loss -0.8277 
2025-03-10 08:29:10.270379: Pseudo dice [np.float32(0.8928), np.float32(0.8756)] 
2025-03-10 08:29:10.272969: Epoch time: 6.59 s 
2025-03-10 08:29:10.820517:  
2025-03-10 08:29:10.826035: Epoch 57 
2025-03-10 08:29:10.828542: Current learning rate: 0.00468 
2025-03-10 08:29:17.398890: train_loss -0.9359 
2025-03-10 08:29:17.404495: val_loss -0.8306 
2025-03-10 08:29:17.407587: Pseudo dice [np.float32(0.897), np.float32(0.8758)] 
2025-03-10 08:29:17.411650: Epoch time: 6.58 s 
2025-03-10 08:29:17.954543:  
2025-03-10 08:29:17.959668: Epoch 58 
2025-03-10 08:29:17.963794: Current learning rate: 0.00458 
2025-03-10 08:29:24.541839: train_loss -0.9358 
2025-03-10 08:29:24.548444: val_loss -0.8269 
2025-03-10 08:29:24.551495: Pseudo dice [np.float32(0.8942), np.float32(0.8741)] 
2025-03-10 08:29:24.555025: Epoch time: 6.59 s 
2025-03-10 08:29:25.259074:  
2025-03-10 08:29:25.265159: Epoch 59 
2025-03-10 08:29:25.268231: Current learning rate: 0.00448 
2025-03-10 08:29:31.832345: train_loss -0.9362 
2025-03-10 08:29:31.837914: val_loss -0.8338 
2025-03-10 08:29:31.841457: Pseudo dice [np.float32(0.8978), np.float32(0.878)] 
2025-03-10 08:29:31.844110: Epoch time: 6.57 s 
2025-03-10 08:29:32.392593:  
2025-03-10 08:29:32.398628: Epoch 60 
2025-03-10 08:29:32.401201: Current learning rate: 0.00438 
2025-03-10 08:29:38.987006: train_loss -0.9369 
2025-03-10 08:29:38.993562: val_loss -0.8282 
2025-03-10 08:29:38.997100: Pseudo dice [np.float32(0.8942), np.float32(0.8756)] 
2025-03-10 08:29:38.999862: Epoch time: 6.59 s 
2025-03-10 08:29:39.553353:  
2025-03-10 08:29:39.558873: Epoch 61 
2025-03-10 08:29:39.562382: Current learning rate: 0.00429 
2025-03-10 08:29:46.141019: train_loss -0.9363 
2025-03-10 08:29:46.147104: val_loss -0.8336 
2025-03-10 08:29:46.150634: Pseudo dice [np.float32(0.8977), np.float32(0.8788)] 
2025-03-10 08:29:46.153101: Epoch time: 6.59 s 
2025-03-10 08:29:46.706458:  
2025-03-10 08:29:46.711473: Epoch 62 
2025-03-10 08:29:46.714981: Current learning rate: 0.00419 
2025-03-10 08:29:53.278572: train_loss -0.9377 
2025-03-10 08:29:53.283736: val_loss -0.8324 
2025-03-10 08:29:53.288335: Pseudo dice [np.float32(0.8971), np.float32(0.8782)] 
2025-03-10 08:29:53.291903: Epoch time: 6.57 s 
2025-03-10 08:29:53.848264:  
2025-03-10 08:29:53.853782: Epoch 63 
2025-03-10 08:29:53.857294: Current learning rate: 0.00409 
2025-03-10 08:30:00.426260: train_loss -0.9393 
2025-03-10 08:30:00.432358: val_loss -0.8342 
2025-03-10 08:30:00.436406: Pseudo dice [np.float32(0.8984), np.float32(0.8784)] 
2025-03-10 08:30:00.439976: Epoch time: 6.58 s 
2025-03-10 08:30:00.993838:  
2025-03-10 08:30:00.999357: Epoch 64 
2025-03-10 08:30:01.003866: Current learning rate: 0.00399 
2025-03-10 08:30:07.590835: train_loss -0.9384 
2025-03-10 08:30:07.597474: val_loss -0.8315 
2025-03-10 08:30:07.601617: Pseudo dice [np.float32(0.8966), np.float32(0.8781)] 
2025-03-10 08:30:07.605628: Epoch time: 6.6 s 
2025-03-10 08:30:08.158346:  
2025-03-10 08:30:08.164365: Epoch 65 
2025-03-10 08:30:08.168378: Current learning rate: 0.00389 
2025-03-10 08:30:14.758485: train_loss -0.9374 
2025-03-10 08:30:14.765155: val_loss -0.8246 
2025-03-10 08:30:14.769180: Pseudo dice [np.float32(0.8924), np.float32(0.874)] 
2025-03-10 08:30:14.772705: Epoch time: 6.6 s 
2025-03-10 08:30:15.478205:  
2025-03-10 08:30:15.484727: Epoch 66 
2025-03-10 08:30:15.488734: Current learning rate: 0.00379 
2025-03-10 08:30:22.058663: train_loss -0.9371 
2025-03-10 08:30:22.065285: val_loss -0.8333 
2025-03-10 08:30:22.068865: Pseudo dice [np.float32(0.8982), np.float32(0.8784)] 
2025-03-10 08:30:22.073876: Epoch time: 6.58 s 
2025-03-10 08:30:22.622351:  
2025-03-10 08:30:22.628381: Epoch 67 
2025-03-10 08:30:22.632390: Current learning rate: 0.00369 
2025-03-10 08:30:29.200805: train_loss -0.9392 
2025-03-10 08:30:29.207017: val_loss -0.8305 
2025-03-10 08:30:29.211050: Pseudo dice [np.float32(0.8963), np.float32(0.8779)] 
2025-03-10 08:30:29.214103: Epoch time: 6.58 s 
2025-03-10 08:30:29.776313:  
2025-03-10 08:30:29.782838: Epoch 68 
2025-03-10 08:30:29.786851: Current learning rate: 0.00359 
2025-03-10 08:30:36.384102: train_loss -0.9381 
2025-03-10 08:30:36.390733: val_loss -0.8362 
2025-03-10 08:30:36.394768: Pseudo dice [np.float32(0.9004), np.float32(0.8801)] 
2025-03-10 08:30:36.398286: Epoch time: 6.61 s 
2025-03-10 08:30:36.958575:  
2025-03-10 08:30:36.965138: Epoch 69 
2025-03-10 08:30:36.968673: Current learning rate: 0.00349 
2025-03-10 08:30:43.540425: train_loss -0.9383 
2025-03-10 08:30:43.546183: val_loss -0.8248 
2025-03-10 08:30:43.550742: Pseudo dice [np.float32(0.8941), np.float32(0.8739)] 
2025-03-10 08:30:43.555327: Epoch time: 6.58 s 
2025-03-10 08:30:44.125435:  
2025-03-10 08:30:44.132514: Epoch 70 
2025-03-10 08:30:44.136112: Current learning rate: 0.00338 
2025-03-10 08:30:50.708607: train_loss -0.9414 
2025-03-10 08:30:50.715275: val_loss -0.8339 
2025-03-10 08:30:50.723364: Pseudo dice [np.float32(0.8988), np.float32(0.8802)] 
2025-03-10 08:30:50.728019: Epoch time: 6.58 s 
2025-03-10 08:30:51.293000:  
2025-03-10 08:30:51.298586: Epoch 71 
2025-03-10 08:30:51.303207: Current learning rate: 0.00328 
2025-03-10 08:30:57.889701: train_loss -0.9412 
2025-03-10 08:30:57.895730: val_loss -0.8313 
2025-03-10 08:30:57.900323: Pseudo dice [np.float32(0.8975), np.float32(0.8785)] 
2025-03-10 08:30:57.904505: Epoch time: 6.6 s 
2025-03-10 08:30:58.472283:  
2025-03-10 08:30:58.478314: Epoch 72 
2025-03-10 08:30:58.481827: Current learning rate: 0.00318 
2025-03-10 08:31:05.070682: train_loss -0.9407 
2025-03-10 08:31:05.077802: val_loss -0.8287 
2025-03-10 08:31:05.081834: Pseudo dice [np.float32(0.8953), np.float32(0.8769)] 
2025-03-10 08:31:05.085440: Epoch time: 6.6 s 
2025-03-10 08:31:05.647472:  
2025-03-10 08:31:05.651532: Epoch 73 
2025-03-10 08:31:05.655044: Current learning rate: 0.00308 
2025-03-10 08:31:12.231810: train_loss -0.9424 
2025-03-10 08:31:12.238000: val_loss -0.8226 
2025-03-10 08:31:12.241722: Pseudo dice [np.float32(0.8931), np.float32(0.8729)] 
2025-03-10 08:31:12.245023: Epoch time: 6.59 s 
2025-03-10 08:31:12.956134:  
2025-03-10 08:31:12.961151: Epoch 74 
2025-03-10 08:31:12.964162: Current learning rate: 0.00297 
2025-03-10 08:31:19.529615: train_loss -0.942 
2025-03-10 08:31:19.536216: val_loss -0.8274 
2025-03-10 08:31:19.538741: Pseudo dice [np.float32(0.8965), np.float32(0.8763)] 
2025-03-10 08:31:19.542773: Epoch time: 6.57 s 
2025-03-10 08:31:20.106254:  
2025-03-10 08:31:20.111291: Epoch 75 
2025-03-10 08:31:20.114871: Current learning rate: 0.00287 
2025-03-10 08:31:26.679316: train_loss -0.9422 
2025-03-10 08:31:26.685393: val_loss -0.8291 
2025-03-10 08:31:26.687921: Pseudo dice [np.float32(0.8958), np.float32(0.8767)] 
2025-03-10 08:31:26.691954: Epoch time: 6.57 s 
2025-03-10 08:31:27.255920:  
2025-03-10 08:31:27.261973: Epoch 76 
2025-03-10 08:31:27.265985: Current learning rate: 0.00277 
2025-03-10 08:31:33.845449: train_loss -0.9404 
2025-03-10 08:31:33.853017: val_loss -0.8265 
2025-03-10 08:31:33.856634: Pseudo dice [np.float32(0.8942), np.float32(0.874)] 
2025-03-10 08:31:33.859729: Epoch time: 6.59 s 
2025-03-10 08:31:34.423198:  
2025-03-10 08:31:34.428721: Epoch 77 
2025-03-10 08:31:34.431231: Current learning rate: 0.00266 
2025-03-10 08:31:41.012549: train_loss -0.942 
2025-03-10 08:31:41.018693: val_loss -0.8265 
2025-03-10 08:31:41.022756: Pseudo dice [np.float32(0.8957), np.float32(0.8754)] 
2025-03-10 08:31:41.025826: Epoch time: 6.59 s 
2025-03-10 08:31:41.604409:  
2025-03-10 08:31:41.610468: Epoch 78 
2025-03-10 08:31:41.613394: Current learning rate: 0.00256 
2025-03-10 08:31:48.212532: train_loss -0.943 
2025-03-10 08:31:48.218170: val_loss -0.8322 
2025-03-10 08:31:48.222278: Pseudo dice [np.float32(0.8981), np.float32(0.8804)] 
2025-03-10 08:31:48.225890: Epoch time: 6.61 s 
2025-03-10 08:31:48.803442:  
2025-03-10 08:31:48.808461: Epoch 79 
2025-03-10 08:31:48.811979: Current learning rate: 0.00245 
2025-03-10 08:31:55.376361: train_loss -0.943 
2025-03-10 08:31:55.382057: val_loss -0.8299 
2025-03-10 08:31:55.386101: Pseudo dice [np.float32(0.8974), np.float32(0.8755)] 
2025-03-10 08:31:55.389650: Epoch time: 6.57 s 
2025-03-10 08:31:55.956226:  
2025-03-10 08:31:55.962751: Epoch 80 
2025-03-10 08:31:55.965261: Current learning rate: 0.00235 
2025-03-10 08:32:02.547600: train_loss -0.9412 
2025-03-10 08:32:02.554181: val_loss -0.8316 
2025-03-10 08:32:02.557773: Pseudo dice [np.float32(0.898), np.float32(0.8789)] 
2025-03-10 08:32:02.560337: Epoch time: 6.59 s 
2025-03-10 08:32:03.137592:  
2025-03-10 08:32:03.142611: Epoch 81 
2025-03-10 08:32:03.146123: Current learning rate: 0.00224 
2025-03-10 08:32:09.714739: train_loss -0.942 
2025-03-10 08:32:09.720760: val_loss -0.8282 
2025-03-10 08:32:09.723895: Pseudo dice [np.float32(0.8958), np.float32(0.8769)] 
2025-03-10 08:32:09.727476: Epoch time: 6.58 s 
2025-03-10 08:32:10.449225:  
2025-03-10 08:32:10.454245: Epoch 82 
2025-03-10 08:32:10.457762: Current learning rate: 0.00214 
2025-03-10 08:32:17.043654: train_loss -0.9446 
2025-03-10 08:32:17.050758: val_loss -0.827 
2025-03-10 08:32:17.054290: Pseudo dice [np.float32(0.8959), np.float32(0.8761)] 
2025-03-10 08:32:17.057334: Epoch time: 6.59 s 
2025-03-10 08:32:17.594249:  
2025-03-10 08:32:17.599803: Epoch 83 
2025-03-10 08:32:17.603357: Current learning rate: 0.00203 
2025-03-10 08:32:24.184706: train_loss -0.943 
2025-03-10 08:32:24.190322: val_loss -0.8209 
2025-03-10 08:32:24.194355: Pseudo dice [np.float32(0.8929), np.float32(0.8731)] 
2025-03-10 08:32:24.197872: Epoch time: 6.59 s 
2025-03-10 08:32:24.733883:  
2025-03-10 08:32:24.740016: Epoch 84 
2025-03-10 08:32:24.742578: Current learning rate: 0.00192 
2025-03-10 08:32:31.319878: train_loss -0.9441 
2025-03-10 08:32:31.325055: val_loss -0.827 
2025-03-10 08:32:31.329131: Pseudo dice [np.float32(0.8948), np.float32(0.8758)] 
2025-03-10 08:32:31.332699: Epoch time: 6.59 s 
2025-03-10 08:32:31.869068:  
2025-03-10 08:32:31.875179: Epoch 85 
2025-03-10 08:32:31.878237: Current learning rate: 0.00181 
2025-03-10 08:32:38.433899: train_loss -0.9439 
2025-03-10 08:32:38.438459: val_loss -0.8265 
2025-03-10 08:32:38.443015: Pseudo dice [np.float32(0.8954), np.float32(0.8759)] 
2025-03-10 08:32:38.446527: Epoch time: 6.56 s 
2025-03-10 08:32:38.980154:  
2025-03-10 08:32:38.985173: Epoch 86 
2025-03-10 08:32:38.988696: Current learning rate: 0.0017 
2025-03-10 08:32:45.568291: train_loss -0.9457 
2025-03-10 08:32:45.575508: val_loss -0.8292 
2025-03-10 08:32:45.579189: Pseudo dice [np.float32(0.8973), np.float32(0.8766)] 
2025-03-10 08:32:45.582301: Epoch time: 6.59 s 
2025-03-10 08:32:46.132514:  
2025-03-10 08:32:46.137591: Epoch 87 
2025-03-10 08:32:46.141108: Current learning rate: 0.00159 
2025-03-10 08:32:52.708697: train_loss -0.9452 
2025-03-10 08:32:52.714804: val_loss -0.8255 
2025-03-10 08:32:52.718383: Pseudo dice [np.float32(0.8956), np.float32(0.8766)] 
2025-03-10 08:32:52.721481: Epoch time: 6.58 s 
2025-03-10 08:32:53.257027:  
2025-03-10 08:32:53.262630: Epoch 88 
2025-03-10 08:32:53.266752: Current learning rate: 0.00148 
2025-03-10 08:32:59.834409: train_loss -0.9439 
2025-03-10 08:32:59.841082: val_loss -0.826 
2025-03-10 08:32:59.844705: Pseudo dice [np.float32(0.8947), np.float32(0.8765)] 
2025-03-10 08:32:59.847805: Epoch time: 6.58 s 
2025-03-10 08:33:00.534589:  
2025-03-10 08:33:00.538609: Epoch 89 
2025-03-10 08:33:00.542626: Current learning rate: 0.00137 
2025-03-10 08:33:07.119805: train_loss -0.9455 
2025-03-10 08:33:07.125875: val_loss -0.8244 
2025-03-10 08:33:07.129530: Pseudo dice [np.float32(0.8946), np.float32(0.8746)] 
2025-03-10 08:33:07.132597: Epoch time: 6.59 s 
2025-03-10 08:33:07.668909:  
2025-03-10 08:33:07.674564: Epoch 90 
2025-03-10 08:33:07.678641: Current learning rate: 0.00126 
2025-03-10 08:33:14.245870: train_loss -0.9456 
2025-03-10 08:33:14.251055: val_loss -0.8281 
2025-03-10 08:33:14.255109: Pseudo dice [np.float32(0.8971), np.float32(0.8769)] 
2025-03-10 08:33:14.258152: Epoch time: 6.58 s 
2025-03-10 08:33:14.791284:  
2025-03-10 08:33:14.796807: Epoch 91 
2025-03-10 08:33:14.800322: Current learning rate: 0.00115 
2025-03-10 08:33:21.382136: train_loss -0.9436 
2025-03-10 08:33:21.388228: val_loss -0.8323 
2025-03-10 08:33:21.391399: Pseudo dice [np.float32(0.8997), np.float32(0.88)] 
2025-03-10 08:33:21.394929: Epoch time: 6.59 s 
2025-03-10 08:33:21.933346:  
2025-03-10 08:33:21.938484: Epoch 92 
2025-03-10 08:33:21.942011: Current learning rate: 0.00103 
2025-03-10 08:33:28.512892: train_loss -0.944 
2025-03-10 08:33:28.518490: val_loss -0.8291 
2025-03-10 08:33:28.523061: Pseudo dice [np.float32(0.8972), np.float32(0.8787)] 
2025-03-10 08:33:28.526614: Epoch time: 6.58 s 
2025-03-10 08:33:29.058474:  
2025-03-10 08:33:29.063622: Epoch 93 
2025-03-10 08:33:29.067140: Current learning rate: 0.00091 
2025-03-10 08:33:35.617871: train_loss -0.947 
2025-03-10 08:33:35.623992: val_loss -0.8245 
2025-03-10 08:33:35.627025: Pseudo dice [np.float32(0.8953), np.float32(0.8751)] 
2025-03-10 08:33:35.630908: Epoch time: 6.56 s 
2025-03-10 08:33:36.169225:  
2025-03-10 08:33:36.174747: Epoch 94 
2025-03-10 08:33:36.178255: Current learning rate: 0.00079 
2025-03-10 08:33:42.759665: train_loss -0.9456 
2025-03-10 08:33:42.766719: val_loss -0.825 
2025-03-10 08:33:42.769230: Pseudo dice [np.float32(0.895), np.float32(0.8747)] 
2025-03-10 08:33:42.774945: Epoch time: 6.59 s 
2025-03-10 08:33:43.308200:  
2025-03-10 08:33:43.313722: Epoch 95 
2025-03-10 08:33:43.317239: Current learning rate: 0.00067 
2025-03-10 08:33:49.881010: train_loss -0.9456 
2025-03-10 08:33:49.887090: val_loss -0.8265 
2025-03-10 08:33:49.890186: Pseudo dice [np.float32(0.8952), np.float32(0.8768)] 
2025-03-10 08:33:49.893822: Epoch time: 6.57 s 
2025-03-10 08:33:50.427504:  
2025-03-10 08:33:50.433609: Epoch 96 
2025-03-10 08:33:50.436666: Current learning rate: 0.00055 
2025-03-10 08:33:57.016205: train_loss -0.9471 
2025-03-10 08:33:57.021223: val_loss -0.8231 
2025-03-10 08:33:57.025280: Pseudo dice [np.float32(0.8942), np.float32(0.8759)] 
2025-03-10 08:33:57.027856: Epoch time: 6.59 s 
2025-03-10 08:33:57.581236:  
2025-03-10 08:33:57.587284: Epoch 97 
2025-03-10 08:33:57.590944: Current learning rate: 0.00043 
2025-03-10 08:34:04.173813: train_loss -0.9469 
2025-03-10 08:34:04.179852: val_loss -0.8287 
2025-03-10 08:34:04.182999: Pseudo dice [np.float32(0.8978), np.float32(0.8772)] 
2025-03-10 08:34:04.187008: Epoch time: 6.59 s 
2025-03-10 08:34:04.885063:  
2025-03-10 08:34:04.890605: Epoch 98 
2025-03-10 08:34:04.894139: Current learning rate: 0.0003 
2025-03-10 08:34:11.465303: train_loss -0.9474 
2025-03-10 08:34:11.471906: val_loss -0.819 
2025-03-10 08:34:11.475516: Pseudo dice [np.float32(0.893), np.float32(0.8723)] 
2025-03-10 08:34:11.478099: Epoch time: 6.58 s 
2025-03-10 08:34:12.031512:  
2025-03-10 08:34:12.037109: Epoch 99 
2025-03-10 08:34:12.041182: Current learning rate: 0.00016 
2025-03-10 08:34:18.606226: train_loss -0.9461 
2025-03-10 08:34:18.612402: val_loss -0.8283 
2025-03-10 08:34:18.615920: Pseudo dice [np.float32(0.8966), np.float32(0.8777)] 
2025-03-10 08:34:18.619001: Epoch time: 6.57 s 
2025-03-10 08:34:19.210212: Training done. 
2025-03-10 08:34:19.261725: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-10 08:34:19.268725: The split file contains 5 splits. 
2025-03-10 08:34:19.272727: Desired fold for training: 0 
2025-03-10 08:34:19.278725: This split has 208 training and 52 validation cases. 
2025-03-10 08:34:19.284725: predicting hippocampus_017 
2025-03-10 08:34:19.289726: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-10 08:34:19.382234: predicting hippocampus_019 
2025-03-10 08:34:19.389235: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-10 08:34:19.424751: predicting hippocampus_033 
2025-03-10 08:34:19.431751: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-10 08:34:19.453752: predicting hippocampus_035 
2025-03-10 08:34:19.460752: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-10 08:34:19.482749: predicting hippocampus_037 
2025-03-10 08:34:19.489751: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-10 08:34:19.513754: predicting hippocampus_049 
2025-03-10 08:34:19.521259: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-10 08:34:19.545259: predicting hippocampus_052 
2025-03-10 08:34:19.551259: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-10 08:34:19.576259: predicting hippocampus_065 
2025-03-10 08:34:19.583259: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-10 08:34:19.607262: predicting hippocampus_083 
2025-03-10 08:34:19.615262: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-10 08:34:19.638767: predicting hippocampus_088 
2025-03-10 08:34:19.645768: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-10 08:34:23.200005: predicting hippocampus_090 
2025-03-10 08:34:23.212009: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-10 08:34:23.262516: predicting hippocampus_092 
2025-03-10 08:34:23.270515: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-10 08:34:23.345028: predicting hippocampus_095 
2025-03-10 08:34:23.371027: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-10 08:34:23.434534: predicting hippocampus_107 
2025-03-10 08:34:23.441535: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-10 08:34:23.487534: predicting hippocampus_108 
2025-03-10 08:34:23.494534: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-10 08:34:23.528041: predicting hippocampus_123 
2025-03-10 08:34:23.533041: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-10 08:34:23.568042: predicting hippocampus_125 
2025-03-10 08:34:23.576043: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-10 08:34:23.634550: predicting hippocampus_157 
2025-03-10 08:34:23.641551: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-10 08:34:23.677550: predicting hippocampus_164 
2025-03-10 08:34:23.688550: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-10 08:34:23.772720: predicting hippocampus_169 
2025-03-10 08:34:23.779719: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-10 08:34:23.809722: predicting hippocampus_175 
2025-03-10 08:34:23.816722: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-10 08:34:23.851229: predicting hippocampus_185 
2025-03-10 08:34:23.858231: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-10 08:34:23.886229: predicting hippocampus_190 
2025-03-10 08:34:23.892230: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-10 08:34:23.918232: predicting hippocampus_194 
2025-03-10 08:34:23.923742: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-10 08:34:23.952740: predicting hippocampus_204 
2025-03-10 08:34:23.958742: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-10 08:34:23.985740: predicting hippocampus_205 
2025-03-10 08:34:23.991741: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-10 08:34:24.018743: predicting hippocampus_210 
2025-03-10 08:34:24.024251: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-10 08:34:24.061251: predicting hippocampus_217 
2025-03-10 08:34:24.066253: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-10 08:34:24.104252: predicting hippocampus_219 
2025-03-10 08:34:24.109254: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-10 08:34:24.141760: predicting hippocampus_229 
2025-03-10 08:34:24.149760: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-10 08:34:24.180760: predicting hippocampus_244 
2025-03-10 08:34:24.185761: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-10 08:34:24.213763: predicting hippocampus_261 
2025-03-10 08:34:24.221763: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-10 08:34:24.274269: predicting hippocampus_264 
2025-03-10 08:34:24.281271: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-10 08:34:24.315273: predicting hippocampus_277 
2025-03-10 08:34:24.324784: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-10 08:34:24.369782: predicting hippocampus_280 
2025-03-10 08:34:24.376783: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-10 08:34:24.406782: predicting hippocampus_286 
2025-03-10 08:34:24.414786: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-10 08:34:24.459294: predicting hippocampus_288 
2025-03-10 08:34:24.467293: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-10 08:34:24.514298: predicting hippocampus_289 
2025-03-10 08:34:24.520296: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-10 08:34:24.548802: predicting hippocampus_296 
2025-03-10 08:34:24.553803: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-10 08:34:24.581802: predicting hippocampus_305 
2025-03-10 08:34:24.586804: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-10 08:34:24.614805: predicting hippocampus_308 
2025-03-10 08:34:24.622307: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-10 08:34:24.661310: predicting hippocampus_317 
2025-03-10 08:34:24.668309: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-10 08:34:24.694310: predicting hippocampus_327 
2025-03-10 08:34:24.700310: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-10 08:34:24.726820: predicting hippocampus_330 
2025-03-10 08:34:24.731820: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-10 08:34:24.768820: predicting hippocampus_332 
2025-03-10 08:34:24.774819: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-10 08:34:24.799819: predicting hippocampus_338 
2025-03-10 08:34:24.806821: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-10 08:34:24.850332: predicting hippocampus_349 
2025-03-10 08:34:24.855332: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-10 08:34:24.888331: predicting hippocampus_350 
2025-03-10 08:34:24.894333: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-10 08:34:24.922841: predicting hippocampus_356 
2025-03-10 08:34:24.927843: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-10 08:34:24.955841: predicting hippocampus_358 
2025-03-10 08:34:24.961842: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-10 08:34:24.995841: predicting hippocampus_374 
2025-03-10 08:34:25.000841: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-10 08:34:25.029352: predicting hippocampus_394 
2025-03-10 08:34:25.034353: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-10 08:34:28.624464: Validation complete 
2025-03-10 08:34:28.629465: Mean Validation Dice:  0.3970550142035511 
