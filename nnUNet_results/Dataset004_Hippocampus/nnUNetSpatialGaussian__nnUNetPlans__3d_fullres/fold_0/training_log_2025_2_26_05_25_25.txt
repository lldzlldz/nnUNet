
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-02-26 05:25:25.959001: do_dummy_2d_data_aug: False 
2025-02-26 05:25:25.984247: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-02-26 05:25:25.992756: The split file contains 5 splits. 
2025-02-26 05:25:25.996756: Desired fold for training: 0 
2025-02-26 05:25:25.999756: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-02-26 05:25:32.405298: unpacking dataset... 
2025-02-26 05:25:32.752585: unpacking done... 
2025-02-26 05:25:33.933422:  
2025-02-26 05:25:33.938436: Epoch 0 
2025-02-26 05:25:33.941946: Current learning rate: 0.01 
2025-02-26 05:25:41.397050: train_loss -0.5349 
2025-02-26 05:25:41.403282: val_loss -0.8163 
2025-02-26 05:25:41.406513: Pseudo dice [np.float32(0.8701), np.float32(0.8578)] 
2025-02-26 05:25:41.409010: Epoch time: 7.46 s 
2025-02-26 05:25:41.413068: Yayy! New best EMA pseudo Dice: 0.8639000058174133 
2025-02-26 05:25:41.889166:  
2025-02-26 05:25:41.894680: Epoch 1 
2025-02-26 05:25:41.898194: Current learning rate: 0.00991 
2025-02-26 05:25:48.433418: train_loss -0.8262 
2025-02-26 05:25:48.438514: val_loss -0.8344 
2025-02-26 05:25:48.442581: Pseudo dice [np.float32(0.8849), np.float32(0.8697)] 
2025-02-26 05:25:48.445697: Epoch time: 6.54 s 
2025-02-26 05:25:48.449811: Yayy! New best EMA pseudo Dice: 0.8652999997138977 
2025-02-26 05:25:48.968469:  
2025-02-26 05:25:48.973498: Epoch 2 
2025-02-26 05:25:48.977183: Current learning rate: 0.00982 
2025-02-26 05:25:55.542365: train_loss -0.8462 
2025-02-26 05:25:55.548019: val_loss -0.8454 
2025-02-26 05:25:55.551603: Pseudo dice [np.float32(0.8932), np.float32(0.8771)] 
2025-02-26 05:25:55.555157: Epoch time: 6.57 s 
2025-02-26 05:25:55.558250: Yayy! New best EMA pseudo Dice: 0.8672000169754028 
2025-02-26 05:25:56.096466:  
2025-02-26 05:25:56.101480: Epoch 3 
2025-02-26 05:25:56.104490: Current learning rate: 0.00973 
2025-02-26 05:26:02.656171: train_loss -0.8564 
2025-02-26 05:26:02.662271: val_loss -0.8411 
2025-02-26 05:26:02.665795: Pseudo dice [np.float32(0.8903), np.float32(0.8738)] 
2025-02-26 05:26:02.668812: Epoch time: 6.56 s 
2025-02-26 05:26:02.671851: Yayy! New best EMA pseudo Dice: 0.8687000274658203 
2025-02-26 05:26:03.197453:  
2025-02-26 05:26:03.203071: Epoch 4 
2025-02-26 05:26:03.206103: Current learning rate: 0.00964 
2025-02-26 05:26:09.715368: train_loss -0.8662 
2025-02-26 05:26:09.721069: val_loss -0.85 
2025-02-26 05:26:09.724676: Pseudo dice [np.float32(0.8971), np.float32(0.8809)] 
2025-02-26 05:26:09.727738: Epoch time: 6.52 s 
2025-02-26 05:26:09.731337: Yayy! New best EMA pseudo Dice: 0.8708000183105469 
2025-02-26 05:26:10.387571:  
2025-02-26 05:26:10.392621: Epoch 5 
2025-02-26 05:26:10.396166: Current learning rate: 0.00955 
2025-02-26 05:26:16.929072: train_loss -0.8716 
2025-02-26 05:26:16.934235: val_loss -0.8459 
2025-02-26 05:26:16.937386: Pseudo dice [np.float32(0.8954), np.float32(0.8776)] 
2025-02-26 05:26:16.941421: Epoch time: 6.54 s 
2025-02-26 05:26:16.945073: Yayy! New best EMA pseudo Dice: 0.8723000288009644 
2025-02-26 05:26:17.460291:  
2025-02-26 05:26:17.466406: Epoch 6 
2025-02-26 05:26:17.468910: Current learning rate: 0.00946 
2025-02-26 05:26:23.961689: train_loss -0.8743 
2025-02-26 05:26:23.967707: val_loss -0.8493 
2025-02-26 05:26:23.973762: Pseudo dice [np.float32(0.8974), np.float32(0.8804)] 
2025-02-26 05:26:23.977865: Epoch time: 6.5 s 
2025-02-26 05:26:23.981567: Yayy! New best EMA pseudo Dice: 0.8740000128746033 
2025-02-26 05:26:24.506522:  
2025-02-26 05:26:24.512039: Epoch 7 
2025-02-26 05:26:24.515549: Current learning rate: 0.00937 
2025-02-26 05:26:31.032410: train_loss -0.8798 
2025-02-26 05:26:31.037926: val_loss -0.8512 
2025-02-26 05:26:31.042486: Pseudo dice [np.float32(0.8992), np.float32(0.8809)] 
2025-02-26 05:26:31.045531: Epoch time: 6.53 s 
2025-02-26 05:26:31.049063: Yayy! New best EMA pseudo Dice: 0.8755999803543091 
2025-02-26 05:26:31.579878:  
2025-02-26 05:26:31.586393: Epoch 8 
2025-02-26 05:26:31.588901: Current learning rate: 0.00928 
2025-02-26 05:26:38.118446: train_loss -0.8828 
2025-02-26 05:26:38.124495: val_loss -0.8383 
2025-02-26 05:26:38.127511: Pseudo dice [np.float32(0.8935), np.float32(0.8722)] 
2025-02-26 05:26:38.131065: Epoch time: 6.54 s 
2025-02-26 05:26:38.134120: Yayy! New best EMA pseudo Dice: 0.8762999773025513 
2025-02-26 05:26:38.679477:  
2025-02-26 05:26:38.685003: Epoch 9 
2025-02-26 05:26:38.688555: Current learning rate: 0.00919 
2025-02-26 05:26:45.233081: train_loss -0.8868 
2025-02-26 05:26:45.238135: val_loss -0.8488 
2025-02-26 05:26:45.241661: Pseudo dice [np.float32(0.898), np.float32(0.8797)] 
2025-02-26 05:26:45.245253: Epoch time: 6.55 s 
2025-02-26 05:26:45.248933: Yayy! New best EMA pseudo Dice: 0.8776000142097473 
2025-02-26 05:26:45.766523:  
2025-02-26 05:26:45.772092: Epoch 10 
2025-02-26 05:26:45.775616: Current learning rate: 0.0091 
2025-02-26 05:26:52.265273: train_loss -0.8895 
2025-02-26 05:26:52.270986: val_loss -0.8425 
2025-02-26 05:26:52.274513: Pseudo dice [np.float32(0.893), np.float32(0.8772)] 
2025-02-26 05:26:52.277565: Epoch time: 6.5 s 
2025-02-26 05:26:52.281121: Yayy! New best EMA pseudo Dice: 0.8783000111579895 
2025-02-26 05:26:52.801065:  
2025-02-26 05:26:52.807103: Epoch 11 
2025-02-26 05:26:52.810615: Current learning rate: 0.009 
2025-02-26 05:26:59.289920: train_loss -0.8918 
2025-02-26 05:26:59.295770: val_loss -0.851 
2025-02-26 05:26:59.299326: Pseudo dice [np.float32(0.8999), np.float32(0.8809)] 
2025-02-26 05:26:59.302383: Epoch time: 6.49 s 
2025-02-26 05:26:59.305470: Yayy! New best EMA pseudo Dice: 0.8794999718666077 
2025-02-26 05:26:59.826130:  
2025-02-26 05:26:59.831698: Epoch 12 
2025-02-26 05:26:59.835775: Current learning rate: 0.00891 
2025-02-26 05:27:06.369073: train_loss -0.8957 
2025-02-26 05:27:06.375621: val_loss -0.8455 
2025-02-26 05:27:06.378656: Pseudo dice [np.float32(0.8969), np.float32(0.8791)] 
2025-02-26 05:27:06.381679: Epoch time: 6.54 s 
2025-02-26 05:27:06.385691: Yayy! New best EMA pseudo Dice: 0.8804000020027161 
2025-02-26 05:27:07.054134:  
2025-02-26 05:27:07.060190: Epoch 13 
2025-02-26 05:27:07.062600: Current learning rate: 0.00882 
2025-02-26 05:27:13.585716: train_loss -0.8992 
2025-02-26 05:27:13.591852: val_loss -0.8511 
2025-02-26 05:27:13.595417: Pseudo dice [np.float32(0.8992), np.float32(0.8824)] 
2025-02-26 05:27:13.598471: Epoch time: 6.53 s 
2025-02-26 05:27:13.602034: Yayy! New best EMA pseudo Dice: 0.8813999891281128 
2025-02-26 05:27:14.132939:  
2025-02-26 05:27:14.137951: Epoch 14 
2025-02-26 05:27:14.141464: Current learning rate: 0.00873 
2025-02-26 05:27:20.651955: train_loss -0.9011 
2025-02-26 05:27:20.658061: val_loss -0.8441 
2025-02-26 05:27:20.661595: Pseudo dice [np.float32(0.8957), np.float32(0.8775)] 
2025-02-26 05:27:20.664710: Epoch time: 6.52 s 
2025-02-26 05:27:20.668309: Yayy! New best EMA pseudo Dice: 0.8819000124931335 
2025-02-26 05:27:21.209054:  
2025-02-26 05:27:21.214623: Epoch 15 
2025-02-26 05:27:21.217642: Current learning rate: 0.00864 
2025-02-26 05:27:27.738942: train_loss -0.9028 
2025-02-26 05:27:27.745096: val_loss -0.8449 
2025-02-26 05:27:27.749126: Pseudo dice [np.float32(0.896), np.float32(0.8805)] 
2025-02-26 05:27:27.752241: Epoch time: 6.53 s 
2025-02-26 05:27:27.757834: Yayy! New best EMA pseudo Dice: 0.8826000094413757 
2025-02-26 05:27:28.296884:  
2025-02-26 05:27:28.302398: Epoch 16 
2025-02-26 05:27:28.305912: Current learning rate: 0.00855 
2025-02-26 05:27:34.824691: train_loss -0.9056 
2025-02-26 05:27:34.830345: val_loss -0.8447 
2025-02-26 05:27:34.833579: Pseudo dice [np.float32(0.8968), np.float32(0.8789)] 
2025-02-26 05:27:34.836092: Epoch time: 6.53 s 
2025-02-26 05:27:34.840349: Yayy! New best EMA pseudo Dice: 0.8830999732017517 
2025-02-26 05:27:35.383254:  
2025-02-26 05:27:35.388825: Epoch 17 
2025-02-26 05:27:35.392862: Current learning rate: 0.00846 
2025-02-26 05:27:41.896863: train_loss -0.9053 
2025-02-26 05:27:41.902467: val_loss -0.8469 
2025-02-26 05:27:41.906013: Pseudo dice [np.float32(0.899), np.float32(0.8797)] 
2025-02-26 05:27:41.909042: Epoch time: 6.51 s 
2025-02-26 05:27:41.912578: Yayy! New best EMA pseudo Dice: 0.8837000131607056 
2025-02-26 05:27:42.453473:  
2025-02-26 05:27:42.458203: Epoch 18 
2025-02-26 05:27:42.462306: Current learning rate: 0.00836 
2025-02-26 05:27:48.961443: train_loss -0.9077 
2025-02-26 05:27:48.968295: val_loss -0.8441 
2025-02-26 05:27:48.971841: Pseudo dice [np.float32(0.8975), np.float32(0.8799)] 
2025-02-26 05:27:48.975027: Epoch time: 6.51 s 
2025-02-26 05:27:48.979059: Yayy! New best EMA pseudo Dice: 0.8841999769210815 
2025-02-26 05:27:49.517895:  
2025-02-26 05:27:49.522917: Epoch 19 
2025-02-26 05:27:49.526496: Current learning rate: 0.00827 
2025-02-26 05:27:56.057720: train_loss -0.9078 
2025-02-26 05:27:56.064240: val_loss -0.8438 
2025-02-26 05:27:56.067287: Pseudo dice [np.float32(0.897), np.float32(0.8792)] 
2025-02-26 05:27:56.070315: Epoch time: 6.54 s 
2025-02-26 05:27:56.073826: Yayy! New best EMA pseudo Dice: 0.8845999836921692 
2025-02-26 05:27:56.749328:  
2025-02-26 05:27:56.754884: Epoch 20 
2025-02-26 05:27:56.757420: Current learning rate: 0.00818 
2025-02-26 05:28:03.288056: train_loss -0.9096 
2025-02-26 05:28:03.294122: val_loss -0.8408 
2025-02-26 05:28:03.297660: Pseudo dice [np.float32(0.8961), np.float32(0.8776)] 
2025-02-26 05:28:03.300230: Epoch time: 6.54 s 
2025-02-26 05:28:03.304804: Yayy! New best EMA pseudo Dice: 0.8848000168800354 
2025-02-26 05:28:03.861240:  
2025-02-26 05:28:03.866777: Epoch 21 
2025-02-26 05:28:03.870286: Current learning rate: 0.00809 
2025-02-26 05:28:10.359988: train_loss -0.91 
2025-02-26 05:28:10.366052: val_loss -0.8409 
2025-02-26 05:28:10.369101: Pseudo dice [np.float32(0.8961), np.float32(0.8771)] 
2025-02-26 05:28:10.373143: Epoch time: 6.5 s 
2025-02-26 05:28:10.376270: Yayy! New best EMA pseudo Dice: 0.8849999904632568 
2025-02-26 05:28:10.896041:  
2025-02-26 05:28:10.900054: Epoch 22 
2025-02-26 05:28:10.904063: Current learning rate: 0.008 
2025-02-26 05:28:17.391274: train_loss -0.9121 
2025-02-26 05:28:17.397288: val_loss -0.8424 
2025-02-26 05:28:17.400145: Pseudo dice [np.float32(0.8959), np.float32(0.8792)] 
2025-02-26 05:28:17.403348: Epoch time: 6.5 s 
2025-02-26 05:28:17.407919: Yayy! New best EMA pseudo Dice: 0.8852999806404114 
2025-02-26 05:28:17.928255:  
2025-02-26 05:28:17.933905: Epoch 23 
2025-02-26 05:28:17.937495: Current learning rate: 0.0079 
2025-02-26 05:28:24.463486: train_loss -0.9128 
2025-02-26 05:28:24.469176: val_loss -0.8405 
2025-02-26 05:28:24.472735: Pseudo dice [np.float32(0.8953), np.float32(0.8784)] 
2025-02-26 05:28:24.475344: Epoch time: 6.54 s 
2025-02-26 05:28:24.479079: Yayy! New best EMA pseudo Dice: 0.8853999972343445 
2025-02-26 05:28:24.994370:  
2025-02-26 05:28:24.998400: Epoch 24 
2025-02-26 05:28:25.002438: Current learning rate: 0.00781 
2025-02-26 05:28:31.513263: train_loss -0.9145 
2025-02-26 05:28:31.518902: val_loss -0.844 
2025-02-26 05:28:31.522450: Pseudo dice [np.float32(0.8969), np.float32(0.8794)] 
2025-02-26 05:28:31.526099: Epoch time: 6.52 s 
2025-02-26 05:28:31.529141: Yayy! New best EMA pseudo Dice: 0.885699987411499 
2025-02-26 05:28:32.067050:  
2025-02-26 05:28:32.071326: Epoch 25 
2025-02-26 05:28:32.075337: Current learning rate: 0.00772 
2025-02-26 05:28:38.607747: train_loss -0.9152 
2025-02-26 05:28:38.615064: val_loss -0.8434 
2025-02-26 05:28:38.618652: Pseudo dice [np.float32(0.8984), np.float32(0.8792)] 
2025-02-26 05:28:38.621671: Epoch time: 6.54 s 
2025-02-26 05:28:38.624917: Yayy! New best EMA pseudo Dice: 0.8859999775886536 
2025-02-26 05:28:39.144148:  
2025-02-26 05:28:39.148787: Epoch 26 
2025-02-26 05:28:39.151323: Current learning rate: 0.00763 
2025-02-26 05:28:45.689478: train_loss -0.9162 
2025-02-26 05:28:45.695057: val_loss -0.8443 
2025-02-26 05:28:45.698612: Pseudo dice [np.float32(0.896), np.float32(0.8814)] 
2025-02-26 05:28:45.701638: Epoch time: 6.55 s 
2025-02-26 05:28:45.705312: Yayy! New best EMA pseudo Dice: 0.8863000273704529 
2025-02-26 05:28:46.229494:  
2025-02-26 05:28:46.235088: Epoch 27 
2025-02-26 05:28:46.238159: Current learning rate: 0.00753 
2025-02-26 05:28:52.730422: train_loss -0.9188 
2025-02-26 05:28:52.735969: val_loss -0.8364 
2025-02-26 05:28:52.740042: Pseudo dice [np.float32(0.8937), np.float32(0.8757)] 
2025-02-26 05:28:52.742881: Epoch time: 6.5 s 
2025-02-26 05:28:53.374230:  
2025-02-26 05:28:53.380258: Epoch 28 
2025-02-26 05:28:53.383771: Current learning rate: 0.00744 
2025-02-26 05:28:59.866643: train_loss -0.9198 
2025-02-26 05:28:59.872211: val_loss -0.8354 
2025-02-26 05:28:59.875759: Pseudo dice [np.float32(0.8944), np.float32(0.8755)] 
2025-02-26 05:28:59.878874: Epoch time: 6.49 s 
2025-02-26 05:29:00.370235:  
2025-02-26 05:29:00.375750: Epoch 29 
2025-02-26 05:29:00.379261: Current learning rate: 0.00735 
2025-02-26 05:29:06.896297: train_loss -0.9182 
2025-02-26 05:29:06.902311: val_loss -0.8479 
2025-02-26 05:29:06.905351: Pseudo dice [np.float32(0.9004), np.float32(0.883)] 
2025-02-26 05:29:06.908894: Epoch time: 6.53 s 
2025-02-26 05:29:06.912429: Yayy! New best EMA pseudo Dice: 0.8866000175476074 
2025-02-26 05:29:07.441281:  
2025-02-26 05:29:07.446860: Epoch 30 
2025-02-26 05:29:07.450897: Current learning rate: 0.00725 
2025-02-26 05:29:13.996894: train_loss -0.9197 
2025-02-26 05:29:14.002495: val_loss -0.84 
2025-02-26 05:29:14.006031: Pseudo dice [np.float32(0.8962), np.float32(0.8786)] 
2025-02-26 05:29:14.009576: Epoch time: 6.56 s 
2025-02-26 05:29:14.013193: Yayy! New best EMA pseudo Dice: 0.8866999745368958 
2025-02-26 05:29:14.542395:  
2025-02-26 05:29:14.547931: Epoch 31 
2025-02-26 05:29:14.551462: Current learning rate: 0.00716 
2025-02-26 05:29:21.068432: train_loss -0.9204 
2025-02-26 05:29:21.075451: val_loss -0.8424 
2025-02-26 05:29:21.078979: Pseudo dice [np.float32(0.8973), np.float32(0.8805)] 
2025-02-26 05:29:21.081484: Epoch time: 6.53 s 
2025-02-26 05:29:21.085496: Yayy! New best EMA pseudo Dice: 0.886900007724762 
2025-02-26 05:29:21.619586:  
2025-02-26 05:29:21.624643: Epoch 32 
2025-02-26 05:29:21.627706: Current learning rate: 0.00707 
2025-02-26 05:29:28.149012: train_loss -0.9225 
2025-02-26 05:29:28.154767: val_loss -0.8433 
2025-02-26 05:29:28.158354: Pseudo dice [np.float32(0.8989), np.float32(0.881)] 
2025-02-26 05:29:28.161377: Epoch time: 6.53 s 
2025-02-26 05:29:28.164947: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-02-26 05:29:28.691852:  
2025-02-26 05:29:28.698546: Epoch 33 
2025-02-26 05:29:28.702090: Current learning rate: 0.00697 
2025-02-26 05:29:35.213877: train_loss -0.9219 
2025-02-26 05:29:35.219404: val_loss -0.8411 
2025-02-26 05:29:35.223493: Pseudo dice [np.float32(0.8977), np.float32(0.8796)] 
2025-02-26 05:29:35.227058: Epoch time: 6.52 s 
2025-02-26 05:29:35.229600: Yayy! New best EMA pseudo Dice: 0.8873000144958496 
2025-02-26 05:29:35.764130:  
2025-02-26 05:29:35.769753: Epoch 34 
2025-02-26 05:29:35.773312: Current learning rate: 0.00688 
2025-02-26 05:29:42.283771: train_loss -0.9233 
2025-02-26 05:29:42.289306: val_loss -0.8383 
2025-02-26 05:29:42.292860: Pseudo dice [np.float32(0.8957), np.float32(0.8774)] 
2025-02-26 05:29:42.296002: Epoch time: 6.52 s 
2025-02-26 05:29:42.802895:  
2025-02-26 05:29:42.808410: Epoch 35 
2025-02-26 05:29:42.810916: Current learning rate: 0.00679 
2025-02-26 05:29:49.299438: train_loss -0.9229 
2025-02-26 05:29:49.305596: val_loss -0.8438 
2025-02-26 05:29:49.308667: Pseudo dice [np.float32(0.9002), np.float32(0.8814)] 
2025-02-26 05:29:49.312697: Epoch time: 6.5 s 
2025-02-26 05:29:49.315749: Yayy! New best EMA pseudo Dice: 0.8876000046730042 
2025-02-26 05:29:49.998650:  
2025-02-26 05:29:50.004766: Epoch 36 
2025-02-26 05:29:50.008279: Current learning rate: 0.00669 
2025-02-26 05:29:56.525334: train_loss -0.9263 
2025-02-26 05:29:56.530934: val_loss -0.8468 
2025-02-26 05:29:56.534472: Pseudo dice [np.float32(0.902), np.float32(0.8849)] 
2025-02-26 05:29:56.538116: Epoch time: 6.53 s 
2025-02-26 05:29:56.540679: Yayy! New best EMA pseudo Dice: 0.8881999850273132 
2025-02-26 05:29:57.080132:  
2025-02-26 05:29:57.085145: Epoch 37 
2025-02-26 05:29:57.088654: Current learning rate: 0.0066 
2025-02-26 05:30:03.629738: train_loss -0.9259 
2025-02-26 05:30:03.635818: val_loss -0.8355 
2025-02-26 05:30:03.638861: Pseudo dice [np.float32(0.8942), np.float32(0.8766)] 
2025-02-26 05:30:03.641903: Epoch time: 6.55 s 
2025-02-26 05:30:04.150437:  
2025-02-26 05:30:04.155951: Epoch 38 
2025-02-26 05:30:04.158458: Current learning rate: 0.0065 
2025-02-26 05:30:10.647818: train_loss -0.9257 
2025-02-26 05:30:10.653360: val_loss -0.84 
2025-02-26 05:30:10.656920: Pseudo dice [np.float32(0.8977), np.float32(0.8806)] 
2025-02-26 05:30:10.659513: Epoch time: 6.5 s 
2025-02-26 05:30:11.166169:  
2025-02-26 05:30:11.171720: Epoch 39 
2025-02-26 05:30:11.175255: Current learning rate: 0.00641 
2025-02-26 05:30:17.671474: train_loss -0.9276 
2025-02-26 05:30:17.678216: val_loss -0.8386 
2025-02-26 05:30:17.681765: Pseudo dice [np.float32(0.8976), np.float32(0.8799)] 
2025-02-26 05:30:17.684810: Epoch time: 6.51 s 
2025-02-26 05:30:18.196528:  
2025-02-26 05:30:18.201542: Epoch 40 
2025-02-26 05:30:18.205050: Current learning rate: 0.00631 
2025-02-26 05:30:24.727500: train_loss -0.928 
2025-02-26 05:30:24.733517: val_loss -0.8394 
2025-02-26 05:30:24.737034: Pseudo dice [np.float32(0.898), np.float32(0.8806)] 
2025-02-26 05:30:24.740596: Epoch time: 6.53 s 
2025-02-26 05:30:24.743634: Yayy! New best EMA pseudo Dice: 0.8881999850273132 
2025-02-26 05:30:25.290275:  
2025-02-26 05:30:25.295305: Epoch 41 
2025-02-26 05:30:25.297846: Current learning rate: 0.00622 
2025-02-26 05:30:31.798755: train_loss -0.9287 
2025-02-26 05:30:31.804446: val_loss -0.8416 
2025-02-26 05:30:31.807481: Pseudo dice [np.float32(0.8986), np.float32(0.8808)] 
2025-02-26 05:30:31.811011: Epoch time: 6.51 s 
2025-02-26 05:30:31.814639: Yayy! New best EMA pseudo Dice: 0.8884000182151794 
2025-02-26 05:30:32.340049:  
2025-02-26 05:30:32.345062: Epoch 42 
2025-02-26 05:30:32.348574: Current learning rate: 0.00612 
2025-02-26 05:30:38.845657: train_loss -0.9277 
2025-02-26 05:30:38.851877: val_loss -0.8348 
2025-02-26 05:30:38.856941: Pseudo dice [np.float32(0.8951), np.float32(0.877)] 
2025-02-26 05:30:38.859969: Epoch time: 6.51 s 
2025-02-26 05:30:39.488688:  
2025-02-26 05:30:39.494230: Epoch 43 
2025-02-26 05:30:39.498254: Current learning rate: 0.00603 
2025-02-26 05:30:46.003490: train_loss -0.9298 
2025-02-26 05:30:46.009014: val_loss -0.8388 
2025-02-26 05:30:46.012927: Pseudo dice [np.float32(0.8981), np.float32(0.8792)] 
2025-02-26 05:30:46.017064: Epoch time: 6.51 s 
2025-02-26 05:30:46.501691:  
2025-02-26 05:30:46.507248: Epoch 44 
2025-02-26 05:30:46.509780: Current learning rate: 0.00593 
2025-02-26 05:30:53.004644: train_loss -0.9301 
2025-02-26 05:30:53.010282: val_loss -0.8302 
2025-02-26 05:30:53.012834: Pseudo dice [np.float32(0.8941), np.float32(0.8743)] 
2025-02-26 05:30:53.016861: Epoch time: 6.5 s 
2025-02-26 05:30:53.503548:  
2025-02-26 05:30:53.509110: Epoch 45 
2025-02-26 05:30:53.512148: Current learning rate: 0.00584 
2025-02-26 05:30:59.993473: train_loss -0.9306 
2025-02-26 05:30:59.999136: val_loss -0.84 
2025-02-26 05:31:00.003682: Pseudo dice [np.float32(0.8982), np.float32(0.8819)] 
2025-02-26 05:31:00.007441: Epoch time: 6.49 s 
2025-02-26 05:31:00.496669:  
2025-02-26 05:31:00.501682: Epoch 46 
2025-02-26 05:31:00.504690: Current learning rate: 0.00574 
2025-02-26 05:31:07.022390: train_loss -0.9311 
2025-02-26 05:31:07.028018: val_loss -0.8336 
2025-02-26 05:31:07.031615: Pseudo dice [np.float32(0.8966), np.float32(0.8773)] 
2025-02-26 05:31:07.034668: Epoch time: 6.53 s 
2025-02-26 05:31:07.518275:  
2025-02-26 05:31:07.523822: Epoch 47 
2025-02-26 05:31:07.526363: Current learning rate: 0.00565 
2025-02-26 05:31:14.067341: train_loss -0.9319 
2025-02-26 05:31:14.072372: val_loss -0.8337 
2025-02-26 05:31:14.076896: Pseudo dice [np.float32(0.8956), np.float32(0.8765)] 
2025-02-26 05:31:14.079461: Epoch time: 6.55 s 
2025-02-26 05:31:14.574201:  
2025-02-26 05:31:14.579715: Epoch 48 
2025-02-26 05:31:14.583225: Current learning rate: 0.00555 
2025-02-26 05:31:21.084088: train_loss -0.9328 
2025-02-26 05:31:21.089742: val_loss -0.8356 
2025-02-26 05:31:21.093297: Pseudo dice [np.float32(0.8964), np.float32(0.8784)] 
2025-02-26 05:31:21.096340: Epoch time: 6.51 s 
2025-02-26 05:31:21.588752:  
2025-02-26 05:31:21.594803: Epoch 49 
2025-02-26 05:31:21.597913: Current learning rate: 0.00546 
2025-02-26 05:31:28.093690: train_loss -0.9323 
2025-02-26 05:31:28.099306: val_loss -0.8336 
2025-02-26 05:31:28.103068: Pseudo dice [np.float32(0.8946), np.float32(0.8768)] 
2025-02-26 05:31:28.106128: Epoch time: 6.51 s 
2025-02-26 05:31:28.626411:  
2025-02-26 05:31:28.632474: Epoch 50 
2025-02-26 05:31:28.635552: Current learning rate: 0.00536 
2025-02-26 05:31:35.142345: train_loss -0.9324 
2025-02-26 05:31:35.149472: val_loss -0.8337 
2025-02-26 05:31:35.153537: Pseudo dice [np.float32(0.8954), np.float32(0.8765)] 
2025-02-26 05:31:35.156636: Epoch time: 6.52 s 
2025-02-26 05:31:35.792789:  
2025-02-26 05:31:35.798373: Epoch 51 
2025-02-26 05:31:35.801908: Current learning rate: 0.00526 
2025-02-26 05:31:42.312278: train_loss -0.9354 
2025-02-26 05:31:42.317353: val_loss -0.8274 
2025-02-26 05:31:42.321063: Pseudo dice [np.float32(0.8921), np.float32(0.874)] 
2025-02-26 05:31:42.324641: Epoch time: 6.52 s 
2025-02-26 05:31:42.818867:  
2025-02-26 05:31:42.824455: Epoch 52 
2025-02-26 05:31:42.827514: Current learning rate: 0.00517 
2025-02-26 05:31:49.326288: train_loss -0.9342 
2025-02-26 05:31:49.331584: val_loss -0.835 
2025-02-26 05:31:49.333593: Pseudo dice [np.float32(0.8968), np.float32(0.8786)] 
2025-02-26 05:31:49.338171: Epoch time: 6.51 s 
2025-02-26 05:31:49.834538:  
2025-02-26 05:31:49.840056: Epoch 53 
2025-02-26 05:31:49.843566: Current learning rate: 0.00507 
2025-02-26 05:31:56.350022: train_loss -0.9334 
2025-02-26 05:31:56.356044: val_loss -0.8332 
2025-02-26 05:31:56.360064: Pseudo dice [np.float32(0.8958), np.float32(0.8784)] 
2025-02-26 05:31:56.363110: Epoch time: 6.52 s 
2025-02-26 05:31:56.856212:  
2025-02-26 05:31:56.862317: Epoch 54 
2025-02-26 05:31:56.864823: Current learning rate: 0.00497 
2025-02-26 05:32:03.369001: train_loss -0.9353 
2025-02-26 05:32:03.375657: val_loss -0.8325 
2025-02-26 05:32:03.381385: Pseudo dice [np.float32(0.8956), np.float32(0.8774)] 
2025-02-26 05:32:03.386998: Epoch time: 6.51 s 
2025-02-26 05:32:03.889346:  
2025-02-26 05:32:03.894902: Epoch 55 
2025-02-26 05:32:03.897433: Current learning rate: 0.00487 
2025-02-26 05:32:10.401300: train_loss -0.9352 
2025-02-26 05:32:10.406449: val_loss -0.8291 
2025-02-26 05:32:10.410493: Pseudo dice [np.float32(0.8943), np.float32(0.8749)] 
2025-02-26 05:32:10.413023: Epoch time: 6.51 s 
2025-02-26 05:32:10.907958:  
2025-02-26 05:32:10.912969: Epoch 56 
2025-02-26 05:32:10.916479: Current learning rate: 0.00478 
2025-02-26 05:32:17.406789: train_loss -0.9356 
2025-02-26 05:32:17.411896: val_loss -0.8287 
2025-02-26 05:32:17.415939: Pseudo dice [np.float32(0.8942), np.float32(0.8755)] 
2025-02-26 05:32:17.419523: Epoch time: 6.5 s 
2025-02-26 05:32:17.909596:  
2025-02-26 05:32:17.915641: Epoch 57 
2025-02-26 05:32:17.919193: Current learning rate: 0.00468 
2025-02-26 05:32:24.437622: train_loss -0.9355 
2025-02-26 05:32:24.444182: val_loss -0.8361 
2025-02-26 05:32:24.447718: Pseudo dice [np.float32(0.8984), np.float32(0.8809)] 
2025-02-26 05:32:24.450756: Epoch time: 6.53 s 
2025-02-26 05:32:24.946568:  
2025-02-26 05:32:24.952087: Epoch 58 
2025-02-26 05:32:24.954593: Current learning rate: 0.00458 
2025-02-26 05:32:31.477568: train_loss -0.9352 
2025-02-26 05:32:31.483652: val_loss -0.8352 
2025-02-26 05:32:31.487337: Pseudo dice [np.float32(0.8978), np.float32(0.8805)] 
2025-02-26 05:32:31.490892: Epoch time: 6.53 s 
2025-02-26 05:32:32.139122:  
2025-02-26 05:32:32.144822: Epoch 59 
2025-02-26 05:32:32.147899: Current learning rate: 0.00448 
2025-02-26 05:32:38.635214: train_loss -0.9376 
2025-02-26 05:32:38.640774: val_loss -0.8353 
2025-02-26 05:32:38.643809: Pseudo dice [np.float32(0.8975), np.float32(0.8806)] 
2025-02-26 05:32:38.647331: Epoch time: 6.5 s 
2025-02-26 05:32:39.148074:  
2025-02-26 05:32:39.155600: Epoch 60 
2025-02-26 05:32:39.160612: Current learning rate: 0.00438 
2025-02-26 05:32:45.663594: train_loss -0.938 
2025-02-26 05:32:45.669171: val_loss -0.8319 
2025-02-26 05:32:45.672214: Pseudo dice [np.float32(0.8969), np.float32(0.8784)] 
2025-02-26 05:32:45.675254: Epoch time: 6.52 s 
2025-02-26 05:32:46.180915:  
2025-02-26 05:32:46.186431: Epoch 61 
2025-02-26 05:32:46.189939: Current learning rate: 0.00429 
2025-02-26 05:32:52.707054: train_loss -0.9378 
2025-02-26 05:32:52.713037: val_loss -0.8321 
2025-02-26 05:32:52.715071: Pseudo dice [np.float32(0.8968), np.float32(0.8784)] 
2025-02-26 05:32:52.719641: Epoch time: 6.53 s 
2025-02-26 05:32:53.221187:  
2025-02-26 05:32:53.226201: Epoch 62 
2025-02-26 05:32:53.229710: Current learning rate: 0.00419 
2025-02-26 05:32:59.741065: train_loss -0.9382 
2025-02-26 05:32:59.746079: val_loss -0.8328 
2025-02-26 05:32:59.750088: Pseudo dice [np.float32(0.8964), np.float32(0.8787)] 
2025-02-26 05:32:59.752594: Epoch time: 6.52 s 
2025-02-26 05:33:00.251787:  
2025-02-26 05:33:00.258308: Epoch 63 
2025-02-26 05:33:00.261818: Current learning rate: 0.00409 
2025-02-26 05:33:06.781921: train_loss -0.9384 
2025-02-26 05:33:06.787483: val_loss -0.8344 
2025-02-26 05:33:06.791421: Pseudo dice [np.float32(0.8972), np.float32(0.879)] 
2025-02-26 05:33:06.794492: Epoch time: 6.53 s 
2025-02-26 05:33:07.300052:  
2025-02-26 05:33:07.305115: Epoch 64 
2025-02-26 05:33:07.308122: Current learning rate: 0.00399 
2025-02-26 05:33:13.842777: train_loss -0.9395 
2025-02-26 05:33:13.848917: val_loss -0.8354 
2025-02-26 05:33:13.852582: Pseudo dice [np.float32(0.8986), np.float32(0.8788)] 
2025-02-26 05:33:13.855668: Epoch time: 6.54 s 
2025-02-26 05:33:14.356671:  
2025-02-26 05:33:14.361756: Epoch 65 
2025-02-26 05:33:14.365355: Current learning rate: 0.00389 
2025-02-26 05:33:20.880982: train_loss -0.9382 
2025-02-26 05:33:20.887201: val_loss -0.8314 
2025-02-26 05:33:20.889737: Pseudo dice [np.float32(0.8957), np.float32(0.8795)] 
2025-02-26 05:33:20.893780: Epoch time: 6.53 s 
2025-02-26 05:33:21.391608:  
2025-02-26 05:33:21.397690: Epoch 66 
2025-02-26 05:33:21.400748: Current learning rate: 0.00379 
2025-02-26 05:33:27.896672: train_loss -0.9394 
2025-02-26 05:33:27.902775: val_loss -0.8383 
2025-02-26 05:33:27.906900: Pseudo dice [np.float32(0.9007), np.float32(0.8808)] 
2025-02-26 05:33:27.909441: Epoch time: 6.51 s 
2025-02-26 05:33:28.559042:  
2025-02-26 05:33:28.564055: Epoch 67 
2025-02-26 05:33:28.567565: Current learning rate: 0.00369 
2025-02-26 05:33:35.070027: train_loss -0.9384 
2025-02-26 05:33:35.076221: val_loss -0.8317 
2025-02-26 05:33:35.079803: Pseudo dice [np.float32(0.8944), np.float32(0.8778)] 
2025-02-26 05:33:35.082830: Epoch time: 6.51 s 
2025-02-26 05:33:35.588748:  
2025-02-26 05:33:35.594792: Epoch 68 
2025-02-26 05:33:35.597842: Current learning rate: 0.00359 
2025-02-26 05:33:42.128558: train_loss -0.9392 
2025-02-26 05:33:42.134256: val_loss -0.8273 
2025-02-26 05:33:42.137812: Pseudo dice [np.float32(0.8943), np.float32(0.876)] 
2025-02-26 05:33:42.140867: Epoch time: 6.54 s 
2025-02-26 05:33:42.648302:  
2025-02-26 05:33:42.653819: Epoch 69 
2025-02-26 05:33:42.657328: Current learning rate: 0.00349 
2025-02-26 05:33:49.149809: train_loss -0.9399 
2025-02-26 05:33:49.154875: val_loss -0.8369 
2025-02-26 05:33:49.157938: Pseudo dice [np.float32(0.8986), np.float32(0.8819)] 
2025-02-26 05:33:49.161446: Epoch time: 6.5 s 
2025-02-26 05:33:49.672388:  
2025-02-26 05:33:49.678400: Epoch 70 
2025-02-26 05:33:49.681411: Current learning rate: 0.00338 
2025-02-26 05:33:56.180559: train_loss -0.9409 
2025-02-26 05:33:56.186181: val_loss -0.8279 
2025-02-26 05:33:56.190209: Pseudo dice [np.float32(0.8945), np.float32(0.8765)] 
2025-02-26 05:33:56.193235: Epoch time: 6.51 s 
2025-02-26 05:33:56.702101:  
2025-02-26 05:33:56.706675: Epoch 71 
2025-02-26 05:33:56.711727: Current learning rate: 0.00328 
2025-02-26 05:34:03.229070: train_loss -0.9418 
2025-02-26 05:34:03.234120: val_loss -0.8318 
2025-02-26 05:34:03.237658: Pseudo dice [np.float32(0.8958), np.float32(0.8793)] 
2025-02-26 05:34:03.241229: Epoch time: 6.53 s 
2025-02-26 05:34:03.757038:  
2025-02-26 05:34:03.763102: Epoch 72 
2025-02-26 05:34:03.766167: Current learning rate: 0.00318 
2025-02-26 05:34:10.272790: train_loss -0.94 
2025-02-26 05:34:10.278486: val_loss -0.8308 
2025-02-26 05:34:10.282130: Pseudo dice [np.float32(0.8956), np.float32(0.8776)] 
2025-02-26 05:34:10.285180: Epoch time: 6.52 s 
2025-02-26 05:34:10.797015:  
2025-02-26 05:34:10.802548: Epoch 73 
2025-02-26 05:34:10.806095: Current learning rate: 0.00308 
2025-02-26 05:34:17.301295: train_loss -0.9407 
2025-02-26 05:34:17.306882: val_loss -0.8316 
2025-02-26 05:34:17.310511: Pseudo dice [np.float32(0.8956), np.float32(0.8805)] 
2025-02-26 05:34:17.313542: Epoch time: 6.51 s 
2025-02-26 05:34:17.828703:  
2025-02-26 05:34:17.833462: Epoch 74 
2025-02-26 05:34:17.836979: Current learning rate: 0.00297 
2025-02-26 05:34:24.346189: train_loss -0.9432 
2025-02-26 05:34:24.352245: val_loss -0.8255 
2025-02-26 05:34:24.355324: Pseudo dice [np.float32(0.893), np.float32(0.8752)] 
2025-02-26 05:34:24.359032: Epoch time: 6.52 s 
2025-02-26 05:34:25.027830:  
2025-02-26 05:34:25.032864: Epoch 75 
2025-02-26 05:34:25.036791: Current learning rate: 0.00287 
2025-02-26 05:34:31.530378: train_loss -0.9415 
2025-02-26 05:34:31.536466: val_loss -0.8326 
2025-02-26 05:34:31.540122: Pseudo dice [np.float32(0.8974), np.float32(0.879)] 
2025-02-26 05:34:31.543164: Epoch time: 6.5 s 
2025-02-26 05:34:32.051338:  
2025-02-26 05:34:32.056899: Epoch 76 
2025-02-26 05:34:32.060474: Current learning rate: 0.00277 
2025-02-26 05:34:38.537013: train_loss -0.9434 
2025-02-26 05:34:38.542074: val_loss -0.8349 
2025-02-26 05:34:38.546127: Pseudo dice [np.float32(0.8987), np.float32(0.8795)] 
2025-02-26 05:34:38.549746: Epoch time: 6.49 s 
2025-02-26 05:34:39.059690:  
2025-02-26 05:34:39.064708: Epoch 77 
2025-02-26 05:34:39.068224: Current learning rate: 0.00266 
2025-02-26 05:34:45.567265: train_loss -0.942 
2025-02-26 05:34:45.573799: val_loss -0.8292 
2025-02-26 05:34:45.577872: Pseudo dice [np.float32(0.8958), np.float32(0.878)] 
2025-02-26 05:34:45.581658: Epoch time: 6.51 s 
2025-02-26 05:34:46.101743:  
2025-02-26 05:34:46.107260: Epoch 78 
2025-02-26 05:34:46.110774: Current learning rate: 0.00256 
2025-02-26 05:34:52.604129: train_loss -0.9437 
2025-02-26 05:34:52.610194: val_loss -0.8298 
2025-02-26 05:34:52.613801: Pseudo dice [np.float32(0.8969), np.float32(0.8785)] 
2025-02-26 05:34:52.616868: Epoch time: 6.5 s 
2025-02-26 05:34:53.134119:  
2025-02-26 05:34:53.139696: Epoch 79 
2025-02-26 05:34:53.143308: Current learning rate: 0.00245 
2025-02-26 05:34:59.653738: train_loss -0.9437 
2025-02-26 05:34:59.659318: val_loss -0.8321 
2025-02-26 05:34:59.662902: Pseudo dice [np.float32(0.8978), np.float32(0.8793)] 
2025-02-26 05:34:59.666467: Epoch time: 6.52 s 
2025-02-26 05:35:00.188136:  
2025-02-26 05:35:00.193148: Epoch 80 
2025-02-26 05:35:00.196158: Current learning rate: 0.00235 
2025-02-26 05:35:06.708556: train_loss -0.9436 
2025-02-26 05:35:06.714751: val_loss -0.8346 
2025-02-26 05:35:06.718264: Pseudo dice [np.float32(0.8977), np.float32(0.8801)] 
2025-02-26 05:35:06.721274: Epoch time: 6.52 s 
2025-02-26 05:35:07.244357:  
2025-02-26 05:35:07.249872: Epoch 81 
2025-02-26 05:35:07.253388: Current learning rate: 0.00224 
2025-02-26 05:35:13.757308: train_loss -0.9445 
2025-02-26 05:35:13.763972: val_loss -0.8243 
2025-02-26 05:35:13.767080: Pseudo dice [np.float32(0.8928), np.float32(0.8746)] 
2025-02-26 05:35:13.770658: Epoch time: 6.51 s 
2025-02-26 05:35:14.422995:  
2025-02-26 05:35:14.428602: Epoch 82 
2025-02-26 05:35:14.431660: Current learning rate: 0.00214 
2025-02-26 05:35:20.940762: train_loss -0.9444 
2025-02-26 05:35:20.946364: val_loss -0.8293 
2025-02-26 05:35:20.950922: Pseudo dice [np.float32(0.8954), np.float32(0.8776)] 
2025-02-26 05:35:20.954490: Epoch time: 6.52 s 
2025-02-26 05:35:21.447946:  
2025-02-26 05:35:21.453492: Epoch 83 
2025-02-26 05:35:21.456535: Current learning rate: 0.00203 
2025-02-26 05:35:27.942643: train_loss -0.9433 
2025-02-26 05:35:27.948990: val_loss -0.8335 
2025-02-26 05:35:27.952513: Pseudo dice [np.float32(0.8973), np.float32(0.8798)] 
2025-02-26 05:35:27.956249: Epoch time: 6.5 s 
2025-02-26 05:35:28.440200:  
2025-02-26 05:35:28.445721: Epoch 84 
2025-02-26 05:35:28.449237: Current learning rate: 0.00192 
2025-02-26 05:35:34.957157: train_loss -0.9445 
2025-02-26 05:35:34.963244: val_loss -0.8276 
2025-02-26 05:35:34.966276: Pseudo dice [np.float32(0.8954), np.float32(0.8766)] 
2025-02-26 05:35:34.969904: Epoch time: 6.52 s 
2025-02-26 05:35:35.456188:  
2025-02-26 05:35:35.462256: Epoch 85 
2025-02-26 05:35:35.465925: Current learning rate: 0.00181 
2025-02-26 05:35:41.971184: train_loss -0.9448 
2025-02-26 05:35:41.978888: val_loss -0.8282 
2025-02-26 05:35:41.982461: Pseudo dice [np.float32(0.8954), np.float32(0.877)] 
2025-02-26 05:35:41.985502: Epoch time: 6.51 s 
2025-02-26 05:35:42.462632:  
2025-02-26 05:35:42.468750: Epoch 86 
2025-02-26 05:35:42.472260: Current learning rate: 0.0017 
2025-02-26 05:35:48.963555: train_loss -0.9467 
2025-02-26 05:35:48.969074: val_loss -0.8332 
2025-02-26 05:35:48.974146: Pseudo dice [np.float32(0.8978), np.float32(0.8806)] 
2025-02-26 05:35:48.979285: Epoch time: 6.5 s 
2025-02-26 05:35:49.460108:  
2025-02-26 05:35:49.465151: Epoch 87 
2025-02-26 05:35:49.467497: Current learning rate: 0.00159 
2025-02-26 05:35:55.967930: train_loss -0.9457 
2025-02-26 05:35:55.973948: val_loss -0.8286 
2025-02-26 05:35:55.977965: Pseudo dice [np.float32(0.8958), np.float32(0.8769)] 
2025-02-26 05:35:55.980537: Epoch time: 6.51 s 
2025-02-26 05:35:56.463978:  
2025-02-26 05:35:56.469001: Epoch 88 
2025-02-26 05:35:56.471638: Current learning rate: 0.00148 
2025-02-26 05:36:02.978901: train_loss -0.9446 
2025-02-26 05:36:02.984701: val_loss -0.8312 
2025-02-26 05:36:02.987733: Pseudo dice [np.float32(0.8977), np.float32(0.8775)] 
2025-02-26 05:36:02.991761: Epoch time: 6.52 s 
2025-02-26 05:36:03.475410:  
2025-02-26 05:36:03.480432: Epoch 89 
2025-02-26 05:36:03.484000: Current learning rate: 0.00137 
2025-02-26 05:36:09.994049: train_loss -0.9455 
2025-02-26 05:36:10.000112: val_loss -0.8301 
2025-02-26 05:36:10.004168: Pseudo dice [np.float32(0.8963), np.float32(0.8781)] 
2025-02-26 05:36:10.007221: Epoch time: 6.52 s 
2025-02-26 05:36:10.489249:  
2025-02-26 05:36:10.495266: Epoch 90 
2025-02-26 05:36:10.497775: Current learning rate: 0.00126 
2025-02-26 05:36:17.139821: train_loss -0.9465 
2025-02-26 05:36:17.145416: val_loss -0.8267 
2025-02-26 05:36:17.148076: Pseudo dice [np.float32(0.8951), np.float32(0.8758)] 
2025-02-26 05:36:17.151717: Epoch time: 6.65 s 
2025-02-26 05:36:17.632870:  
2025-02-26 05:36:17.638432: Epoch 91 
2025-02-26 05:36:17.641470: Current learning rate: 0.00115 
2025-02-26 05:36:24.139600: train_loss -0.9461 
2025-02-26 05:36:24.144633: val_loss -0.8293 
2025-02-26 05:36:24.148678: Pseudo dice [np.float32(0.8956), np.float32(0.8778)] 
2025-02-26 05:36:24.151770: Epoch time: 6.51 s 
2025-02-26 05:36:24.633468:  
2025-02-26 05:36:24.638489: Epoch 92 
2025-02-26 05:36:24.642005: Current learning rate: 0.00103 
2025-02-26 05:36:31.148805: train_loss -0.9466 
2025-02-26 05:36:31.154887: val_loss -0.8324 
2025-02-26 05:36:31.159040: Pseudo dice [np.float32(0.8978), np.float32(0.8803)] 
2025-02-26 05:36:31.162088: Epoch time: 6.52 s 
2025-02-26 05:36:31.641989:  
2025-02-26 05:36:31.647548: Epoch 93 
2025-02-26 05:36:31.650609: Current learning rate: 0.00091 
2025-02-26 05:36:38.126782: train_loss -0.9456 
2025-02-26 05:36:38.131119: val_loss -0.8256 
2025-02-26 05:36:38.135884: Pseudo dice [np.float32(0.8943), np.float32(0.8755)] 
2025-02-26 05:36:38.138941: Epoch time: 6.48 s 
2025-02-26 05:36:38.624995:  
2025-02-26 05:36:38.630021: Epoch 94 
2025-02-26 05:36:38.632444: Current learning rate: 0.00079 
2025-02-26 05:36:45.119987: train_loss -0.9471 
2025-02-26 05:36:45.125577: val_loss -0.8291 
2025-02-26 05:36:45.128653: Pseudo dice [np.float32(0.8961), np.float32(0.8781)] 
2025-02-26 05:36:45.132339: Epoch time: 6.5 s 
2025-02-26 05:36:45.614854:  
2025-02-26 05:36:45.620378: Epoch 95 
2025-02-26 05:36:45.623890: Current learning rate: 0.00067 
2025-02-26 05:36:52.138947: train_loss -0.9466 
2025-02-26 05:36:52.145049: val_loss -0.8285 
2025-02-26 05:36:52.148608: Pseudo dice [np.float32(0.8969), np.float32(0.8774)] 
2025-02-26 05:36:52.151638: Epoch time: 6.53 s 
2025-02-26 05:36:52.634567:  
2025-02-26 05:36:52.639132: Epoch 96 
2025-02-26 05:36:52.643718: Current learning rate: 0.00055 
2025-02-26 05:36:59.136034: train_loss -0.9474 
2025-02-26 05:36:59.142207: val_loss -0.8332 
2025-02-26 05:36:59.146773: Pseudo dice [np.float32(0.8977), np.float32(0.8817)] 
2025-02-26 05:36:59.149962: Epoch time: 6.5 s 
2025-02-26 05:36:59.640452:  
2025-02-26 05:36:59.646011: Epoch 97 
2025-02-26 05:36:59.650124: Current learning rate: 0.00043 
2025-02-26 05:37:06.148742: train_loss -0.9444 
2025-02-26 05:37:06.155793: val_loss -0.8316 
2025-02-26 05:37:06.159301: Pseudo dice [np.float32(0.8977), np.float32(0.8789)] 
2025-02-26 05:37:06.162833: Epoch time: 6.51 s 
2025-02-26 05:37:06.653481:  
2025-02-26 05:37:06.658996: Epoch 98 
2025-02-26 05:37:06.662505: Current learning rate: 0.0003 
2025-02-26 05:37:13.165197: train_loss -0.9475 
2025-02-26 05:37:13.170891: val_loss -0.8312 
2025-02-26 05:37:13.174465: Pseudo dice [np.float32(0.8966), np.float32(0.8793)] 
2025-02-26 05:37:13.178535: Epoch time: 6.51 s 
2025-02-26 05:37:13.819242:  
2025-02-26 05:37:13.824254: Epoch 99 
2025-02-26 05:37:13.827773: Current learning rate: 0.00016 
2025-02-26 05:37:20.330644: train_loss -0.9464 
2025-02-26 05:37:20.336666: val_loss -0.8276 
2025-02-26 05:37:20.340172: Pseudo dice [np.float32(0.8966), np.float32(0.8767)] 
2025-02-26 05:37:20.343879: Epoch time: 6.51 s 
2025-02-26 05:37:20.888718: Training done. 
2025-02-26 05:37:20.925718: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-02-26 05:37:20.934228: The split file contains 5 splits. 
2025-02-26 05:37:20.939232: Desired fold for training: 0 
2025-02-26 05:37:20.945230: This split has 208 training and 52 validation cases. 
2025-02-26 05:37:20.949229: predicting hippocampus_017 
2025-02-26 05:37:20.956228: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-02-26 05:37:21.044514: predicting hippocampus_019 
2025-02-26 05:37:21.051515: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-02-26 05:37:21.095521: predicting hippocampus_033 
2025-02-26 05:37:21.101515: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-02-26 05:37:21.121520: predicting hippocampus_035 
2025-02-26 05:37:21.127519: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-02-26 05:37:21.148952: predicting hippocampus_037 
2025-02-26 05:37:21.155954: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-02-26 05:37:21.175951: predicting hippocampus_049 
2025-02-26 05:37:21.182953: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-02-26 05:37:21.205951: predicting hippocampus_052 
2025-02-26 05:37:21.211952: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-02-26 05:37:21.234463: predicting hippocampus_065 
2025-02-26 05:37:21.240463: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-02-26 05:37:21.264464: predicting hippocampus_083 
2025-02-26 05:37:21.271464: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-02-26 05:37:21.291464: predicting hippocampus_088 
2025-02-26 05:37:21.298464: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-02-26 05:37:24.743166: predicting hippocampus_090 
2025-02-26 05:37:24.750677: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-02-26 05:37:24.793678: predicting hippocampus_092 
2025-02-26 05:37:24.801678: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-02-26 05:37:24.848188: predicting hippocampus_095 
2025-02-26 05:37:24.856188: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-02-26 05:37:24.904187: predicting hippocampus_107 
2025-02-26 05:37:24.911191: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-02-26 05:37:24.963704: predicting hippocampus_108 
2025-02-26 05:37:24.973702: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-02-26 05:37:25.016705: predicting hippocampus_123 
2025-02-26 05:37:25.026705: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-02-26 05:37:25.062210: predicting hippocampus_125 
2025-02-26 05:37:25.070211: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-02-26 05:37:25.124213: predicting hippocampus_157 
2025-02-26 05:37:25.131718: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-02-26 05:37:25.161718: predicting hippocampus_164 
2025-02-26 05:37:25.168718: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-02-26 05:37:25.253227: predicting hippocampus_169 
2025-02-26 05:37:25.261228: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-02-26 05:37:25.290227: predicting hippocampus_175 
2025-02-26 05:37:25.297228: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-02-26 05:37:25.326234: predicting hippocampus_185 
2025-02-26 05:37:25.334742: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-02-26 05:37:25.363742: predicting hippocampus_190 
2025-02-26 05:37:25.371742: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-02-26 05:37:25.401742: predicting hippocampus_194 
2025-02-26 05:37:25.409742: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-02-26 05:37:25.438249: predicting hippocampus_204 
2025-02-26 05:37:25.444249: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-02-26 05:37:25.473249: predicting hippocampus_205 
2025-02-26 05:37:25.481250: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-02-26 05:37:25.510251: predicting hippocampus_210 
2025-02-26 05:37:25.517255: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-02-26 05:37:25.545762: predicting hippocampus_217 
2025-02-26 05:37:25.552765: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-02-26 05:37:25.582762: predicting hippocampus_219 
2025-02-26 05:37:25.589763: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-02-26 05:37:25.617766: predicting hippocampus_229 
2025-02-26 05:37:25.624765: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-02-26 05:37:25.655271: predicting hippocampus_244 
2025-02-26 05:37:25.661271: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-02-26 05:37:25.689271: predicting hippocampus_261 
2025-02-26 05:37:25.696272: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-02-26 05:37:25.741780: predicting hippocampus_264 
2025-02-26 05:37:25.747782: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-02-26 05:37:25.778780: predicting hippocampus_277 
2025-02-26 05:37:25.784783: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-02-26 05:37:25.830289: predicting hippocampus_280 
2025-02-26 05:37:25.837291: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-02-26 05:37:25.866290: predicting hippocampus_286 
2025-02-26 05:37:25.874290: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-02-26 05:37:25.922292: predicting hippocampus_288 
2025-02-26 05:37:25.929796: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-02-26 05:37:25.974799: predicting hippocampus_289 
2025-02-26 05:37:25.981801: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-02-26 05:37:26.009799: predicting hippocampus_296 
2025-02-26 05:37:26.015802: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-02-26 05:37:26.044307: predicting hippocampus_305 
2025-02-26 05:37:26.052308: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-02-26 05:37:26.081307: predicting hippocampus_308 
2025-02-26 05:37:26.088307: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-02-26 05:37:26.117311: predicting hippocampus_317 
2025-02-26 05:37:26.124310: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-02-26 05:37:26.151816: predicting hippocampus_327 
2025-02-26 05:37:26.158816: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-02-26 05:37:26.186816: predicting hippocampus_330 
2025-02-26 05:37:26.193816: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-02-26 05:37:26.221819: predicting hippocampus_332 
2025-02-26 05:37:26.227820: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-02-26 05:37:26.256326: predicting hippocampus_338 
2025-02-26 05:37:26.262326: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-02-26 05:37:26.308326: predicting hippocampus_349 
2025-02-26 05:37:26.315330: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-02-26 05:37:26.343834: predicting hippocampus_350 
2025-02-26 05:37:26.349835: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-02-26 05:37:26.378834: predicting hippocampus_356 
2025-02-26 05:37:26.385834: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-02-26 05:37:26.414836: predicting hippocampus_358 
2025-02-26 05:37:26.422837: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-02-26 05:37:26.451343: predicting hippocampus_374 
2025-02-26 05:37:26.456345: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-02-26 05:37:26.485343: predicting hippocampus_394 
2025-02-26 05:37:26.492344: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-02-26 05:37:30.092271: Validation complete 
2025-02-26 05:37:30.098271: Mean Validation Dice:  0.33087358939505573 
