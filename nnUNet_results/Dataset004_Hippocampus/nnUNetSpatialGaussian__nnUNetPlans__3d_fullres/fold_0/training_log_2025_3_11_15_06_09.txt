
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-11 15:06:09.940796: do_dummy_2d_data_aug: False 
2025-03-11 15:06:09.957882: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-11 15:06:09.966779: The split file contains 5 splits. 
2025-03-11 15:06:09.969734: Desired fold for training: 0 
2025-03-11 15:06:09.973197: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-11 15:06:16.682539: unpacking dataset... 
2025-03-11 15:06:17.547400: unpacking done... 
2025-03-11 15:06:18.843674:  
2025-03-11 15:06:18.848823: Epoch 0 
2025-03-11 15:06:18.852374: Current learning rate: 0.01 
2025-03-11 15:06:26.475590: train_loss -0.4889 
2025-03-11 15:06:26.481774: val_loss -0.8065 
2025-03-11 15:06:26.485440: Pseudo dice [np.float32(0.8589), np.float32(0.8549)] 
2025-03-11 15:06:26.489582: Epoch time: 7.63 s 
2025-03-11 15:06:26.493215: Yayy! New best EMA pseudo Dice: 0.8568999767303467 
2025-03-11 15:06:27.028368:  
2025-03-11 15:06:27.034487: Epoch 1 
2025-03-11 15:06:27.038072: Current learning rate: 0.00991 
2025-03-11 15:06:33.770277: train_loss -0.8194 
2025-03-11 15:06:33.775943: val_loss -0.8338 
2025-03-11 15:06:33.779757: Pseudo dice [np.float32(0.8829), np.float32(0.8675)] 
2025-03-11 15:06:33.782866: Epoch time: 6.74 s 
2025-03-11 15:06:33.788171: Yayy! New best EMA pseudo Dice: 0.8587999939918518 
2025-03-11 15:06:34.386969:  
2025-03-11 15:06:34.392009: Epoch 2 
2025-03-11 15:06:34.396540: Current learning rate: 0.00982 
2025-03-11 15:06:41.151549: train_loss -0.8416 
2025-03-11 15:06:41.157724: val_loss -0.837 
2025-03-11 15:06:41.160993: Pseudo dice [np.float32(0.8853), np.float32(0.8703)] 
2025-03-11 15:06:41.164752: Epoch time: 6.77 s 
2025-03-11 15:06:41.167956: Yayy! New best EMA pseudo Dice: 0.8607000112533569 
2025-03-11 15:06:41.783426:  
2025-03-11 15:06:41.788995: Epoch 3 
2025-03-11 15:06:41.792610: Current learning rate: 0.00973 
2025-03-11 15:06:48.530776: train_loss -0.8547 
2025-03-11 15:06:48.535627: val_loss -0.8429 
2025-03-11 15:06:48.538871: Pseudo dice [np.float32(0.8919), np.float32(0.8746)] 
2025-03-11 15:06:48.542482: Epoch time: 6.75 s 
2025-03-11 15:06:48.545592: Yayy! New best EMA pseudo Dice: 0.8629000186920166 
2025-03-11 15:06:49.150065:  
2025-03-11 15:06:49.155222: Epoch 4 
2025-03-11 15:06:49.160040: Current learning rate: 0.00964 
2025-03-11 15:06:55.888202: train_loss -0.8615 
2025-03-11 15:06:55.894271: val_loss -0.8455 
2025-03-11 15:06:55.898399: Pseudo dice [np.float32(0.8946), np.float32(0.8758)] 
2025-03-11 15:06:55.901971: Epoch time: 6.74 s 
2025-03-11 15:06:55.905042: Yayy! New best EMA pseudo Dice: 0.8651000261306763 
2025-03-11 15:06:56.651652:  
2025-03-11 15:06:56.657227: Epoch 5 
2025-03-11 15:06:56.660288: Current learning rate: 0.00955 
2025-03-11 15:07:03.385343: train_loss -0.8684 
2025-03-11 15:07:03.391477: val_loss -0.8414 
2025-03-11 15:07:03.395066: Pseudo dice [np.float32(0.892), np.float32(0.8713)] 
2025-03-11 15:07:03.398608: Epoch time: 6.73 s 
2025-03-11 15:07:03.402231: Yayy! New best EMA pseudo Dice: 0.8668000102043152 
2025-03-11 15:07:03.991982:  
2025-03-11 15:07:03.998563: Epoch 6 
2025-03-11 15:07:04.002090: Current learning rate: 0.00946 
2025-03-11 15:07:10.736443: train_loss -0.8772 
2025-03-11 15:07:10.742722: val_loss -0.8455 
2025-03-11 15:07:10.746298: Pseudo dice [np.float32(0.8947), np.float32(0.8758)] 
2025-03-11 15:07:10.749824: Epoch time: 6.74 s 
2025-03-11 15:07:10.752849: Yayy! New best EMA pseudo Dice: 0.8686000108718872 
2025-03-11 15:07:11.354509:  
2025-03-11 15:07:11.360753: Epoch 7 
2025-03-11 15:07:11.364438: Current learning rate: 0.00937 
2025-03-11 15:07:18.105515: train_loss -0.8809 
2025-03-11 15:07:18.111522: val_loss -0.8477 
2025-03-11 15:07:18.114648: Pseudo dice [np.float32(0.8965), np.float32(0.8794)] 
2025-03-11 15:07:18.118544: Epoch time: 6.75 s 
2025-03-11 15:07:18.121903: Yayy! New best EMA pseudo Dice: 0.8705999851226807 
2025-03-11 15:07:18.735034:  
2025-03-11 15:07:18.740596: Epoch 8 
2025-03-11 15:07:18.744233: Current learning rate: 0.00928 
2025-03-11 15:07:25.478974: train_loss -0.8851 
2025-03-11 15:07:25.485130: val_loss -0.8443 
2025-03-11 15:07:25.488615: Pseudo dice [np.float32(0.8946), np.float32(0.8757)] 
2025-03-11 15:07:25.492252: Epoch time: 6.74 s 
2025-03-11 15:07:25.495436: Yayy! New best EMA pseudo Dice: 0.871999979019165 
2025-03-11 15:07:26.117924:  
2025-03-11 15:07:26.123178: Epoch 9 
2025-03-11 15:07:26.127046: Current learning rate: 0.00919 
2025-03-11 15:07:32.832725: train_loss -0.8881 
2025-03-11 15:07:32.838386: val_loss -0.847 
2025-03-11 15:07:32.842010: Pseudo dice [np.float32(0.8967), np.float32(0.8786)] 
2025-03-11 15:07:32.845728: Epoch time: 6.72 s 
2025-03-11 15:07:32.848844: Yayy! New best EMA pseudo Dice: 0.8736000061035156 
2025-03-11 15:07:33.439735:  
2025-03-11 15:07:33.445345: Epoch 10 
2025-03-11 15:07:33.448497: Current learning rate: 0.0091 
2025-03-11 15:07:40.186980: train_loss -0.8901 
2025-03-11 15:07:40.193598: val_loss -0.8403 
2025-03-11 15:07:40.197171: Pseudo dice [np.float32(0.8917), np.float32(0.8764)] 
2025-03-11 15:07:40.200798: Epoch time: 6.75 s 
2025-03-11 15:07:40.203946: Yayy! New best EMA pseudo Dice: 0.8745999932289124 
2025-03-11 15:07:40.801151:  
2025-03-11 15:07:40.805294: Epoch 11 
2025-03-11 15:07:40.809339: Current learning rate: 0.009 
2025-03-11 15:07:47.546675: train_loss -0.8933 
2025-03-11 15:07:47.552663: val_loss -0.8402 
2025-03-11 15:07:47.556288: Pseudo dice [np.float32(0.8931), np.float32(0.8744)] 
2025-03-11 15:07:47.559932: Epoch time: 6.75 s 
2025-03-11 15:07:47.563049: Yayy! New best EMA pseudo Dice: 0.8755000233650208 
2025-03-11 15:07:48.167449:  
2025-03-11 15:07:48.173654: Epoch 12 
2025-03-11 15:07:48.177327: Current learning rate: 0.00891 
2025-03-11 15:07:54.892155: train_loss -0.8968 
2025-03-11 15:07:54.898764: val_loss -0.8484 
2025-03-11 15:07:54.902376: Pseudo dice [np.float32(0.8992), np.float32(0.8794)] 
2025-03-11 15:07:54.905458: Epoch time: 6.73 s 
2025-03-11 15:07:54.909507: Yayy! New best EMA pseudo Dice: 0.8769000172615051 
2025-03-11 15:07:55.663918:  
2025-03-11 15:07:55.670138: Epoch 13 
2025-03-11 15:07:55.673773: Current learning rate: 0.00882 
2025-03-11 15:08:02.387389: train_loss -0.8997 
2025-03-11 15:08:02.393466: val_loss -0.8436 
2025-03-11 15:08:02.397010: Pseudo dice [np.float32(0.8961), np.float32(0.8752)] 
2025-03-11 15:08:02.400652: Epoch time: 6.72 s 
2025-03-11 15:08:02.404319: Yayy! New best EMA pseudo Dice: 0.8777999877929688 
2025-03-11 15:08:03.004288:  
2025-03-11 15:08:03.009986: Epoch 14 
2025-03-11 15:08:03.013605: Current learning rate: 0.00873 
2025-03-11 15:08:09.749391: train_loss -0.8986 
2025-03-11 15:08:09.756118: val_loss -0.8407 
2025-03-11 15:08:09.759726: Pseudo dice [np.float32(0.8932), np.float32(0.8752)] 
2025-03-11 15:08:09.763406: Epoch time: 6.75 s 
2025-03-11 15:08:09.767017: Yayy! New best EMA pseudo Dice: 0.8784000277519226 
2025-03-11 15:08:10.370730:  
2025-03-11 15:08:10.376468: Epoch 15 
2025-03-11 15:08:10.380066: Current learning rate: 0.00864 
2025-03-11 15:08:17.101206: train_loss -0.9028 
2025-03-11 15:08:17.107830: val_loss -0.8422 
2025-03-11 15:08:17.111594: Pseudo dice [np.float32(0.8959), np.float32(0.8749)] 
2025-03-11 15:08:17.115176: Epoch time: 6.73 s 
2025-03-11 15:08:17.118024: Yayy! New best EMA pseudo Dice: 0.8791000247001648 
2025-03-11 15:08:17.754141:  
2025-03-11 15:08:17.760184: Epoch 16 
2025-03-11 15:08:17.763733: Current learning rate: 0.00855 
2025-03-11 15:08:24.480354: train_loss -0.9043 
2025-03-11 15:08:24.486653: val_loss -0.8473 
2025-03-11 15:08:24.490234: Pseudo dice [np.float32(0.8986), np.float32(0.8812)] 
2025-03-11 15:08:24.493355: Epoch time: 6.73 s 
2025-03-11 15:08:24.496505: Yayy! New best EMA pseudo Dice: 0.8802000284194946 
2025-03-11 15:08:25.117076:  
2025-03-11 15:08:25.122193: Epoch 17 
2025-03-11 15:08:25.126449: Current learning rate: 0.00846 
2025-03-11 15:08:31.862796: train_loss -0.9047 
2025-03-11 15:08:31.869534: val_loss -0.8434 
2025-03-11 15:08:31.873214: Pseudo dice [np.float32(0.8948), np.float32(0.8792)] 
2025-03-11 15:08:31.876845: Epoch time: 6.75 s 
2025-03-11 15:08:31.880489: Yayy! New best EMA pseudo Dice: 0.8809000253677368 
2025-03-11 15:08:32.496928:  
2025-03-11 15:08:32.503226: Epoch 18 
2025-03-11 15:08:32.506839: Current learning rate: 0.00836 
2025-03-11 15:08:39.218129: train_loss -0.9086 
2025-03-11 15:08:39.224930: val_loss -0.8429 
2025-03-11 15:08:39.228564: Pseudo dice [np.float32(0.8975), np.float32(0.8781)] 
2025-03-11 15:08:39.232179: Epoch time: 6.72 s 
2025-03-11 15:08:39.235810: Yayy! New best EMA pseudo Dice: 0.881600022315979 
2025-03-11 15:08:39.862380:  
2025-03-11 15:08:39.869163: Epoch 19 
2025-03-11 15:08:39.872799: Current learning rate: 0.00827 
2025-03-11 15:08:46.589468: train_loss -0.91 
2025-03-11 15:08:46.596178: val_loss -0.8384 
2025-03-11 15:08:46.600390: Pseudo dice [np.float32(0.8939), np.float32(0.8745)] 
2025-03-11 15:08:46.603715: Epoch time: 6.73 s 
2025-03-11 15:08:46.607348: Yayy! New best EMA pseudo Dice: 0.8817999958992004 
2025-03-11 15:08:47.366079:  
2025-03-11 15:08:47.371743: Epoch 20 
2025-03-11 15:08:47.375312: Current learning rate: 0.00818 
2025-03-11 15:08:54.090307: train_loss -0.9113 
2025-03-11 15:08:54.094987: val_loss -0.8419 
2025-03-11 15:08:54.098593: Pseudo dice [np.float32(0.8963), np.float32(0.8796)] 
2025-03-11 15:08:54.102128: Epoch time: 6.72 s 
2025-03-11 15:08:54.105767: Yayy! New best EMA pseudo Dice: 0.8823999762535095 
2025-03-11 15:08:54.722029:  
2025-03-11 15:08:54.727242: Epoch 21 
2025-03-11 15:08:54.731723: Current learning rate: 0.00809 
2025-03-11 15:09:01.438877: train_loss -0.9128 
2025-03-11 15:09:01.444951: val_loss -0.8454 
2025-03-11 15:09:01.449005: Pseudo dice [np.float32(0.8977), np.float32(0.88)] 
2025-03-11 15:09:01.452144: Epoch time: 6.72 s 
2025-03-11 15:09:01.455688: Yayy! New best EMA pseudo Dice: 0.8830999732017517 
2025-03-11 15:09:02.047020:  
2025-03-11 15:09:02.052720: Epoch 22 
2025-03-11 15:09:02.056437: Current learning rate: 0.008 
2025-03-11 15:09:08.774188: train_loss -0.9142 
2025-03-11 15:09:08.781007: val_loss -0.842 
2025-03-11 15:09:08.784790: Pseudo dice [np.float32(0.8963), np.float32(0.8784)] 
2025-03-11 15:09:08.788053: Epoch time: 6.73 s 
2025-03-11 15:09:08.791182: Yayy! New best EMA pseudo Dice: 0.8834999799728394 
2025-03-11 15:09:09.382682:  
2025-03-11 15:09:09.388429: Epoch 23 
2025-03-11 15:09:09.392072: Current learning rate: 0.0079 
2025-03-11 15:09:16.115617: train_loss -0.9126 
2025-03-11 15:09:16.121792: val_loss -0.8417 
2025-03-11 15:09:16.126306: Pseudo dice [np.float32(0.8963), np.float32(0.8784)] 
2025-03-11 15:09:16.130548: Epoch time: 6.73 s 
2025-03-11 15:09:16.134181: Yayy! New best EMA pseudo Dice: 0.883899986743927 
2025-03-11 15:09:16.709701:  
2025-03-11 15:09:16.715912: Epoch 24 
2025-03-11 15:09:16.719022: Current learning rate: 0.00781 
2025-03-11 15:09:23.439996: train_loss -0.9131 
2025-03-11 15:09:23.447037: val_loss -0.8396 
2025-03-11 15:09:23.450134: Pseudo dice [np.float32(0.8953), np.float32(0.8777)] 
2025-03-11 15:09:23.453890: Epoch time: 6.73 s 
2025-03-11 15:09:23.457223: Yayy! New best EMA pseudo Dice: 0.8841999769210815 
2025-03-11 15:09:24.072331:  
2025-03-11 15:09:24.078508: Epoch 25 
2025-03-11 15:09:24.082173: Current learning rate: 0.00772 
2025-03-11 15:09:30.789628: train_loss -0.9172 
2025-03-11 15:09:30.795694: val_loss -0.8432 
2025-03-11 15:09:30.798738: Pseudo dice [np.float32(0.897), np.float32(0.8809)] 
2025-03-11 15:09:30.801796: Epoch time: 6.72 s 
2025-03-11 15:09:30.805338: Yayy! New best EMA pseudo Dice: 0.8845999836921692 
2025-03-11 15:09:31.399240:  
2025-03-11 15:09:31.405051: Epoch 26 
2025-03-11 15:09:31.408593: Current learning rate: 0.00763 
2025-03-11 15:09:38.123379: train_loss -0.9182 
2025-03-11 15:09:38.129618: val_loss -0.843 
2025-03-11 15:09:38.133218: Pseudo dice [np.float32(0.8993), np.float32(0.8813)] 
2025-03-11 15:09:38.136818: Epoch time: 6.72 s 
2025-03-11 15:09:38.139331: Yayy! New best EMA pseudo Dice: 0.885200023651123 
2025-03-11 15:09:38.728934:  
2025-03-11 15:09:38.734528: Epoch 27 
2025-03-11 15:09:38.737679: Current learning rate: 0.00753 
2025-03-11 15:09:45.469242: train_loss -0.9179 
2025-03-11 15:09:45.475107: val_loss -0.8386 
2025-03-11 15:09:45.479178: Pseudo dice [np.float32(0.8964), np.float32(0.8768)] 
2025-03-11 15:09:45.482435: Epoch time: 6.74 s 
2025-03-11 15:09:45.485551: Yayy! New best EMA pseudo Dice: 0.8852999806404114 
2025-03-11 15:09:46.240559:  
2025-03-11 15:09:46.246254: Epoch 28 
2025-03-11 15:09:46.249804: Current learning rate: 0.00744 
2025-03-11 15:09:52.973617: train_loss -0.919 
2025-03-11 15:09:52.980232: val_loss -0.8364 
2025-03-11 15:09:52.983844: Pseudo dice [np.float32(0.8956), np.float32(0.8758)] 
2025-03-11 15:09:52.986928: Epoch time: 6.73 s 
2025-03-11 15:09:52.990654: Yayy! New best EMA pseudo Dice: 0.8853999972343445 
2025-03-11 15:09:53.581525:  
2025-03-11 15:09:53.587160: Epoch 29 
2025-03-11 15:09:53.590713: Current learning rate: 0.00735 
2025-03-11 15:10:00.308495: train_loss -0.9206 
2025-03-11 15:10:00.315303: val_loss -0.8388 
2025-03-11 15:10:00.321086: Pseudo dice [np.float32(0.8966), np.float32(0.8791)] 
2025-03-11 15:10:00.324773: Epoch time: 6.73 s 
2025-03-11 15:10:00.327880: Yayy! New best EMA pseudo Dice: 0.8855999708175659 
2025-03-11 15:10:00.935070:  
2025-03-11 15:10:00.941313: Epoch 30 
2025-03-11 15:10:00.944920: Current learning rate: 0.00725 
2025-03-11 15:10:07.654812: train_loss -0.9221 
2025-03-11 15:10:07.660916: val_loss -0.8321 
2025-03-11 15:10:07.664347: Pseudo dice [np.float32(0.892), np.float32(0.8736)] 
2025-03-11 15:10:07.667898: Epoch time: 6.72 s 
2025-03-11 15:10:08.236073:  
2025-03-11 15:10:08.241390: Epoch 31 
2025-03-11 15:10:08.245068: Current learning rate: 0.00716 
2025-03-11 15:10:14.961718: train_loss -0.9214 
2025-03-11 15:10:14.967358: val_loss -0.8418 
2025-03-11 15:10:14.971567: Pseudo dice [np.float32(0.8979), np.float32(0.8811)] 
2025-03-11 15:10:14.974751: Epoch time: 6.73 s 
2025-03-11 15:10:14.978012: Yayy! New best EMA pseudo Dice: 0.8858000040054321 
2025-03-11 15:10:15.577649:  
2025-03-11 15:10:15.583262: Epoch 32 
2025-03-11 15:10:15.586853: Current learning rate: 0.00707 
2025-03-11 15:10:22.294997: train_loss -0.9227 
2025-03-11 15:10:22.301561: val_loss -0.8367 
2025-03-11 15:10:22.304605: Pseudo dice [np.float32(0.8957), np.float32(0.8771)] 
2025-03-11 15:10:22.308143: Epoch time: 6.72 s 
2025-03-11 15:10:22.310697: Yayy! New best EMA pseudo Dice: 0.8858000040054321 
2025-03-11 15:10:22.911369:  
2025-03-11 15:10:22.917209: Epoch 33 
2025-03-11 15:10:22.920830: Current learning rate: 0.00697 
2025-03-11 15:10:29.636576: train_loss -0.9237 
2025-03-11 15:10:29.642902: val_loss -0.8404 
2025-03-11 15:10:29.646526: Pseudo dice [np.float32(0.8975), np.float32(0.8805)] 
2025-03-11 15:10:29.649665: Epoch time: 6.73 s 
2025-03-11 15:10:29.652865: Yayy! New best EMA pseudo Dice: 0.8860999941825867 
2025-03-11 15:10:30.274578:  
2025-03-11 15:10:30.281293: Epoch 34 
2025-03-11 15:10:30.285603: Current learning rate: 0.00688 
2025-03-11 15:10:36.993387: train_loss -0.9256 
2025-03-11 15:10:36.999439: val_loss -0.8358 
2025-03-11 15:10:37.002554: Pseudo dice [np.float32(0.8959), np.float32(0.8788)] 
2025-03-11 15:10:37.006121: Epoch time: 6.72 s 
2025-03-11 15:10:37.009168: Yayy! New best EMA pseudo Dice: 0.8863000273704529 
2025-03-11 15:10:37.613978:  
2025-03-11 15:10:37.618005: Epoch 35 
2025-03-11 15:10:37.621523: Current learning rate: 0.00679 
2025-03-11 15:10:44.352369: train_loss -0.9253 
2025-03-11 15:10:44.358575: val_loss -0.8346 
2025-03-11 15:10:44.362482: Pseudo dice [np.float32(0.895), np.float32(0.8767)] 
2025-03-11 15:10:44.366085: Epoch time: 6.74 s 
2025-03-11 15:10:45.107375:  
2025-03-11 15:10:45.113119: Epoch 36 
2025-03-11 15:10:45.116759: Current learning rate: 0.00669 
2025-03-11 15:10:51.824883: train_loss -0.9254 
2025-03-11 15:10:51.831548: val_loss -0.8387 
2025-03-11 15:10:51.835152: Pseudo dice [np.float32(0.8969), np.float32(0.8804)] 
2025-03-11 15:10:51.838734: Epoch time: 6.72 s 
2025-03-11 15:10:51.841843: Yayy! New best EMA pseudo Dice: 0.8865000009536743 
2025-03-11 15:10:52.470012:  
2025-03-11 15:10:52.475594: Epoch 37 
2025-03-11 15:10:52.478710: Current learning rate: 0.0066 
2025-03-11 15:10:59.190657: train_loss -0.9273 
2025-03-11 15:10:59.196822: val_loss -0.8308 
2025-03-11 15:10:59.200474: Pseudo dice [np.float32(0.895), np.float32(0.876)] 
2025-03-11 15:10:59.203604: Epoch time: 6.72 s 
2025-03-11 15:10:59.780027:  
2025-03-11 15:10:59.785762: Epoch 38 
2025-03-11 15:10:59.788802: Current learning rate: 0.0065 
2025-03-11 15:11:06.510975: train_loss -0.9266 
2025-03-11 15:11:06.517168: val_loss -0.8366 
2025-03-11 15:11:06.520260: Pseudo dice [np.float32(0.8954), np.float32(0.8792)] 
2025-03-11 15:11:06.523930: Epoch time: 6.73 s 
2025-03-11 15:11:07.103876:  
2025-03-11 15:11:07.109587: Epoch 39 
2025-03-11 15:11:07.113191: Current learning rate: 0.00641 
2025-03-11 15:11:13.838885: train_loss -0.9274 
2025-03-11 15:11:13.844652: val_loss -0.8386 
2025-03-11 15:11:13.847725: Pseudo dice [np.float32(0.8975), np.float32(0.8788)] 
2025-03-11 15:11:13.851288: Epoch time: 6.74 s 
2025-03-11 15:11:13.853822: Yayy! New best EMA pseudo Dice: 0.8866000175476074 
2025-03-11 15:11:14.475578:  
2025-03-11 15:11:14.481787: Epoch 40 
2025-03-11 15:11:14.484856: Current learning rate: 0.00631 
2025-03-11 15:11:21.208189: train_loss -0.9262 
2025-03-11 15:11:21.215297: val_loss -0.837 
2025-03-11 15:11:21.218534: Pseudo dice [np.float32(0.8974), np.float32(0.8791)] 
2025-03-11 15:11:21.222143: Epoch time: 6.73 s 
2025-03-11 15:11:21.225240: Yayy! New best EMA pseudo Dice: 0.8867999911308289 
2025-03-11 15:11:21.848855:  
2025-03-11 15:11:21.855069: Epoch 41 
2025-03-11 15:11:21.858195: Current learning rate: 0.00622 
2025-03-11 15:11:28.571126: train_loss -0.929 
2025-03-11 15:11:28.577297: val_loss -0.8367 
2025-03-11 15:11:28.580508: Pseudo dice [np.float32(0.898), np.float32(0.8789)] 
2025-03-11 15:11:28.584085: Epoch time: 6.72 s 
2025-03-11 15:11:28.587247: Yayy! New best EMA pseudo Dice: 0.8870000243186951 
2025-03-11 15:11:29.174944:  
2025-03-11 15:11:29.180595: Epoch 42 
2025-03-11 15:11:29.183756: Current learning rate: 0.00612 
2025-03-11 15:11:35.910263: train_loss -0.9291 
2025-03-11 15:11:35.915524: val_loss -0.8311 
2025-03-11 15:11:35.918900: Pseudo dice [np.float32(0.8944), np.float32(0.876)] 
2025-03-11 15:11:35.922535: Epoch time: 6.74 s 
2025-03-11 15:11:36.475927:  
2025-03-11 15:11:36.481550: Epoch 43 
2025-03-11 15:11:36.484237: Current learning rate: 0.00603 
2025-03-11 15:11:43.203912: train_loss -0.9309 
2025-03-11 15:11:43.210503: val_loss -0.8323 
2025-03-11 15:11:43.214180: Pseudo dice [np.float32(0.8952), np.float32(0.8775)] 
2025-03-11 15:11:43.216802: Epoch time: 6.73 s 
2025-03-11 15:11:43.927089:  
2025-03-11 15:11:43.932132: Epoch 44 
2025-03-11 15:11:43.935166: Current learning rate: 0.00593 
2025-03-11 15:11:50.669708: train_loss -0.9282 
2025-03-11 15:11:50.675299: val_loss -0.8318 
2025-03-11 15:11:50.679210: Pseudo dice [np.float32(0.8943), np.float32(0.8777)] 
2025-03-11 15:11:50.682806: Epoch time: 6.74 s 
2025-03-11 15:11:51.232670:  
2025-03-11 15:11:51.238966: Epoch 45 
2025-03-11 15:11:51.242080: Current learning rate: 0.00584 
2025-03-11 15:11:57.957432: train_loss -0.9306 
2025-03-11 15:11:57.963492: val_loss -0.8286 
2025-03-11 15:11:57.967124: Pseudo dice [np.float32(0.8933), np.float32(0.8753)] 
2025-03-11 15:11:57.970159: Epoch time: 6.73 s 
2025-03-11 15:11:58.513815:  
2025-03-11 15:11:58.519433: Epoch 46 
2025-03-11 15:11:58.523978: Current learning rate: 0.00574 
2025-03-11 15:12:05.245005: train_loss -0.9294 
2025-03-11 15:12:05.251079: val_loss -0.8309 
2025-03-11 15:12:05.254625: Pseudo dice [np.float32(0.8944), np.float32(0.8773)] 
2025-03-11 15:12:05.257660: Epoch time: 6.73 s 
2025-03-11 15:12:05.802034:  
2025-03-11 15:12:05.807736: Epoch 47 
2025-03-11 15:12:05.810912: Current learning rate: 0.00565 
2025-03-11 15:12:12.527523: train_loss -0.9305 
2025-03-11 15:12:12.533999: val_loss -0.8341 
2025-03-11 15:12:12.537609: Pseudo dice [np.float32(0.8963), np.float32(0.8787)] 
2025-03-11 15:12:12.540724: Epoch time: 6.73 s 
2025-03-11 15:12:13.090245:  
2025-03-11 15:12:13.096309: Epoch 48 
2025-03-11 15:12:13.099828: Current learning rate: 0.00555 
2025-03-11 15:12:19.806823: train_loss -0.9322 
2025-03-11 15:12:19.813460: val_loss -0.834 
2025-03-11 15:12:19.817012: Pseudo dice [np.float32(0.8963), np.float32(0.8798)] 
2025-03-11 15:12:19.819560: Epoch time: 6.72 s 
2025-03-11 15:12:20.382725:  
2025-03-11 15:12:20.387910: Epoch 49 
2025-03-11 15:12:20.391468: Current learning rate: 0.00546 
2025-03-11 15:12:27.116037: train_loss -0.9324 
2025-03-11 15:12:27.122207: val_loss -0.838 
2025-03-11 15:12:27.125466: Pseudo dice [np.float32(0.8982), np.float32(0.8814)] 
2025-03-11 15:12:27.129093: Epoch time: 6.73 s 
2025-03-11 15:12:27.718406:  
2025-03-11 15:12:27.724768: Epoch 50 
2025-03-11 15:12:27.727701: Current learning rate: 0.00536 
2025-03-11 15:12:34.426778: train_loss -0.9342 
2025-03-11 15:12:34.433907: val_loss -0.8354 
2025-03-11 15:12:34.437508: Pseudo dice [np.float32(0.8991), np.float32(0.8802)] 
2025-03-11 15:12:34.440588: Epoch time: 6.71 s 
2025-03-11 15:12:34.443727: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-03-11 15:12:35.034222:  
2025-03-11 15:12:35.039906: Epoch 51 
2025-03-11 15:12:35.043442: Current learning rate: 0.00526 
2025-03-11 15:12:41.771057: train_loss -0.9343 
2025-03-11 15:12:41.777301: val_loss -0.8357 
2025-03-11 15:12:41.780408: Pseudo dice [np.float32(0.8983), np.float32(0.8802)] 
2025-03-11 15:12:41.784030: Epoch time: 6.74 s 
2025-03-11 15:12:41.787660: Yayy! New best EMA pseudo Dice: 0.8873999714851379 
2025-03-11 15:12:42.377682:  
2025-03-11 15:12:42.382804: Epoch 52 
2025-03-11 15:12:42.386476: Current learning rate: 0.00517 
2025-03-11 15:12:49.113894: train_loss -0.9331 
2025-03-11 15:12:49.120079: val_loss -0.8331 
2025-03-11 15:12:49.123139: Pseudo dice [np.float32(0.897), np.float32(0.8781)] 
2025-03-11 15:12:49.126179: Epoch time: 6.74 s 
2025-03-11 15:12:49.129707: Yayy! New best EMA pseudo Dice: 0.8873999714851379 
2025-03-11 15:12:49.883380:  
2025-03-11 15:12:49.889453: Epoch 53 
2025-03-11 15:12:49.893074: Current learning rate: 0.00507 
2025-03-11 15:12:56.622935: train_loss -0.9324 
2025-03-11 15:12:56.629331: val_loss -0.8271 
2025-03-11 15:12:56.632484: Pseudo dice [np.float32(0.8942), np.float32(0.8759)] 
2025-03-11 15:12:56.635921: Epoch time: 6.74 s 
2025-03-11 15:12:57.190040:  
2025-03-11 15:12:57.196258: Epoch 54 
2025-03-11 15:12:57.199400: Current learning rate: 0.00497 
2025-03-11 15:13:03.903750: train_loss -0.9354 
2025-03-11 15:13:03.909842: val_loss -0.8285 
2025-03-11 15:13:03.913396: Pseudo dice [np.float32(0.8944), np.float32(0.8766)] 
2025-03-11 15:13:03.916951: Epoch time: 6.71 s 
2025-03-11 15:13:04.476063:  
2025-03-11 15:13:04.481614: Epoch 55 
2025-03-11 15:13:04.484641: Current learning rate: 0.00487 
2025-03-11 15:13:11.203282: train_loss -0.9343 
2025-03-11 15:13:11.209407: val_loss -0.8306 
2025-03-11 15:13:11.213019: Pseudo dice [np.float32(0.8955), np.float32(0.8774)] 
2025-03-11 15:13:11.216066: Epoch time: 6.73 s 
2025-03-11 15:13:11.772135:  
2025-03-11 15:13:11.778217: Epoch 56 
2025-03-11 15:13:11.780746: Current learning rate: 0.00478 
2025-03-11 15:13:18.497362: train_loss -0.9338 
2025-03-11 15:13:18.503461: val_loss -0.832 
2025-03-11 15:13:18.507032: Pseudo dice [np.float32(0.8972), np.float32(0.8785)] 
2025-03-11 15:13:18.510644: Epoch time: 6.73 s 
2025-03-11 15:13:19.062961:  
2025-03-11 15:13:19.068511: Epoch 57 
2025-03-11 15:13:19.072119: Current learning rate: 0.00468 
2025-03-11 15:13:25.777379: train_loss -0.9357 
2025-03-11 15:13:25.783711: val_loss -0.8286 
2025-03-11 15:13:25.787417: Pseudo dice [np.float32(0.8942), np.float32(0.876)] 
2025-03-11 15:13:25.790539: Epoch time: 6.71 s 
2025-03-11 15:13:26.346465:  
2025-03-11 15:13:26.352643: Epoch 58 
2025-03-11 15:13:26.356669: Current learning rate: 0.00458 
2025-03-11 15:13:33.086250: train_loss -0.9344 
2025-03-11 15:13:33.092648: val_loss -0.8298 
2025-03-11 15:13:33.096766: Pseudo dice [np.float32(0.895), np.float32(0.8767)] 
2025-03-11 15:13:33.100398: Epoch time: 6.74 s 
2025-03-11 15:13:33.664808:  
2025-03-11 15:13:33.670511: Epoch 59 
2025-03-11 15:13:33.675277: Current learning rate: 0.00448 
2025-03-11 15:13:40.405591: train_loss -0.9372 
2025-03-11 15:13:40.412687: val_loss -0.833 
2025-03-11 15:13:40.416736: Pseudo dice [np.float32(0.8984), np.float32(0.8788)] 
2025-03-11 15:13:40.419823: Epoch time: 6.74 s 
2025-03-11 15:13:40.981458:  
2025-03-11 15:13:40.986753: Epoch 60 
2025-03-11 15:13:40.991471: Current learning rate: 0.00438 
2025-03-11 15:13:47.729311: train_loss -0.9389 
2025-03-11 15:13:47.735916: val_loss -0.8327 
2025-03-11 15:13:47.739444: Pseudo dice [np.float32(0.898), np.float32(0.8797)] 
2025-03-11 15:13:47.743504: Epoch time: 6.75 s 
2025-03-11 15:13:48.455380:  
2025-03-11 15:13:48.460829: Epoch 61 
2025-03-11 15:13:48.464469: Current learning rate: 0.00429 
2025-03-11 15:13:55.189108: train_loss -0.9384 
2025-03-11 15:13:55.195905: val_loss -0.8268 
2025-03-11 15:13:55.199105: Pseudo dice [np.float32(0.8947), np.float32(0.877)] 
2025-03-11 15:13:55.202702: Epoch time: 6.73 s 
2025-03-11 15:13:55.794132:  
2025-03-11 15:13:55.799815: Epoch 62 
2025-03-11 15:13:55.803526: Current learning rate: 0.00419 
2025-03-11 15:14:02.523183: train_loss -0.9366 
2025-03-11 15:14:02.529337: val_loss -0.8226 
2025-03-11 15:14:02.533142: Pseudo dice [np.float32(0.8928), np.float32(0.8743)] 
2025-03-11 15:14:02.536743: Epoch time: 6.73 s 
2025-03-11 15:14:03.100712:  
2025-03-11 15:14:03.106393: Epoch 63 
2025-03-11 15:14:03.109499: Current learning rate: 0.00409 
2025-03-11 15:14:09.824483: train_loss -0.9378 
2025-03-11 15:14:09.830036: val_loss -0.8302 
2025-03-11 15:14:09.833575: Pseudo dice [np.float32(0.8962), np.float32(0.8776)] 
2025-03-11 15:14:09.837099: Epoch time: 6.72 s 
2025-03-11 15:14:10.413303:  
2025-03-11 15:14:10.419165: Epoch 64 
2025-03-11 15:14:10.422731: Current learning rate: 0.00399 
2025-03-11 15:14:17.123276: train_loss -0.9377 
2025-03-11 15:14:17.128889: val_loss -0.831 
2025-03-11 15:14:17.133335: Pseudo dice [np.float32(0.8962), np.float32(0.8785)] 
2025-03-11 15:14:17.136549: Epoch time: 6.71 s 
2025-03-11 15:14:17.700758:  
2025-03-11 15:14:17.706885: Epoch 65 
2025-03-11 15:14:17.710482: Current learning rate: 0.00389 
2025-03-11 15:14:24.425708: train_loss -0.9389 
2025-03-11 15:14:24.431847: val_loss -0.8291 
2025-03-11 15:14:24.435438: Pseudo dice [np.float32(0.8953), np.float32(0.8776)] 
2025-03-11 15:14:24.438516: Epoch time: 6.72 s 
2025-03-11 15:14:25.006475:  
2025-03-11 15:14:25.012854: Epoch 66 
2025-03-11 15:14:25.016428: Current learning rate: 0.00379 
2025-03-11 15:14:31.745108: train_loss -0.9386 
2025-03-11 15:14:31.751747: val_loss -0.8323 
2025-03-11 15:14:31.755302: Pseudo dice [np.float32(0.8971), np.float32(0.8805)] 
2025-03-11 15:14:31.758495: Epoch time: 6.74 s 
2025-03-11 15:14:32.321589:  
2025-03-11 15:14:32.329298: Epoch 67 
2025-03-11 15:14:32.334355: Current learning rate: 0.00369 
2025-03-11 15:14:39.068528: train_loss -0.9397 
2025-03-11 15:14:39.074283: val_loss -0.8197 
2025-03-11 15:14:39.077890: Pseudo dice [np.float32(0.8908), np.float32(0.8728)] 
2025-03-11 15:14:39.081485: Epoch time: 6.75 s 
2025-03-11 15:14:39.655706:  
2025-03-11 15:14:39.661294: Epoch 68 
2025-03-11 15:14:39.663919: Current learning rate: 0.00359 
2025-03-11 15:14:46.385715: train_loss -0.9402 
2025-03-11 15:14:46.392379: val_loss -0.8305 
2025-03-11 15:14:46.395986: Pseudo dice [np.float32(0.8961), np.float32(0.8771)] 
2025-03-11 15:14:46.399158: Epoch time: 6.73 s 
2025-03-11 15:14:47.143052:  
2025-03-11 15:14:47.148654: Epoch 69 
2025-03-11 15:14:47.151190: Current learning rate: 0.00349 
2025-03-11 15:14:53.866056: train_loss -0.9381 
2025-03-11 15:14:53.870239: val_loss -0.8257 
2025-03-11 15:14:53.873861: Pseudo dice [np.float32(0.895), np.float32(0.8752)] 
2025-03-11 15:14:53.877125: Epoch time: 6.72 s 
2025-03-11 15:14:54.448443:  
2025-03-11 15:14:54.454377: Epoch 70 
2025-03-11 15:14:54.457977: Current learning rate: 0.00338 
2025-03-11 15:15:01.161587: train_loss -0.939 
2025-03-11 15:15:01.167161: val_loss -0.8268 
2025-03-11 15:15:01.170436: Pseudo dice [np.float32(0.895), np.float32(0.8772)] 
2025-03-11 15:15:01.174219: Epoch time: 6.71 s 
2025-03-11 15:15:01.745134:  
2025-03-11 15:15:01.750242: Epoch 71 
2025-03-11 15:15:01.753771: Current learning rate: 0.00328 
2025-03-11 15:15:08.467396: train_loss -0.9407 
2025-03-11 15:15:08.473045: val_loss -0.8303 
2025-03-11 15:15:08.476685: Pseudo dice [np.float32(0.8974), np.float32(0.8777)] 
2025-03-11 15:15:08.479778: Epoch time: 6.72 s 
2025-03-11 15:15:09.059523:  
2025-03-11 15:15:09.065780: Epoch 72 
2025-03-11 15:15:09.069388: Current learning rate: 0.00318 
2025-03-11 15:15:15.773539: train_loss -0.9406 
2025-03-11 15:15:15.779788: val_loss -0.8221 
2025-03-11 15:15:15.783374: Pseudo dice [np.float32(0.892), np.float32(0.8739)] 
2025-03-11 15:15:15.786428: Epoch time: 6.71 s 
2025-03-11 15:15:16.371168:  
2025-03-11 15:15:16.377395: Epoch 73 
2025-03-11 15:15:16.380667: Current learning rate: 0.00308 
2025-03-11 15:15:23.092502: train_loss -0.9401 
2025-03-11 15:15:23.099075: val_loss -0.8303 
2025-03-11 15:15:23.102602: Pseudo dice [np.float32(0.8963), np.float32(0.8796)] 
2025-03-11 15:15:23.105629: Epoch time: 6.72 s 
2025-03-11 15:15:23.686798:  
2025-03-11 15:15:23.693010: Epoch 74 
2025-03-11 15:15:23.696592: Current learning rate: 0.00297 
2025-03-11 15:15:30.411061: train_loss -0.9415 
2025-03-11 15:15:30.417140: val_loss -0.8183 
2025-03-11 15:15:30.420199: Pseudo dice [np.float32(0.8915), np.float32(0.8721)] 
2025-03-11 15:15:30.423726: Epoch time: 6.72 s 
2025-03-11 15:15:30.996048:  
2025-03-11 15:15:31.001591: Epoch 75 
2025-03-11 15:15:31.005207: Current learning rate: 0.00287 
2025-03-11 15:15:37.726334: train_loss -0.9416 
2025-03-11 15:15:37.732068: val_loss -0.8229 
2025-03-11 15:15:37.736194: Pseudo dice [np.float32(0.8941), np.float32(0.8744)] 
2025-03-11 15:15:37.739518: Epoch time: 6.73 s 
2025-03-11 15:15:38.324870:  
2025-03-11 15:15:38.330426: Epoch 76 
2025-03-11 15:15:38.333493: Current learning rate: 0.00277 
2025-03-11 15:15:45.055031: train_loss -0.9421 
2025-03-11 15:15:45.061081: val_loss -0.8244 
2025-03-11 15:15:45.064611: Pseudo dice [np.float32(0.8938), np.float32(0.8763)] 
2025-03-11 15:15:45.067640: Epoch time: 6.73 s 
2025-03-11 15:15:45.802297:  
2025-03-11 15:15:45.808362: Epoch 77 
2025-03-11 15:15:45.811932: Current learning rate: 0.00266 
2025-03-11 15:15:52.512719: train_loss -0.9415 
2025-03-11 15:15:52.519493: val_loss -0.8229 
2025-03-11 15:15:52.522583: Pseudo dice [np.float32(0.8946), np.float32(0.8762)] 
2025-03-11 15:15:52.526789: Epoch time: 6.71 s 
2025-03-11 15:15:53.107909:  
2025-03-11 15:15:53.113578: Epoch 78 
2025-03-11 15:15:53.116097: Current learning rate: 0.00256 
2025-03-11 15:15:59.824378: train_loss -0.9423 
2025-03-11 15:15:59.830629: val_loss -0.8305 
2025-03-11 15:15:59.834348: Pseudo dice [np.float32(0.8978), np.float32(0.8797)] 
2025-03-11 15:15:59.837006: Epoch time: 6.72 s 
2025-03-11 15:16:00.419190:  
2025-03-11 15:16:00.424365: Epoch 79 
2025-03-11 15:16:00.428188: Current learning rate: 0.00245 
2025-03-11 15:16:07.136930: train_loss -0.9419 
2025-03-11 15:16:07.142990: val_loss -0.831 
2025-03-11 15:16:07.146509: Pseudo dice [np.float32(0.8973), np.float32(0.8811)] 
2025-03-11 15:16:07.149559: Epoch time: 6.72 s 
2025-03-11 15:16:07.736582:  
2025-03-11 15:16:07.742657: Epoch 80 
2025-03-11 15:16:07.746197: Current learning rate: 0.00235 
2025-03-11 15:16:14.466010: train_loss -0.9437 
2025-03-11 15:16:14.472202: val_loss -0.8244 
2025-03-11 15:16:14.475807: Pseudo dice [np.float32(0.8953), np.float32(0.8764)] 
2025-03-11 15:16:14.478986: Epoch time: 6.73 s 
2025-03-11 15:16:15.074872:  
2025-03-11 15:16:15.080561: Epoch 81 
2025-03-11 15:16:15.083618: Current learning rate: 0.00224 
2025-03-11 15:16:21.880075: train_loss -0.9433 
2025-03-11 15:16:21.886212: val_loss -0.8186 
2025-03-11 15:16:21.889763: Pseudo dice [np.float32(0.8908), np.float32(0.8727)] 
2025-03-11 15:16:21.893328: Epoch time: 6.81 s 
2025-03-11 15:16:22.484807:  
2025-03-11 15:16:22.491033: Epoch 82 
2025-03-11 15:16:22.494172: Current learning rate: 0.00214 
2025-03-11 15:16:29.233581: train_loss -0.9435 
2025-03-11 15:16:29.239836: val_loss -0.8256 
2025-03-11 15:16:29.242906: Pseudo dice [np.float32(0.8946), np.float32(0.876)] 
2025-03-11 15:16:29.246271: Epoch time: 6.75 s 
2025-03-11 15:16:29.801296:  
2025-03-11 15:16:29.806871: Epoch 83 
2025-03-11 15:16:29.810425: Current learning rate: 0.00203 
2025-03-11 15:16:36.553444: train_loss -0.9415 
2025-03-11 15:16:36.559581: val_loss -0.8348 
2025-03-11 15:16:36.562710: Pseudo dice [np.float32(0.8997), np.float32(0.8825)] 
2025-03-11 15:16:36.566441: Epoch time: 6.75 s 
2025-03-11 15:16:37.264635:  
2025-03-11 15:16:37.270241: Epoch 84 
2025-03-11 15:16:37.273775: Current learning rate: 0.00192 
2025-03-11 15:16:43.991729: train_loss -0.9436 
2025-03-11 15:16:43.998134: val_loss -0.8257 
2025-03-11 15:16:44.001732: Pseudo dice [np.float32(0.8953), np.float32(0.8768)] 
2025-03-11 15:16:44.005418: Epoch time: 6.73 s 
2025-03-11 15:16:44.556676:  
2025-03-11 15:16:44.562760: Epoch 85 
2025-03-11 15:16:44.565876: Current learning rate: 0.00181 
2025-03-11 15:16:51.277694: train_loss -0.9433 
2025-03-11 15:16:51.283725: val_loss -0.8244 
2025-03-11 15:16:51.287320: Pseudo dice [np.float32(0.8957), np.float32(0.8759)] 
2025-03-11 15:16:51.291437: Epoch time: 6.72 s 
2025-03-11 15:16:51.837928:  
2025-03-11 15:16:51.843060: Epoch 86 
2025-03-11 15:16:51.847191: Current learning rate: 0.0017 
2025-03-11 15:16:58.557571: train_loss -0.944 
2025-03-11 15:16:58.563176: val_loss -0.8255 
2025-03-11 15:16:58.567675: Pseudo dice [np.float32(0.8941), np.float32(0.8774)] 
2025-03-11 15:16:58.570727: Epoch time: 6.72 s 
2025-03-11 15:16:59.116431:  
2025-03-11 15:16:59.122133: Epoch 87 
2025-03-11 15:16:59.125727: Current learning rate: 0.00159 
2025-03-11 15:17:05.842353: train_loss -0.9448 
2025-03-11 15:17:05.848068: val_loss -0.8221 
2025-03-11 15:17:05.852145: Pseudo dice [np.float32(0.8943), np.float32(0.8754)] 
2025-03-11 15:17:05.855259: Epoch time: 6.73 s 
2025-03-11 15:17:06.398343:  
2025-03-11 15:17:06.403898: Epoch 88 
2025-03-11 15:17:06.406939: Current learning rate: 0.00148 
2025-03-11 15:17:13.106765: train_loss -0.9454 
2025-03-11 15:17:13.112894: val_loss -0.8257 
2025-03-11 15:17:13.116052: Pseudo dice [np.float32(0.895), np.float32(0.8785)] 
2025-03-11 15:17:13.119601: Epoch time: 6.71 s 
2025-03-11 15:17:13.663263:  
2025-03-11 15:17:13.668895: Epoch 89 
2025-03-11 15:17:13.672302: Current learning rate: 0.00137 
2025-03-11 15:17:20.387454: train_loss -0.9459 
2025-03-11 15:17:20.393596: val_loss -0.825 
2025-03-11 15:17:20.397253: Pseudo dice [np.float32(0.8953), np.float32(0.8768)] 
2025-03-11 15:17:20.400817: Epoch time: 6.72 s 
2025-03-11 15:17:20.955530:  
2025-03-11 15:17:20.960569: Epoch 90 
2025-03-11 15:17:20.964612: Current learning rate: 0.00126 
2025-03-11 15:17:27.729551: train_loss -0.9459 
2025-03-11 15:17:27.736185: val_loss -0.8184 
2025-03-11 15:17:27.738706: Pseudo dice [np.float32(0.8922), np.float32(0.8734)] 
2025-03-11 15:17:27.742234: Epoch time: 6.78 s 
2025-03-11 15:17:28.289501:  
2025-03-11 15:17:28.295264: Epoch 91 
2025-03-11 15:17:28.298856: Current learning rate: 0.00115 
2025-03-11 15:17:35.062123: train_loss -0.9455 
2025-03-11 15:17:35.068221: val_loss -0.8312 
2025-03-11 15:17:35.071270: Pseudo dice [np.float32(0.899), np.float32(0.8816)] 
2025-03-11 15:17:35.075190: Epoch time: 6.77 s 
2025-03-11 15:17:35.619121:  
2025-03-11 15:17:35.624694: Epoch 92 
2025-03-11 15:17:35.628230: Current learning rate: 0.00103 
2025-03-11 15:17:42.346478: train_loss -0.9472 
2025-03-11 15:17:42.352263: val_loss -0.8232 
2025-03-11 15:17:42.356234: Pseudo dice [np.float32(0.8925), np.float32(0.8761)] 
2025-03-11 15:17:42.359852: Epoch time: 6.73 s 
2025-03-11 15:17:43.062743:  
2025-03-11 15:17:43.068462: Epoch 93 
2025-03-11 15:17:43.072065: Current learning rate: 0.00091 
2025-03-11 15:17:49.768510: train_loss -0.9461 
2025-03-11 15:17:49.773677: val_loss -0.8324 
2025-03-11 15:17:49.777239: Pseudo dice [np.float32(0.899), np.float32(0.8815)] 
2025-03-11 15:17:49.781421: Epoch time: 6.71 s 
2025-03-11 15:17:50.329719:  
2025-03-11 15:17:50.335417: Epoch 94 
2025-03-11 15:17:50.339030: Current learning rate: 0.00079 
2025-03-11 15:17:57.039355: train_loss -0.9454 
2025-03-11 15:17:57.046004: val_loss -0.8241 
2025-03-11 15:17:57.049560: Pseudo dice [np.float32(0.8943), np.float32(0.8765)] 
2025-03-11 15:17:57.052898: Epoch time: 6.71 s 
2025-03-11 15:17:57.596628:  
2025-03-11 15:17:57.602501: Epoch 95 
2025-03-11 15:17:57.606110: Current learning rate: 0.00067 
2025-03-11 15:18:04.315822: train_loss -0.9445 
2025-03-11 15:18:04.322455: val_loss -0.8165 
2025-03-11 15:18:04.325648: Pseudo dice [np.float32(0.8911), np.float32(0.8723)] 
2025-03-11 15:18:04.329254: Epoch time: 6.72 s 
2025-03-11 15:18:04.879178:  
2025-03-11 15:18:04.884401: Epoch 96 
2025-03-11 15:18:04.887528: Current learning rate: 0.00055 
2025-03-11 15:18:11.595657: train_loss -0.947 
2025-03-11 15:18:11.601738: val_loss -0.8208 
2025-03-11 15:18:11.605287: Pseudo dice [np.float32(0.8916), np.float32(0.8747)] 
2025-03-11 15:18:11.608864: Epoch time: 6.72 s 
2025-03-11 15:18:12.166744:  
2025-03-11 15:18:12.172465: Epoch 97 
2025-03-11 15:18:12.176511: Current learning rate: 0.00043 
2025-03-11 15:18:18.897610: train_loss -0.9454 
2025-03-11 15:18:18.902143: val_loss -0.8243 
2025-03-11 15:18:18.905208: Pseudo dice [np.float32(0.8941), np.float32(0.876)] 
2025-03-11 15:18:18.908771: Epoch time: 6.73 s 
2025-03-11 15:18:19.467007:  
2025-03-11 15:18:19.472614: Epoch 98 
2025-03-11 15:18:19.476183: Current learning rate: 0.0003 
2025-03-11 15:18:26.200476: train_loss -0.9462 
2025-03-11 15:18:26.206143: val_loss -0.8264 
2025-03-11 15:18:26.210187: Pseudo dice [np.float32(0.8959), np.float32(0.8795)] 
2025-03-11 15:18:26.213219: Epoch time: 6.73 s 
2025-03-11 15:18:26.777800:  
2025-03-11 15:18:26.783974: Epoch 99 
2025-03-11 15:18:26.787668: Current learning rate: 0.00016 
2025-03-11 15:18:33.518600: train_loss -0.9461 
2025-03-11 15:18:33.525155: val_loss -0.8247 
2025-03-11 15:18:33.528794: Pseudo dice [np.float32(0.895), np.float32(0.8774)] 
2025-03-11 15:18:33.531910: Epoch time: 6.74 s 
2025-03-11 15:18:34.141815: Training done. 
2025-03-11 15:18:34.179869: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-11 15:18:34.188986: The split file contains 5 splits. 
2025-03-11 15:18:34.194517: Desired fold for training: 0 
2025-03-11 15:18:34.198790: This split has 208 training and 52 validation cases. 
2025-03-11 15:18:34.203908: predicting hippocampus_017 
2025-03-11 15:18:34.209808: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-11 15:18:34.304639: predicting hippocampus_019 
2025-03-11 15:18:34.311780: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-11 15:18:34.350012: predicting hippocampus_033 
2025-03-11 15:18:34.356871: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-11 15:18:34.379517: predicting hippocampus_035 
2025-03-11 15:18:34.385442: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-11 15:18:34.409561: predicting hippocampus_037 
2025-03-11 15:18:34.416496: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-11 15:18:34.444849: predicting hippocampus_049 
2025-03-11 15:18:34.451383: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-11 15:18:34.474522: predicting hippocampus_052 
2025-03-11 15:18:34.481902: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-11 15:18:34.505105: predicting hippocampus_065 
2025-03-11 15:18:34.512246: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-11 15:18:34.537768: predicting hippocampus_083 
2025-03-11 15:18:34.544436: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-11 15:18:34.567525: predicting hippocampus_088 
2025-03-11 15:18:34.574822: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-11 15:18:38.229712: predicting hippocampus_090 
2025-03-11 15:18:38.239771: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-11 15:18:38.298404: predicting hippocampus_092 
2025-03-11 15:18:38.306470: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-11 15:18:38.350046: predicting hippocampus_095 
2025-03-11 15:18:38.357946: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-11 15:18:38.404518: predicting hippocampus_107 
2025-03-11 15:18:38.412212: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-11 15:18:38.464001: predicting hippocampus_108 
2025-03-11 15:18:38.474028: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-11 15:18:38.526802: predicting hippocampus_123 
2025-03-11 15:18:38.534952: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-11 15:18:38.566942: predicting hippocampus_125 
2025-03-11 15:18:38.578984: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-11 15:18:38.630969: predicting hippocampus_157 
2025-03-11 15:18:38.637942: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-11 15:18:38.672759: predicting hippocampus_164 
2025-03-11 15:18:38.680984: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-11 15:18:38.764472: predicting hippocampus_169 
2025-03-11 15:18:38.771504: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-11 15:18:38.798410: predicting hippocampus_175 
2025-03-11 15:18:38.803888: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-11 15:18:38.830407: predicting hippocampus_185 
2025-03-11 15:18:38.837055: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-11 15:18:38.865484: predicting hippocampus_190 
2025-03-11 15:18:38.872755: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-11 15:18:38.903386: predicting hippocampus_194 
2025-03-11 15:18:38.911608: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-11 15:18:38.944780: predicting hippocampus_204 
2025-03-11 15:18:38.951523: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-11 15:18:38.979669: predicting hippocampus_205 
2025-03-11 15:18:38.986505: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-11 15:18:39.013779: predicting hippocampus_210 
2025-03-11 15:18:39.020459: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-11 15:18:39.054877: predicting hippocampus_217 
2025-03-11 15:18:39.064016: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-11 15:18:39.094957: predicting hippocampus_219 
2025-03-11 15:18:39.101795: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-11 15:18:39.132354: predicting hippocampus_229 
2025-03-11 15:18:39.138734: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-11 15:18:39.172044: predicting hippocampus_244 
2025-03-11 15:18:39.178845: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-11 15:18:39.213593: predicting hippocampus_261 
2025-03-11 15:18:39.221053: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-11 15:18:39.281834: predicting hippocampus_264 
2025-03-11 15:18:39.289027: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-11 15:18:39.316455: predicting hippocampus_277 
2025-03-11 15:18:39.323420: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-11 15:18:39.370007: predicting hippocampus_280 
2025-03-11 15:18:39.379091: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-11 15:18:39.407075: predicting hippocampus_286 
2025-03-11 15:18:39.414075: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-11 15:18:39.459688: predicting hippocampus_288 
2025-03-11 15:18:39.468097: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-11 15:18:39.512805: predicting hippocampus_289 
2025-03-11 15:18:39.522371: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-11 15:18:39.550800: predicting hippocampus_296 
2025-03-11 15:18:39.557451: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-11 15:18:39.591502: predicting hippocampus_305 
2025-03-11 15:18:39.599927: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-11 15:18:39.629797: predicting hippocampus_308 
2025-03-11 15:18:39.636046: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-11 15:18:39.666069: predicting hippocampus_317 
2025-03-11 15:18:39.671894: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-11 15:18:39.703187: predicting hippocampus_327 
2025-03-11 15:18:39.709974: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-11 15:18:39.739617: predicting hippocampus_330 
2025-03-11 15:18:39.747594: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-11 15:18:39.784055: predicting hippocampus_332 
2025-03-11 15:18:39.790352: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-11 15:18:39.819573: predicting hippocampus_338 
2025-03-11 15:18:39.826050: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-11 15:18:39.873228: predicting hippocampus_349 
2025-03-11 15:18:39.880796: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-11 15:18:39.908368: predicting hippocampus_350 
2025-03-11 15:18:39.914946: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-11 15:18:39.943246: predicting hippocampus_356 
2025-03-11 15:18:39.952773: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-11 15:18:39.986174: predicting hippocampus_358 
2025-03-11 15:18:39.994620: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-11 15:18:40.033728: predicting hippocampus_374 
2025-03-11 15:18:40.042694: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-11 15:18:40.070770: predicting hippocampus_394 
2025-03-11 15:18:40.079989: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-11 15:18:44.047792: Validation complete 
2025-03-11 15:18:44.053566: Mean Validation Dice:  0.45033424325933247 
