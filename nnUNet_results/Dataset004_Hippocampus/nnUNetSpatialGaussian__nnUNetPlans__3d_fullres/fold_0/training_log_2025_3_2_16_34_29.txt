
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-02 16:34:29.422812: do_dummy_2d_data_aug: False 
2025-03-02 16:34:29.424812: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-02 16:34:29.430811: The split file contains 5 splits. 
2025-03-02 16:34:29.433811: Desired fold for training: 0 
2025-03-02 16:34:29.436811: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-02 16:34:35.705081: unpacking dataset... 
2025-03-02 16:34:35.899842: unpacking done... 
2025-03-02 16:34:37.355945:  
2025-03-02 16:34:37.360958: Epoch 0 
2025-03-02 16:34:37.364971: Current learning rate: 0.01 
2025-03-02 16:34:45.135957: train_loss -0.3185 
2025-03-02 16:34:45.140768: val_loss -0.6031 
2025-03-02 16:34:45.145844: Pseudo dice [np.float32(0.7183), np.float32(0.8095)] 
2025-03-02 16:34:45.149395: Epoch time: 7.78 s 
2025-03-02 16:34:45.152946: Yayy! New best EMA pseudo Dice: 0.7638999819755554 
2025-03-02 16:34:45.683836:  
2025-03-02 16:34:45.689912: Epoch 1 
2025-03-02 16:34:45.692965: Current learning rate: 0.00991 
2025-03-02 16:34:52.502619: train_loss -0.777 
2025-03-02 16:34:52.507766: val_loss -0.8286 
2025-03-02 16:34:52.512316: Pseudo dice [np.float32(0.8821), np.float32(0.8634)] 
2025-03-02 16:34:52.515828: Epoch time: 6.82 s 
2025-03-02 16:34:52.519336: Yayy! New best EMA pseudo Dice: 0.7748000025749207 
2025-03-02 16:34:53.097332:  
2025-03-02 16:34:53.102851: Epoch 2 
2025-03-02 16:34:53.106362: Current learning rate: 0.00982 
2025-03-02 16:34:59.876161: train_loss -0.8426 
2025-03-02 16:34:59.882237: val_loss -0.8435 
2025-03-02 16:34:59.886794: Pseudo dice [np.float32(0.8918), np.float32(0.8751)] 
2025-03-02 16:34:59.889833: Epoch time: 6.78 s 
2025-03-02 16:34:59.893869: Yayy! New best EMA pseudo Dice: 0.7857000231742859 
2025-03-02 16:35:00.496937:  
2025-03-02 16:35:00.502491: Epoch 3 
2025-03-02 16:35:00.505507: Current learning rate: 0.00973 
2025-03-02 16:35:07.289885: train_loss -0.8556 
2025-03-02 16:35:07.295407: val_loss -0.8431 
2025-03-02 16:35:07.299946: Pseudo dice [np.float32(0.8928), np.float32(0.8732)] 
2025-03-02 16:35:07.303481: Epoch time: 6.79 s 
2025-03-02 16:35:07.307206: Yayy! New best EMA pseudo Dice: 0.7954000234603882 
2025-03-02 16:35:07.898083:  
2025-03-02 16:35:07.903607: Epoch 4 
2025-03-02 16:35:07.906947: Current learning rate: 0.00964 
2025-03-02 16:35:14.660691: train_loss -0.8647 
2025-03-02 16:35:14.666260: val_loss -0.8461 
2025-03-02 16:35:14.670304: Pseudo dice [np.float32(0.8972), np.float32(0.8761)] 
2025-03-02 16:35:14.673829: Epoch time: 6.76 s 
2025-03-02 16:35:14.677398: Yayy! New best EMA pseudo Dice: 0.8044999837875366 
2025-03-02 16:35:15.429147:  
2025-03-02 16:35:15.435168: Epoch 5 
2025-03-02 16:35:15.438669: Current learning rate: 0.00955 
2025-03-02 16:35:22.195497: train_loss -0.8694 
2025-03-02 16:35:22.201567: val_loss -0.8472 
2025-03-02 16:35:22.207690: Pseudo dice [np.float32(0.8953), np.float32(0.8781)] 
2025-03-02 16:35:22.211311: Epoch time: 6.77 s 
2025-03-02 16:35:22.214856: Yayy! New best EMA pseudo Dice: 0.8127999901771545 
2025-03-02 16:35:22.806714:  
2025-03-02 16:35:22.812232: Epoch 6 
2025-03-02 16:35:22.815746: Current learning rate: 0.00946 
2025-03-02 16:35:29.588133: train_loss -0.8766 
2025-03-02 16:35:29.593284: val_loss -0.8454 
2025-03-02 16:35:29.597860: Pseudo dice [np.float32(0.8953), np.float32(0.8756)] 
2025-03-02 16:35:29.600923: Epoch time: 6.78 s 
2025-03-02 16:35:29.604990: Yayy! New best EMA pseudo Dice: 0.8199999928474426 
2025-03-02 16:35:30.197638:  
2025-03-02 16:35:30.203661: Epoch 7 
2025-03-02 16:35:30.207685: Current learning rate: 0.00937 
2025-03-02 16:35:36.954224: train_loss -0.8809 
2025-03-02 16:35:36.960806: val_loss -0.8466 
2025-03-02 16:35:36.964352: Pseudo dice [np.float32(0.896), np.float32(0.8769)] 
2025-03-02 16:35:36.968383: Epoch time: 6.76 s 
2025-03-02 16:35:36.971516: Yayy! New best EMA pseudo Dice: 0.82669997215271 
2025-03-02 16:35:37.576766:  
2025-03-02 16:35:37.582792: Epoch 8 
2025-03-02 16:35:37.586802: Current learning rate: 0.00928 
2025-03-02 16:35:44.352335: train_loss -0.8842 
2025-03-02 16:35:44.357412: val_loss -0.8427 
2025-03-02 16:35:44.361973: Pseudo dice [np.float32(0.8928), np.float32(0.877)] 
2025-03-02 16:35:44.365608: Epoch time: 6.78 s 
2025-03-02 16:35:44.369138: Yayy! New best EMA pseudo Dice: 0.8324999809265137 
2025-03-02 16:35:44.969858:  
2025-03-02 16:35:44.974887: Epoch 9 
2025-03-02 16:35:44.978739: Current learning rate: 0.00919 
2025-03-02 16:35:51.703432: train_loss -0.8868 
2025-03-02 16:35:51.709969: val_loss -0.8439 
2025-03-02 16:35:51.713484: Pseudo dice [np.float32(0.8955), np.float32(0.8743)] 
2025-03-02 16:35:51.717506: Epoch time: 6.73 s 
2025-03-02 16:35:51.721055: Yayy! New best EMA pseudo Dice: 0.8377000093460083 
2025-03-02 16:35:52.290118:  
2025-03-02 16:35:52.296138: Epoch 10 
2025-03-02 16:35:52.299646: Current learning rate: 0.0091 
2025-03-02 16:35:59.039075: train_loss -0.8919 
2025-03-02 16:35:59.046211: val_loss -0.8439 
2025-03-02 16:35:59.050266: Pseudo dice [np.float32(0.8942), np.float32(0.8766)] 
2025-03-02 16:35:59.054861: Epoch time: 6.75 s 
2025-03-02 16:35:59.057976: Yayy! New best EMA pseudo Dice: 0.8424999713897705 
2025-03-02 16:35:59.633617:  
2025-03-02 16:35:59.639156: Epoch 11 
2025-03-02 16:35:59.642729: Current learning rate: 0.009 
2025-03-02 16:36:06.415239: train_loss -0.8978 
2025-03-02 16:36:06.421320: val_loss -0.8451 
2025-03-02 16:36:06.426365: Pseudo dice [np.float32(0.8976), np.float32(0.8753)] 
2025-03-02 16:36:06.429895: Epoch time: 6.78 s 
2025-03-02 16:36:06.433946: Yayy! New best EMA pseudo Dice: 0.8468999862670898 
2025-03-02 16:36:07.184907:  
2025-03-02 16:36:07.190447: Epoch 12 
2025-03-02 16:36:07.194001: Current learning rate: 0.00891 
2025-03-02 16:36:13.955270: train_loss -0.8996 
2025-03-02 16:36:13.960951: val_loss -0.8458 
2025-03-02 16:36:13.966039: Pseudo dice [np.float32(0.8969), np.float32(0.8768)] 
2025-03-02 16:36:13.970074: Epoch time: 6.77 s 
2025-03-02 16:36:13.973715: Yayy! New best EMA pseudo Dice: 0.8508999943733215 
2025-03-02 16:36:14.550270:  
2025-03-02 16:36:14.555785: Epoch 13 
2025-03-02 16:36:14.560300: Current learning rate: 0.00882 
2025-03-02 16:36:21.314628: train_loss -0.9004 
2025-03-02 16:36:21.320208: val_loss -0.8433 
2025-03-02 16:36:21.324743: Pseudo dice [np.float32(0.8959), np.float32(0.8752)] 
2025-03-02 16:36:21.328273: Epoch time: 6.77 s 
2025-03-02 16:36:21.331840: Yayy! New best EMA pseudo Dice: 0.8543999791145325 
2025-03-02 16:36:21.926552:  
2025-03-02 16:36:21.932069: Epoch 14 
2025-03-02 16:36:21.936583: Current learning rate: 0.00873 
2025-03-02 16:36:28.678866: train_loss -0.9019 
2025-03-02 16:36:28.684413: val_loss -0.8425 
2025-03-02 16:36:28.689075: Pseudo dice [np.float32(0.8957), np.float32(0.8742)] 
2025-03-02 16:36:28.693317: Epoch time: 6.75 s 
2025-03-02 16:36:28.696883: Yayy! New best EMA pseudo Dice: 0.8574000000953674 
2025-03-02 16:36:29.297400:  
2025-03-02 16:36:29.303442: Epoch 15 
2025-03-02 16:36:29.307386: Current learning rate: 0.00864 
2025-03-02 16:36:36.069769: train_loss -0.9046 
2025-03-02 16:36:36.074910: val_loss -0.8423 
2025-03-02 16:36:36.078962: Pseudo dice [np.float32(0.8953), np.float32(0.8751)] 
2025-03-02 16:36:36.083543: Epoch time: 6.77 s 
2025-03-02 16:36:36.087100: Yayy! New best EMA pseudo Dice: 0.8601999878883362 
2025-03-02 16:36:36.683057:  
2025-03-02 16:36:36.689085: Epoch 16 
2025-03-02 16:36:36.693611: Current learning rate: 0.00855 
2025-03-02 16:36:43.418910: train_loss -0.9064 
2025-03-02 16:36:43.425941: val_loss -0.8473 
2025-03-02 16:36:43.429975: Pseudo dice [np.float32(0.8996), np.float32(0.8796)] 
2025-03-02 16:36:43.435111: Epoch time: 6.74 s 
2025-03-02 16:36:43.439423: Yayy! New best EMA pseudo Dice: 0.863099992275238 
2025-03-02 16:36:44.059517:  
2025-03-02 16:36:44.065103: Epoch 17 
2025-03-02 16:36:44.068694: Current learning rate: 0.00846 
2025-03-02 16:36:50.821622: train_loss -0.9042 
2025-03-02 16:36:50.828183: val_loss -0.8399 
2025-03-02 16:36:50.833316: Pseudo dice [np.float32(0.8957), np.float32(0.8726)] 
2025-03-02 16:36:50.837886: Epoch time: 6.76 s 
2025-03-02 16:36:50.842964: Yayy! New best EMA pseudo Dice: 0.8651999831199646 
2025-03-02 16:36:51.452134:  
2025-03-02 16:36:51.458761: Epoch 18 
2025-03-02 16:36:51.462798: Current learning rate: 0.00836 
2025-03-02 16:36:58.220480: train_loss -0.9082 
2025-03-02 16:36:58.227022: val_loss -0.8416 
2025-03-02 16:36:58.231571: Pseudo dice [np.float32(0.8953), np.float32(0.8756)] 
2025-03-02 16:36:58.235677: Epoch time: 6.77 s 
2025-03-02 16:36:58.239202: Yayy! New best EMA pseudo Dice: 0.8672999739646912 
2025-03-02 16:36:58.856070:  
2025-03-02 16:36:58.861587: Epoch 19 
2025-03-02 16:36:58.866604: Current learning rate: 0.00827 
2025-03-02 16:37:05.659337: train_loss -0.9099 
2025-03-02 16:37:05.665895: val_loss -0.8448 
2025-03-02 16:37:05.670028: Pseudo dice [np.float32(0.8983), np.float32(0.8789)] 
2025-03-02 16:37:05.674097: Epoch time: 6.8 s 
2025-03-02 16:37:05.678208: Yayy! New best EMA pseudo Dice: 0.8694000244140625 
2025-03-02 16:37:06.443204:  
2025-03-02 16:37:06.448729: Epoch 20 
2025-03-02 16:37:06.453238: Current learning rate: 0.00818 
2025-03-02 16:37:13.213419: train_loss -0.9108 
2025-03-02 16:37:13.220696: val_loss -0.8455 
2025-03-02 16:37:13.225352: Pseudo dice [np.float32(0.8987), np.float32(0.8782)] 
2025-03-02 16:37:13.229903: Epoch time: 6.77 s 
2025-03-02 16:37:13.233566: Yayy! New best EMA pseudo Dice: 0.8712999820709229 
2025-03-02 16:37:13.848496:  
2025-03-02 16:37:13.854012: Epoch 21 
2025-03-02 16:37:13.859025: Current learning rate: 0.00809 
2025-03-02 16:37:20.616130: train_loss -0.9126 
2025-03-02 16:37:20.622265: val_loss -0.8423 
2025-03-02 16:37:20.627497: Pseudo dice [np.float32(0.8968), np.float32(0.8769)] 
2025-03-02 16:37:20.632070: Epoch time: 6.77 s 
2025-03-02 16:37:20.636127: Yayy! New best EMA pseudo Dice: 0.8727999925613403 
2025-03-02 16:37:21.220477:  
2025-03-02 16:37:21.225992: Epoch 22 
2025-03-02 16:37:21.231008: Current learning rate: 0.008 
2025-03-02 16:37:27.996296: train_loss -0.9139 
2025-03-02 16:37:28.003412: val_loss -0.8432 
2025-03-02 16:37:28.009589: Pseudo dice [np.float32(0.8965), np.float32(0.8773)] 
2025-03-02 16:37:28.015177: Epoch time: 6.78 s 
2025-03-02 16:37:28.018715: Yayy! New best EMA pseudo Dice: 0.8743000030517578 
2025-03-02 16:37:28.601055:  
2025-03-02 16:37:28.606604: Epoch 23 
2025-03-02 16:37:28.611114: Current learning rate: 0.0079 
2025-03-02 16:37:35.368569: train_loss -0.9157 
2025-03-02 16:37:35.374624: val_loss -0.8412 
2025-03-02 16:37:35.378668: Pseudo dice [np.float32(0.8963), np.float32(0.8758)] 
2025-03-02 16:37:35.382195: Epoch time: 6.77 s 
2025-03-02 16:37:35.386256: Yayy! New best EMA pseudo Dice: 0.8754000067710876 
2025-03-02 16:37:35.969417:  
2025-03-02 16:37:35.974964: Epoch 24 
2025-03-02 16:37:35.979555: Current learning rate: 0.00781 
2025-03-02 16:37:42.744212: train_loss -0.915 
2025-03-02 16:37:42.749372: val_loss -0.8374 
2025-03-02 16:37:42.754921: Pseudo dice [np.float32(0.8944), np.float32(0.8735)] 
2025-03-02 16:37:42.758933: Epoch time: 6.77 s 
2025-03-02 16:37:42.763447: Yayy! New best EMA pseudo Dice: 0.8762999773025513 
2025-03-02 16:37:43.346434:  
2025-03-02 16:37:43.351987: Epoch 25 
2025-03-02 16:37:43.355626: Current learning rate: 0.00772 
2025-03-02 16:37:50.108618: train_loss -0.917 
2025-03-02 16:37:50.114557: val_loss -0.8388 
2025-03-02 16:37:50.118569: Pseudo dice [np.float32(0.894), np.float32(0.8748)] 
2025-03-02 16:37:50.123082: Epoch time: 6.76 s 
2025-03-02 16:37:50.126094: Yayy! New best EMA pseudo Dice: 0.8770999908447266 
2025-03-02 16:37:50.715736:  
2025-03-02 16:37:50.721291: Epoch 26 
2025-03-02 16:37:50.726388: Current learning rate: 0.00763 
2025-03-02 16:37:57.462118: train_loss -0.9168 
2025-03-02 16:37:57.468200: val_loss -0.8409 
2025-03-02 16:37:57.472726: Pseudo dice [np.float32(0.8959), np.float32(0.8774)] 
2025-03-02 16:37:57.476818: Epoch time: 6.75 s 
2025-03-02 16:37:57.479849: Yayy! New best EMA pseudo Dice: 0.8780999779701233 
2025-03-02 16:37:58.052931:  
2025-03-02 16:37:58.059064: Epoch 27 
2025-03-02 16:37:58.063133: Current learning rate: 0.00753 
2025-03-02 16:38:04.825413: train_loss -0.9182 
2025-03-02 16:38:04.832606: val_loss -0.8379 
2025-03-02 16:38:04.837115: Pseudo dice [np.float32(0.8945), np.float32(0.8757)] 
2025-03-02 16:38:04.843707: Epoch time: 6.77 s 
2025-03-02 16:38:04.848890: Yayy! New best EMA pseudo Dice: 0.8787999749183655 
2025-03-02 16:38:05.586841:  
2025-03-02 16:38:05.592385: Epoch 28 
2025-03-02 16:38:05.596846: Current learning rate: 0.00744 
2025-03-02 16:38:12.323651: train_loss -0.919 
2025-03-02 16:38:12.330235: val_loss -0.8474 
2025-03-02 16:38:12.335276: Pseudo dice [np.float32(0.9005), np.float32(0.8806)] 
2025-03-02 16:38:12.340075: Epoch time: 6.74 s 
2025-03-02 16:38:12.344120: Yayy! New best EMA pseudo Dice: 0.8798999786376953 
2025-03-02 16:38:12.931091:  
2025-03-02 16:38:12.936611: Epoch 29 
2025-03-02 16:38:12.940489: Current learning rate: 0.00735 
2025-03-02 16:38:19.665429: train_loss -0.9204 
2025-03-02 16:38:19.672532: val_loss -0.8506 
2025-03-02 16:38:19.677056: Pseudo dice [np.float32(0.9028), np.float32(0.8849)] 
2025-03-02 16:38:19.680621: Epoch time: 6.74 s 
2025-03-02 16:38:19.685158: Yayy! New best EMA pseudo Dice: 0.8812999725341797 
2025-03-02 16:38:20.285874:  
2025-03-02 16:38:20.291466: Epoch 30 
2025-03-02 16:38:20.295539: Current learning rate: 0.00725 
2025-03-02 16:38:27.051871: train_loss -0.9211 
2025-03-02 16:38:27.057960: val_loss -0.8479 
2025-03-02 16:38:27.062034: Pseudo dice [np.float32(0.9001), np.float32(0.8829)] 
2025-03-02 16:38:27.066086: Epoch time: 6.77 s 
2025-03-02 16:38:27.070187: Yayy! New best EMA pseudo Dice: 0.8823000192642212 
2025-03-02 16:38:27.652448:  
2025-03-02 16:38:27.658478: Epoch 31 
2025-03-02 16:38:27.662518: Current learning rate: 0.00716 
2025-03-02 16:38:34.415468: train_loss -0.9225 
2025-03-02 16:38:34.421529: val_loss -0.837 
2025-03-02 16:38:34.425631: Pseudo dice [np.float32(0.8936), np.float32(0.8754)] 
2025-03-02 16:38:34.429166: Epoch time: 6.76 s 
2025-03-02 16:38:34.433744: Yayy! New best EMA pseudo Dice: 0.8826000094413757 
2025-03-02 16:38:35.029672:  
2025-03-02 16:38:35.035230: Epoch 32 
2025-03-02 16:38:35.040372: Current learning rate: 0.00707 
2025-03-02 16:38:41.806077: train_loss -0.9225 
2025-03-02 16:38:41.813310: val_loss -0.8395 
2025-03-02 16:38:41.817820: Pseudo dice [np.float32(0.8953), np.float32(0.8776)] 
2025-03-02 16:38:41.820830: Epoch time: 6.78 s 
2025-03-02 16:38:41.824341: Yayy! New best EMA pseudo Dice: 0.8828999996185303 
2025-03-02 16:38:42.417660:  
2025-03-02 16:38:42.423210: Epoch 33 
2025-03-02 16:38:42.427807: Current learning rate: 0.00697 
2025-03-02 16:38:49.177230: train_loss -0.9241 
2025-03-02 16:38:49.182039: val_loss -0.836 
2025-03-02 16:38:49.188432: Pseudo dice [np.float32(0.8927), np.float32(0.8751)] 
2025-03-02 16:38:49.192512: Epoch time: 6.76 s 
2025-03-02 16:38:49.196554: Yayy! New best EMA pseudo Dice: 0.8830000162124634 
2025-03-02 16:38:49.783814:  
2025-03-02 16:38:49.789330: Epoch 34 
2025-03-02 16:38:49.793845: Current learning rate: 0.00688 
2025-03-02 16:38:56.566478: train_loss -0.9221 
2025-03-02 16:38:56.572566: val_loss -0.8428 
2025-03-02 16:38:56.576621: Pseudo dice [np.float32(0.8973), np.float32(0.8803)] 
2025-03-02 16:38:56.580759: Epoch time: 6.78 s 
2025-03-02 16:38:56.584772: Yayy! New best EMA pseudo Dice: 0.8835999965667725 
2025-03-02 16:38:57.297812:  
2025-03-02 16:38:57.303852: Epoch 35 
2025-03-02 16:38:57.307899: Current learning rate: 0.00679 
2025-03-02 16:39:04.163104: train_loss -0.9246 
2025-03-02 16:39:04.170123: val_loss -0.8396 
2025-03-02 16:39:04.174776: Pseudo dice [np.float32(0.8971), np.float32(0.8775)] 
2025-03-02 16:39:04.178842: Epoch time: 6.87 s 
2025-03-02 16:39:04.181906: Yayy! New best EMA pseudo Dice: 0.8840000033378601 
2025-03-02 16:39:04.986531:  
2025-03-02 16:39:04.992551: Epoch 36 
2025-03-02 16:39:04.996565: Current learning rate: 0.00669 
2025-03-02 16:39:11.876252: train_loss -0.9249 
2025-03-02 16:39:11.882553: val_loss -0.8379 
2025-03-02 16:39:11.888588: Pseudo dice [np.float32(0.8964), np.float32(0.8767)] 
2025-03-02 16:39:11.894144: Epoch time: 6.89 s 
2025-03-02 16:39:11.899072: Yayy! New best EMA pseudo Dice: 0.8841999769210815 
2025-03-02 16:39:12.558189:  
2025-03-02 16:39:12.563739: Epoch 37 
2025-03-02 16:39:12.568818: Current learning rate: 0.0066 
2025-03-02 16:39:19.352362: train_loss -0.9253 
2025-03-02 16:39:19.358488: val_loss -0.8362 
2025-03-02 16:39:19.363637: Pseudo dice [np.float32(0.8942), np.float32(0.8762)] 
2025-03-02 16:39:19.367182: Epoch time: 6.79 s 
2025-03-02 16:39:19.370857: Yayy! New best EMA pseudo Dice: 0.8842999935150146 
2025-03-02 16:39:19.973376:  
2025-03-02 16:39:19.979396: Epoch 38 
2025-03-02 16:39:19.983402: Current learning rate: 0.0065 
2025-03-02 16:39:26.736710: train_loss -0.9266 
2025-03-02 16:39:26.743288: val_loss -0.8389 
2025-03-02 16:39:26.748412: Pseudo dice [np.float32(0.8972), np.float32(0.8769)] 
2025-03-02 16:39:26.752449: Epoch time: 6.76 s 
2025-03-02 16:39:26.758527: Yayy! New best EMA pseudo Dice: 0.8845999836921692 
2025-03-02 16:39:27.415882:  
2025-03-02 16:39:27.421901: Epoch 39 
2025-03-02 16:39:27.425911: Current learning rate: 0.00641 
2025-03-02 16:39:34.342789: train_loss -0.9265 
2025-03-02 16:39:34.348411: val_loss -0.8366 
2025-03-02 16:39:34.468735: Pseudo dice [np.float32(0.8959), np.float32(0.8758)] 
2025-03-02 16:39:34.475371: Epoch time: 6.93 s 
2025-03-02 16:39:34.479522: Yayy! New best EMA pseudo Dice: 0.8847000002861023 
2025-03-02 16:39:35.232727:  
2025-03-02 16:39:35.237382: Epoch 40 
2025-03-02 16:39:35.241938: Current learning rate: 0.00631 
2025-03-02 16:39:42.731767: train_loss -0.9294 
2025-03-02 16:39:42.736869: val_loss -0.8413 
2025-03-02 16:39:42.741461: Pseudo dice [np.float32(0.8969), np.float32(0.8802)] 
2025-03-02 16:39:42.746116: Epoch time: 7.5 s 
2025-03-02 16:39:42.750656: Yayy! New best EMA pseudo Dice: 0.8851000070571899 
2025-03-02 16:39:43.523241:  
2025-03-02 16:39:43.528809: Epoch 41 
2025-03-02 16:39:43.531354: Current learning rate: 0.00622 
2025-03-02 16:39:50.465065: train_loss -0.9293 
2025-03-02 16:39:50.470630: val_loss -0.8413 
2025-03-02 16:39:50.475672: Pseudo dice [np.float32(0.8984), np.float32(0.8794)] 
2025-03-02 16:39:50.479701: Epoch time: 6.94 s 
2025-03-02 16:39:50.483239: Yayy! New best EMA pseudo Dice: 0.8855000138282776 
2025-03-02 16:39:51.208642:  
2025-03-02 16:39:51.214714: Epoch 42 
2025-03-02 16:39:51.217785: Current learning rate: 0.00612 
2025-03-02 16:39:58.121633: train_loss -0.9286 
2025-03-02 16:39:58.127740: val_loss -0.8391 
2025-03-02 16:39:58.132343: Pseudo dice [np.float32(0.8968), np.float32(0.8776)] 
2025-03-02 16:39:58.136389: Epoch time: 6.91 s 
2025-03-02 16:39:58.139953: Yayy! New best EMA pseudo Dice: 0.885699987411499 
2025-03-02 16:39:59.023086:  
2025-03-02 16:39:59.028106: Epoch 43 
2025-03-02 16:39:59.032619: Current learning rate: 0.00603 
2025-03-02 16:40:05.907184: train_loss -0.9291 
2025-03-02 16:40:05.913900: val_loss -0.8424 
2025-03-02 16:40:05.917506: Pseudo dice [np.float32(0.8989), np.float32(0.8812)] 
2025-03-02 16:40:05.921556: Epoch time: 6.88 s 
2025-03-02 16:40:05.925613: Yayy! New best EMA pseudo Dice: 0.8860999941825867 
2025-03-02 16:40:06.906060:  
2025-03-02 16:40:06.911619: Epoch 44 
2025-03-02 16:40:06.914776: Current learning rate: 0.00593 
2025-03-02 16:40:13.696674: train_loss -0.9295 
2025-03-02 16:40:13.703246: val_loss -0.837 
2025-03-02 16:40:13.707320: Pseudo dice [np.float32(0.895), np.float32(0.8786)] 
2025-03-02 16:40:13.711869: Epoch time: 6.79 s 
2025-03-02 16:40:13.715503: Yayy! New best EMA pseudo Dice: 0.8862000107765198 
2025-03-02 16:40:14.329832:  
2025-03-02 16:40:14.335966: Epoch 45 
2025-03-02 16:40:14.339533: Current learning rate: 0.00584 
2025-03-02 16:40:21.105370: train_loss -0.9312 
2025-03-02 16:40:21.111449: val_loss -0.8383 
2025-03-02 16:40:21.115026: Pseudo dice [np.float32(0.8972), np.float32(0.8769)] 
2025-03-02 16:40:21.119123: Epoch time: 6.78 s 
2025-03-02 16:40:21.122679: Yayy! New best EMA pseudo Dice: 0.8863000273704529 
2025-03-02 16:40:21.697613:  
2025-03-02 16:40:21.703171: Epoch 46 
2025-03-02 16:40:21.707270: Current learning rate: 0.00574 
2025-03-02 16:40:28.485382: train_loss -0.931 
2025-03-02 16:40:28.490971: val_loss -0.8372 
2025-03-02 16:40:28.495018: Pseudo dice [np.float32(0.8962), np.float32(0.8797)] 
2025-03-02 16:40:28.499039: Epoch time: 6.79 s 
2025-03-02 16:40:28.503225: Yayy! New best EMA pseudo Dice: 0.8863999843597412 
2025-03-02 16:40:29.143113:  
2025-03-02 16:40:29.149684: Epoch 47 
2025-03-02 16:40:29.152273: Current learning rate: 0.00565 
2025-03-02 16:40:35.969750: train_loss -0.9329 
2025-03-02 16:40:35.974835: val_loss -0.8349 
2025-03-02 16:40:35.977969: Pseudo dice [np.float32(0.8946), np.float32(0.8754)] 
2025-03-02 16:40:35.982059: Epoch time: 6.83 s 
2025-03-02 16:40:36.564234:  
2025-03-02 16:40:36.570281: Epoch 48 
2025-03-02 16:40:36.572799: Current learning rate: 0.00555 
2025-03-02 16:40:43.342585: train_loss -0.9339 
2025-03-02 16:40:43.349183: val_loss -0.8309 
2025-03-02 16:40:43.352807: Pseudo dice [np.float32(0.8937), np.float32(0.8743)] 
2025-03-02 16:40:43.356368: Epoch time: 6.78 s 
2025-03-02 16:40:43.899822:  
2025-03-02 16:40:43.905371: Epoch 49 
2025-03-02 16:40:43.908452: Current learning rate: 0.00546 
2025-03-02 16:40:50.674011: train_loss -0.933 
2025-03-02 16:40:50.680171: val_loss -0.8337 
2025-03-02 16:40:50.684216: Pseudo dice [np.float32(0.8952), np.float32(0.876)] 
2025-03-02 16:40:50.687782: Epoch time: 6.77 s 
2025-03-02 16:40:51.279960:  
2025-03-02 16:40:51.285264: Epoch 50 
2025-03-02 16:40:51.288783: Current learning rate: 0.00536 
2025-03-02 16:40:58.077048: train_loss -0.9338 
2025-03-02 16:40:58.083237: val_loss -0.8416 
2025-03-02 16:40:58.086828: Pseudo dice [np.float32(0.8985), np.float32(0.8801)] 
2025-03-02 16:40:58.091426: Epoch time: 6.8 s 
2025-03-02 16:40:58.837330:  
2025-03-02 16:40:58.843851: Epoch 51 
2025-03-02 16:40:58.848361: Current learning rate: 0.00526 
2025-03-02 16:41:05.664829: train_loss -0.9344 
2025-03-02 16:41:05.669879: val_loss -0.8329 
2025-03-02 16:41:05.676436: Pseudo dice [np.float32(0.894), np.float32(0.8765)] 
2025-03-02 16:41:05.681542: Epoch time: 6.83 s 
2025-03-02 16:41:06.278242:  
2025-03-02 16:41:06.284312: Epoch 52 
2025-03-02 16:41:06.287374: Current learning rate: 0.00517 
2025-03-02 16:41:13.059790: train_loss -0.9345 
2025-03-02 16:41:13.065816: val_loss -0.8417 
2025-03-02 16:41:13.069853: Pseudo dice [np.float32(0.8996), np.float32(0.8818)] 
2025-03-02 16:41:13.073397: Epoch time: 6.78 s 
2025-03-02 16:41:13.076461: Yayy! New best EMA pseudo Dice: 0.8866999745368958 
2025-03-02 16:41:13.681568:  
2025-03-02 16:41:13.689286: Epoch 53 
2025-03-02 16:41:13.692316: Current learning rate: 0.00507 
2025-03-02 16:41:20.434011: train_loss -0.9345 
2025-03-02 16:41:20.439381: val_loss -0.8379 
2025-03-02 16:41:20.443939: Pseudo dice [np.float32(0.8979), np.float32(0.8786)] 
2025-03-02 16:41:20.447454: Epoch time: 6.75 s 
2025-03-02 16:41:20.450971: Yayy! New best EMA pseudo Dice: 0.8867999911308289 
2025-03-02 16:41:21.045110:  
2025-03-02 16:41:21.050687: Epoch 54 
2025-03-02 16:41:21.054740: Current learning rate: 0.00497 
2025-03-02 16:41:27.807464: train_loss -0.936 
2025-03-02 16:41:27.813298: val_loss -0.8402 
2025-03-02 16:41:27.816805: Pseudo dice [np.float32(0.8984), np.float32(0.8791)] 
2025-03-02 16:41:27.819941: Epoch time: 6.76 s 
2025-03-02 16:41:27.823453: Yayy! New best EMA pseudo Dice: 0.8870000243186951 
2025-03-02 16:41:28.435522:  
2025-03-02 16:41:28.441061: Epoch 55 
2025-03-02 16:41:28.444605: Current learning rate: 0.00487 
2025-03-02 16:41:35.281661: train_loss -0.9353 
2025-03-02 16:41:35.287738: val_loss -0.8309 
2025-03-02 16:41:35.291857: Pseudo dice [np.float32(0.894), np.float32(0.8745)] 
2025-03-02 16:41:35.295429: Epoch time: 6.85 s 
2025-03-02 16:41:35.885753:  
2025-03-02 16:41:35.891294: Epoch 56 
2025-03-02 16:41:35.894840: Current learning rate: 0.00478 
2025-03-02 16:41:42.684371: train_loss -0.9346 
2025-03-02 16:41:42.690034: val_loss -0.8384 
2025-03-02 16:41:42.694948: Pseudo dice [np.float32(0.8971), np.float32(0.8791)] 
2025-03-02 16:41:42.698968: Epoch time: 6.8 s 
2025-03-02 16:41:43.283669:  
2025-03-02 16:41:43.290195: Epoch 57 
2025-03-02 16:41:43.294212: Current learning rate: 0.00468 
2025-03-02 16:41:50.076208: train_loss -0.9358 
2025-03-02 16:41:50.082804: val_loss -0.8345 
2025-03-02 16:41:50.087387: Pseudo dice [np.float32(0.8943), np.float32(0.8775)] 
2025-03-02 16:41:50.091558: Epoch time: 6.79 s 
2025-03-02 16:41:50.676769:  
2025-03-02 16:41:50.683460: Epoch 58 
2025-03-02 16:41:50.687525: Current learning rate: 0.00458 
2025-03-02 16:41:57.524322: train_loss -0.9367 
2025-03-02 16:41:57.530403: val_loss -0.8329 
2025-03-02 16:41:57.534486: Pseudo dice [np.float32(0.8947), np.float32(0.8769)] 
2025-03-02 16:41:57.538056: Epoch time: 6.85 s 
2025-03-02 16:41:58.315023:  
2025-03-02 16:41:58.320583: Epoch 59 
2025-03-02 16:41:58.325148: Current learning rate: 0.00448 
2025-03-02 16:42:05.149861: train_loss -0.9359 
2025-03-02 16:42:05.155421: val_loss -0.83 
2025-03-02 16:42:05.159464: Pseudo dice [np.float32(0.892), np.float32(0.8747)] 
2025-03-02 16:42:05.163073: Epoch time: 6.84 s 
2025-03-02 16:42:05.811187:  
2025-03-02 16:42:05.817780: Epoch 60 
2025-03-02 16:42:05.820388: Current learning rate: 0.00438 
2025-03-02 16:42:12.623056: train_loss -0.9359 
2025-03-02 16:42:12.629122: val_loss -0.8357 
2025-03-02 16:42:12.632656: Pseudo dice [np.float32(0.8961), np.float32(0.8789)] 
2025-03-02 16:42:12.639097: Epoch time: 6.81 s 
2025-03-02 16:42:13.238805:  
2025-03-02 16:42:13.244323: Epoch 61 
2025-03-02 16:42:13.247831: Current learning rate: 0.00429 
2025-03-02 16:42:20.041523: train_loss -0.9382 
2025-03-02 16:42:20.047183: val_loss -0.8303 
2025-03-02 16:42:20.051840: Pseudo dice [np.float32(0.8949), np.float32(0.875)] 
2025-03-02 16:42:20.054486: Epoch time: 6.8 s 
2025-03-02 16:42:20.651045:  
2025-03-02 16:42:20.657634: Epoch 62 
2025-03-02 16:42:20.660190: Current learning rate: 0.00419 
2025-03-02 16:42:27.437285: train_loss -0.938 
2025-03-02 16:42:27.443875: val_loss -0.8382 
2025-03-02 16:42:27.447935: Pseudo dice [np.float32(0.8978), np.float32(0.879)] 
2025-03-02 16:42:27.452058: Epoch time: 6.79 s 
2025-03-02 16:42:28.052278:  
2025-03-02 16:42:28.057832: Epoch 63 
2025-03-02 16:42:28.061908: Current learning rate: 0.00409 
2025-03-02 16:42:34.874938: train_loss -0.937 
2025-03-02 16:42:34.881526: val_loss -0.8368 
2025-03-02 16:42:34.885585: Pseudo dice [np.float32(0.8966), np.float32(0.8783)] 
2025-03-02 16:42:34.889215: Epoch time: 6.82 s 
2025-03-02 16:42:35.478185:  
2025-03-02 16:42:35.484201: Epoch 64 
2025-03-02 16:42:35.487213: Current learning rate: 0.00399 
2025-03-02 16:42:42.275100: train_loss -0.939 
2025-03-02 16:42:42.280709: val_loss -0.8416 
2025-03-02 16:42:42.283249: Pseudo dice [np.float32(0.9006), np.float32(0.8813)] 
2025-03-02 16:42:42.287913: Epoch time: 6.8 s 
2025-03-02 16:42:42.291446: Yayy! New best EMA pseudo Dice: 0.8870000243186951 
2025-03-02 16:42:42.961321:  
2025-03-02 16:42:42.966838: Epoch 65 
2025-03-02 16:42:42.970350: Current learning rate: 0.00389 
2025-03-02 16:42:49.786335: train_loss -0.9372 
2025-03-02 16:42:49.791912: val_loss -0.8336 
2025-03-02 16:42:49.794974: Pseudo dice [np.float32(0.8949), np.float32(0.8789)] 
2025-03-02 16:42:49.799614: Epoch time: 6.83 s 
2025-03-02 16:42:50.413966:  
2025-03-02 16:42:50.419484: Epoch 66 
2025-03-02 16:42:50.422995: Current learning rate: 0.00379 
2025-03-02 16:42:57.246338: train_loss -0.9402 
2025-03-02 16:42:57.253056: val_loss -0.8324 
2025-03-02 16:42:57.257095: Pseudo dice [np.float32(0.8951), np.float32(0.8764)] 
2025-03-02 16:42:57.260141: Epoch time: 6.83 s 
2025-03-02 16:42:58.027151:  
2025-03-02 16:42:58.032706: Epoch 67 
2025-03-02 16:42:58.036284: Current learning rate: 0.00369 
2025-03-02 16:43:04.875412: train_loss -0.9386 
2025-03-02 16:43:04.882022: val_loss -0.8289 
2025-03-02 16:43:04.886104: Pseudo dice [np.float32(0.893), np.float32(0.8751)] 
2025-03-02 16:43:04.890671: Epoch time: 6.85 s 
2025-03-02 16:43:05.547925:  
2025-03-02 16:43:05.552938: Epoch 68 
2025-03-02 16:43:05.557450: Current learning rate: 0.00359 
2025-03-02 16:43:12.342544: train_loss -0.9401 
2025-03-02 16:43:12.349643: val_loss -0.8341 
2025-03-02 16:43:12.353684: Pseudo dice [np.float32(0.8972), np.float32(0.878)] 
2025-03-02 16:43:12.357745: Epoch time: 6.8 s 
2025-03-02 16:43:12.989376:  
2025-03-02 16:43:12.994936: Epoch 69 
2025-03-02 16:43:12.999041: Current learning rate: 0.00349 
2025-03-02 16:43:19.796395: train_loss -0.9405 
2025-03-02 16:43:19.802486: val_loss -0.8326 
2025-03-02 16:43:19.806547: Pseudo dice [np.float32(0.8957), np.float32(0.876)] 
2025-03-02 16:43:19.810186: Epoch time: 6.81 s 
2025-03-02 16:43:20.416548:  
2025-03-02 16:43:20.423065: Epoch 70 
2025-03-02 16:43:20.426577: Current learning rate: 0.00338 
2025-03-02 16:43:27.260544: train_loss -0.94 
2025-03-02 16:43:27.267161: val_loss -0.8353 
2025-03-02 16:43:27.271231: Pseudo dice [np.float32(0.8979), np.float32(0.8784)] 
2025-03-02 16:43:27.274887: Epoch time: 6.84 s 
2025-03-02 16:43:27.891313:  
2025-03-02 16:43:27.897397: Epoch 71 
2025-03-02 16:43:27.901000: Current learning rate: 0.00328 
2025-03-02 16:43:34.747777: train_loss -0.9421 
2025-03-02 16:43:34.753364: val_loss -0.8268 
2025-03-02 16:43:34.757509: Pseudo dice [np.float32(0.8935), np.float32(0.8741)] 
2025-03-02 16:43:34.760547: Epoch time: 6.86 s 
2025-03-02 16:43:35.379212:  
2025-03-02 16:43:35.384227: Epoch 72 
2025-03-02 16:43:35.388241: Current learning rate: 0.00318 
2025-03-02 16:43:42.163390: train_loss -0.9425 
2025-03-02 16:43:42.171017: val_loss -0.8364 
2025-03-02 16:43:42.175145: Pseudo dice [np.float32(0.8981), np.float32(0.8789)] 
2025-03-02 16:43:42.178699: Epoch time: 6.78 s 
2025-03-02 16:43:42.798014:  
2025-03-02 16:43:42.803512: Epoch 73 
2025-03-02 16:43:42.807019: Current learning rate: 0.00308 
2025-03-02 16:43:49.620880: train_loss -0.9412 
2025-03-02 16:43:49.627692: val_loss -0.8308 
2025-03-02 16:43:49.631776: Pseudo dice [np.float32(0.8956), np.float32(0.8771)] 
2025-03-02 16:43:49.634893: Epoch time: 6.82 s 
2025-03-02 16:43:50.237946:  
2025-03-02 16:43:50.243508: Epoch 74 
2025-03-02 16:43:50.246052: Current learning rate: 0.00297 
2025-03-02 16:43:57.032883: train_loss -0.9423 
2025-03-02 16:43:57.038441: val_loss -0.8273 
2025-03-02 16:43:57.041995: Pseudo dice [np.float32(0.8943), np.float32(0.875)] 
2025-03-02 16:43:57.046034: Epoch time: 6.8 s 
2025-03-02 16:43:57.837163:  
2025-03-02 16:43:57.842674: Epoch 75 
2025-03-02 16:43:57.846183: Current learning rate: 0.00287 
2025-03-02 16:44:04.651861: train_loss -0.9414 
2025-03-02 16:44:04.658033: val_loss -0.8355 
2025-03-02 16:44:04.661610: Pseudo dice [np.float32(0.8971), np.float32(0.8804)] 
2025-03-02 16:44:04.665169: Epoch time: 6.82 s 
2025-03-02 16:44:05.284250:  
2025-03-02 16:44:05.290290: Epoch 76 
2025-03-02 16:44:05.293326: Current learning rate: 0.00277 
2025-03-02 16:44:12.117986: train_loss -0.9431 
2025-03-02 16:44:12.123550: val_loss -0.8319 
2025-03-02 16:44:12.127120: Pseudo dice [np.float32(0.8961), np.float32(0.8766)] 
2025-03-02 16:44:12.131167: Epoch time: 6.83 s 
2025-03-02 16:44:12.742195:  
2025-03-02 16:44:12.747840: Epoch 77 
2025-03-02 16:44:12.752397: Current learning rate: 0.00266 
2025-03-02 16:44:19.597765: train_loss -0.9437 
2025-03-02 16:44:19.602821: val_loss -0.8308 
2025-03-02 16:44:19.607418: Pseudo dice [np.float32(0.8959), np.float32(0.8767)] 
2025-03-02 16:44:19.610428: Epoch time: 6.86 s 
2025-03-02 16:44:20.218246:  
2025-03-02 16:44:20.224310: Epoch 78 
2025-03-02 16:44:20.227347: Current learning rate: 0.00256 
2025-03-02 16:44:26.950097: train_loss -0.9428 
2025-03-02 16:44:26.956693: val_loss -0.8317 
2025-03-02 16:44:26.959706: Pseudo dice [np.float32(0.8953), np.float32(0.8783)] 
2025-03-02 16:44:26.963729: Epoch time: 6.73 s 
2025-03-02 16:44:27.572583:  
2025-03-02 16:44:27.578106: Epoch 79 
2025-03-02 16:44:27.581619: Current learning rate: 0.00245 
2025-03-02 16:44:34.294604: train_loss -0.9438 
2025-03-02 16:44:34.300153: val_loss -0.8298 
2025-03-02 16:44:34.304796: Pseudo dice [np.float32(0.8955), np.float32(0.876)] 
2025-03-02 16:44:34.308828: Epoch time: 6.72 s 
2025-03-02 16:44:34.925647:  
2025-03-02 16:44:34.930684: Epoch 80 
2025-03-02 16:44:34.933663: Current learning rate: 0.00235 
2025-03-02 16:44:41.658192: train_loss -0.9434 
2025-03-02 16:44:41.666224: val_loss -0.8343 
2025-03-02 16:44:41.670299: Pseudo dice [np.float32(0.8976), np.float32(0.8783)] 
2025-03-02 16:44:41.672839: Epoch time: 6.73 s 
2025-03-02 16:44:42.286661:  
2025-03-02 16:44:42.292178: Epoch 81 
2025-03-02 16:44:42.294684: Current learning rate: 0.00224 
2025-03-02 16:44:49.012839: train_loss -0.9417 
2025-03-02 16:44:49.018976: val_loss -0.8313 
2025-03-02 16:44:49.022597: Pseudo dice [np.float32(0.8954), np.float32(0.876)] 
2025-03-02 16:44:49.027178: Epoch time: 6.73 s 
2025-03-02 16:44:49.796411:  
2025-03-02 16:44:49.801932: Epoch 82 
2025-03-02 16:44:49.805447: Current learning rate: 0.00214 
2025-03-02 16:44:56.518785: train_loss -0.9432 
2025-03-02 16:44:56.524911: val_loss -0.8338 
2025-03-02 16:44:56.528469: Pseudo dice [np.float32(0.8981), np.float32(0.8791)] 
2025-03-02 16:44:56.532553: Epoch time: 6.72 s 
2025-03-02 16:44:57.116626:  
2025-03-02 16:44:57.121165: Epoch 83 
2025-03-02 16:44:57.124770: Current learning rate: 0.00203 
2025-03-02 16:45:03.849334: train_loss -0.9438 
2025-03-02 16:45:03.855924: val_loss -0.8313 
2025-03-02 16:45:03.860593: Pseudo dice [np.float32(0.8951), np.float32(0.8769)] 
2025-03-02 16:45:03.864175: Epoch time: 6.73 s 
2025-03-02 16:45:04.435978:  
2025-03-02 16:45:04.441533: Epoch 84 
2025-03-02 16:45:04.444583: Current learning rate: 0.00192 
2025-03-02 16:45:11.150745: train_loss -0.946 
2025-03-02 16:45:11.157292: val_loss -0.8305 
2025-03-02 16:45:11.160863: Pseudo dice [np.float32(0.895), np.float32(0.877)] 
2025-03-02 16:45:11.163392: Epoch time: 6.72 s 
2025-03-02 16:45:11.747511:  
2025-03-02 16:45:11.752530: Epoch 85 
2025-03-02 16:45:11.755544: Current learning rate: 0.00181 
2025-03-02 16:45:18.476790: train_loss -0.9452 
2025-03-02 16:45:18.482447: val_loss -0.8339 
2025-03-02 16:45:18.486495: Pseudo dice [np.float32(0.8974), np.float32(0.8784)] 
2025-03-02 16:45:18.490055: Epoch time: 6.73 s 
2025-03-02 16:45:19.059332:  
2025-03-02 16:45:19.064347: Epoch 86 
2025-03-02 16:45:19.067858: Current learning rate: 0.0017 
2025-03-02 16:45:25.772456: train_loss -0.945 
2025-03-02 16:45:25.777593: val_loss -0.8324 
2025-03-02 16:45:25.780688: Pseudo dice [np.float32(0.8968), np.float32(0.8784)] 
2025-03-02 16:45:25.784791: Epoch time: 6.71 s 
2025-03-02 16:45:26.398004:  
2025-03-02 16:45:26.402025: Epoch 87 
2025-03-02 16:45:26.405535: Current learning rate: 0.00159 
2025-03-02 16:45:33.120937: train_loss -0.9452 
2025-03-02 16:45:33.127067: val_loss -0.828 
2025-03-02 16:45:33.130212: Pseudo dice [np.float32(0.8961), np.float32(0.8754)] 
2025-03-02 16:45:33.134236: Epoch time: 6.72 s 
2025-03-02 16:45:33.715732:  
2025-03-02 16:45:33.721779: Epoch 88 
2025-03-02 16:45:33.725302: Current learning rate: 0.00148 
2025-03-02 16:45:40.444403: train_loss -0.9469 
2025-03-02 16:45:40.450976: val_loss -0.8331 
2025-03-02 16:45:40.454551: Pseudo dice [np.float32(0.8966), np.float32(0.8783)] 
2025-03-02 16:45:40.457590: Epoch time: 6.73 s 
2025-03-02 16:45:41.032794:  
2025-03-02 16:45:41.039457: Epoch 89 
2025-03-02 16:45:41.045002: Current learning rate: 0.00137 
2025-03-02 16:45:47.767119: train_loss -0.9456 
2025-03-02 16:45:47.772738: val_loss -0.8366 
2025-03-02 16:45:47.776281: Pseudo dice [np.float32(0.8996), np.float32(0.8804)] 
2025-03-02 16:45:47.779433: Epoch time: 6.73 s 
2025-03-02 16:45:47.784018: Yayy! New best EMA pseudo Dice: 0.8870999813079834 
2025-03-02 16:45:48.553758:  
2025-03-02 16:45:48.559312: Epoch 90 
2025-03-02 16:45:48.562862: Current learning rate: 0.00126 
2025-03-02 16:45:55.258988: train_loss -0.9455 
2025-03-02 16:45:55.264530: val_loss -0.8325 
2025-03-02 16:45:55.268547: Pseudo dice [np.float32(0.8971), np.float32(0.8782)] 
2025-03-02 16:45:55.272067: Epoch time: 6.71 s 
2025-03-02 16:45:55.275577: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-03-02 16:45:55.873659:  
2025-03-02 16:45:55.879754: Epoch 91 
2025-03-02 16:45:55.882829: Current learning rate: 0.00115 
2025-03-02 16:46:02.573179: train_loss -0.946 
2025-03-02 16:46:02.579332: val_loss -0.8326 
2025-03-02 16:46:02.582402: Pseudo dice [np.float32(0.8978), np.float32(0.8774)] 
2025-03-02 16:46:02.584964: Epoch time: 6.7 s 
2025-03-02 16:46:02.589677: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-03-02 16:46:03.207960:  
2025-03-02 16:46:03.213487: Epoch 92 
2025-03-02 16:46:03.217004: Current learning rate: 0.00103 
2025-03-02 16:46:09.932596: train_loss -0.9466 
2025-03-02 16:46:09.938270: val_loss -0.8267 
2025-03-02 16:46:09.942340: Pseudo dice [np.float32(0.8935), np.float32(0.8747)] 
2025-03-02 16:46:09.945406: Epoch time: 6.73 s 
2025-03-02 16:46:10.513524:  
2025-03-02 16:46:10.519042: Epoch 93 
2025-03-02 16:46:10.522556: Current learning rate: 0.00091 
2025-03-02 16:46:17.250430: train_loss -0.9466 
2025-03-02 16:46:17.256011: val_loss -0.8342 
2025-03-02 16:46:17.262066: Pseudo dice [np.float32(0.8987), np.float32(0.879)] 
2025-03-02 16:46:17.266126: Epoch time: 6.74 s 
2025-03-02 16:46:17.854111:  
2025-03-02 16:46:17.860165: Epoch 94 
2025-03-02 16:46:17.864680: Current learning rate: 0.00079 
2025-03-02 16:46:24.573957: train_loss -0.9479 
2025-03-02 16:46:24.580165: val_loss -0.8284 
2025-03-02 16:46:24.584255: Pseudo dice [np.float32(0.8935), np.float32(0.8756)] 
2025-03-02 16:46:24.587844: Epoch time: 6.72 s 
2025-03-02 16:46:25.151939:  
2025-03-02 16:46:25.157958: Epoch 95 
2025-03-02 16:46:25.161976: Current learning rate: 0.00067 
2025-03-02 16:46:31.873004: train_loss -0.9468 
2025-03-02 16:46:31.878054: val_loss -0.8285 
2025-03-02 16:46:31.883114: Pseudo dice [np.float32(0.8952), np.float32(0.8765)] 
2025-03-02 16:46:31.887150: Epoch time: 6.72 s 
2025-03-02 16:46:32.480546:  
2025-03-02 16:46:32.486136: Epoch 96 
2025-03-02 16:46:32.490262: Current learning rate: 0.00055 
2025-03-02 16:46:39.156112: train_loss -0.9475 
2025-03-02 16:46:39.162202: val_loss -0.8312 
2025-03-02 16:46:39.165282: Pseudo dice [np.float32(0.8971), np.float32(0.878)] 
2025-03-02 16:46:39.169315: Epoch time: 6.68 s 
2025-03-02 16:46:39.745354:  
2025-03-02 16:46:39.750376: Epoch 97 
2025-03-02 16:46:39.755397: Current learning rate: 0.00043 
2025-03-02 16:46:46.443697: train_loss -0.9476 
2025-03-02 16:46:46.449759: val_loss -0.8284 
2025-03-02 16:46:46.454635: Pseudo dice [np.float32(0.8955), np.float32(0.8763)] 
2025-03-02 16:46:46.457649: Epoch time: 6.7 s 
2025-03-02 16:46:47.200364:  
2025-03-02 16:46:47.203914: Epoch 98 
2025-03-02 16:46:47.207459: Current learning rate: 0.0003 
2025-03-02 16:46:53.911488: train_loss -0.9471 
2025-03-02 16:46:53.917526: val_loss -0.8319 
2025-03-02 16:46:53.921055: Pseudo dice [np.float32(0.8965), np.float32(0.877)] 
2025-03-02 16:46:53.925094: Epoch time: 6.71 s 
2025-03-02 16:46:54.513101:  
2025-03-02 16:46:54.518663: Epoch 99 
2025-03-02 16:46:54.521209: Current learning rate: 0.00016 
2025-03-02 16:47:01.198949: train_loss -0.9474 
2025-03-02 16:47:01.203964: val_loss -0.8347 
2025-03-02 16:47:01.207478: Pseudo dice [np.float32(0.8984), np.float32(0.8797)] 
2025-03-02 16:47:01.211509: Epoch time: 6.69 s 
2025-03-02 16:47:01.854713: Training done. 
2025-03-02 16:47:01.915226: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-02 16:47:01.927226: The split file contains 5 splits. 
2025-03-02 16:47:01.932226: Desired fold for training: 0 
2025-03-02 16:47:01.938226: This split has 208 training and 52 validation cases. 
2025-03-02 16:47:01.944227: predicting hippocampus_017 
2025-03-02 16:47:01.951226: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-02 16:47:02.134249: predicting hippocampus_019 
2025-03-02 16:47:02.140250: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-02 16:47:02.185253: predicting hippocampus_033 
2025-03-02 16:47:02.191252: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-02 16:47:02.218343: predicting hippocampus_035 
2025-03-02 16:47:02.224344: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-02 16:47:02.251343: predicting hippocampus_037 
2025-03-02 16:47:02.258343: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-02 16:47:02.286346: predicting hippocampus_049 
2025-03-02 16:47:02.292346: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-02 16:47:02.319851: predicting hippocampus_052 
2025-03-02 16:47:02.325851: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-02 16:47:02.354851: predicting hippocampus_065 
2025-03-02 16:47:02.361851: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-02 16:47:02.387857: predicting hippocampus_083 
2025-03-02 16:47:02.394366: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-02 16:47:02.428365: predicting hippocampus_088 
2025-03-02 16:47:02.440369: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-02 16:47:06.091330: predicting hippocampus_090 
2025-03-02 16:47:06.099839: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-02 16:47:06.156838: predicting hippocampus_092 
2025-03-02 16:47:06.170838: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-02 16:47:06.240349: predicting hippocampus_095 
2025-03-02 16:47:06.248349: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-02 16:47:06.303858: predicting hippocampus_107 
2025-03-02 16:47:06.311860: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-02 16:47:06.364859: predicting hippocampus_108 
2025-03-02 16:47:06.373858: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-02 16:47:06.415369: predicting hippocampus_123 
2025-03-02 16:47:06.422369: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-02 16:47:06.461368: predicting hippocampus_125 
2025-03-02 16:47:06.468369: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-02 16:47:06.528876: predicting hippocampus_157 
2025-03-02 16:47:06.537878: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-02 16:47:06.574876: predicting hippocampus_164 
2025-03-02 16:47:06.581880: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-02 16:47:06.675391: predicting hippocampus_169 
2025-03-02 16:47:06.682396: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-02 16:47:06.712902: predicting hippocampus_175 
2025-03-02 16:47:06.719902: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-02 16:47:06.758902: predicting hippocampus_185 
2025-03-02 16:47:06.765902: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-02 16:47:06.801932: predicting hippocampus_190 
2025-03-02 16:47:06.808929: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-02 16:47:06.844928: predicting hippocampus_194 
2025-03-02 16:47:06.851931: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-02 16:47:06.881934: predicting hippocampus_204 
2025-03-02 16:47:06.888935: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-02 16:47:06.917439: predicting hippocampus_205 
2025-03-02 16:47:06.924441: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-02 16:47:06.957440: predicting hippocampus_210 
2025-03-02 16:47:06.964441: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-02 16:47:06.991444: predicting hippocampus_217 
2025-03-02 16:47:06.997950: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-02 16:47:07.042951: predicting hippocampus_219 
2025-03-02 16:47:07.050951: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-02 16:47:07.088956: predicting hippocampus_229 
2025-03-02 16:47:07.096462: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-02 16:47:07.130462: predicting hippocampus_244 
2025-03-02 16:47:07.141463: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-02 16:47:07.182465: predicting hippocampus_261 
2025-03-02 16:47:07.189467: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-02 16:47:07.235976: predicting hippocampus_264 
2025-03-02 16:47:07.243974: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-02 16:47:07.273974: predicting hippocampus_277 
2025-03-02 16:47:07.284977: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-02 16:47:07.330482: predicting hippocampus_280 
2025-03-02 16:47:07.337482: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-02 16:47:07.366484: predicting hippocampus_286 
2025-03-02 16:47:07.372483: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-02 16:47:07.418994: predicting hippocampus_288 
2025-03-02 16:47:07.424994: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-02 16:47:07.475993: predicting hippocampus_289 
2025-03-02 16:47:07.482997: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-02 16:47:07.511502: predicting hippocampus_296 
2025-03-02 16:47:07.518502: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-02 16:47:07.545502: predicting hippocampus_305 
2025-03-02 16:47:07.552502: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-02 16:47:07.584504: predicting hippocampus_308 
2025-03-02 16:47:07.592505: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-02 16:47:07.623012: predicting hippocampus_317 
2025-03-02 16:47:07.629012: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-02 16:47:07.658012: predicting hippocampus_327 
2025-03-02 16:47:07.664012: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-02 16:47:07.692016: predicting hippocampus_330 
2025-03-02 16:47:07.698521: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-02 16:47:07.726521: predicting hippocampus_332 
2025-03-02 16:47:07.732522: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-02 16:47:07.765522: predicting hippocampus_338 
2025-03-02 16:47:07.772521: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-02 16:47:07.825033: predicting hippocampus_349 
2025-03-02 16:47:07.833031: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-02 16:47:07.861031: predicting hippocampus_350 
2025-03-02 16:47:07.867033: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-02 16:47:07.895537: predicting hippocampus_356 
2025-03-02 16:47:07.902541: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-02 16:47:07.931541: predicting hippocampus_358 
2025-03-02 16:47:07.937541: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-02 16:47:07.966541: predicting hippocampus_374 
2025-03-02 16:47:07.973541: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-02 16:47:08.005051: predicting hippocampus_394 
2025-03-02 16:47:08.011051: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-02 16:47:11.670080: Validation complete 
2025-03-02 16:47:11.675082: Mean Validation Dice:  0.30150378445764153 
