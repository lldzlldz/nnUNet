
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-13 08:27:07.431003: do_dummy_2d_data_aug: False 
2025-03-13 08:27:07.441702: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-13 08:27:07.449706: The split file contains 5 splits. 
2025-03-13 08:27:07.451705: Desired fold for training: 0 
2025-03-13 08:27:07.454702: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-13 08:27:13.579944: unpacking dataset... 
2025-03-13 08:27:14.414503: unpacking done... 
2025-03-13 08:27:16.486391:  
2025-03-13 08:27:16.491410: Epoch 0 
2025-03-13 08:27:16.493922: Current learning rate: 0.01 
2025-03-13 08:27:23.752310: train_loss -0.3243 
2025-03-13 08:27:23.758353: val_loss -0.7536 
2025-03-13 08:27:23.761395: Pseudo dice [np.float32(0.8238), np.float32(0.8152)] 
2025-03-13 08:27:23.764919: Epoch time: 7.27 s 
2025-03-13 08:27:23.767453: Yayy! New best EMA pseudo Dice: 0.8195000290870667 
2025-03-13 08:27:24.250548:  
2025-03-13 08:27:24.256098: Epoch 1 
2025-03-13 08:27:24.259131: Current learning rate: 0.00991 
2025-03-13 08:27:30.679640: train_loss -0.7705 
2025-03-13 08:27:30.685205: val_loss -0.7945 
2025-03-13 08:27:30.688734: Pseudo dice [np.float32(0.8532), np.float32(0.841)] 
2025-03-13 08:27:30.690797: Epoch time: 6.43 s 
2025-03-13 08:27:30.695388: Yayy! New best EMA pseudo Dice: 0.8223000168800354 
2025-03-13 08:27:31.242979:  
2025-03-13 08:27:31.249022: Epoch 2 
2025-03-13 08:27:31.252086: Current learning rate: 0.00982 
2025-03-13 08:27:37.705504: train_loss -0.8016 
2025-03-13 08:27:37.711592: val_loss -0.8161 
2025-03-13 08:27:37.715136: Pseudo dice [np.float32(0.8715), np.float32(0.8547)] 
2025-03-13 08:27:37.718219: Epoch time: 6.46 s 
2025-03-13 08:27:37.721267: Yayy! New best EMA pseudo Dice: 0.8263999819755554 
2025-03-13 08:27:38.294178:  
2025-03-13 08:27:38.299045: Epoch 3 
2025-03-13 08:27:38.302557: Current learning rate: 0.00973 
2025-03-13 08:27:44.695764: train_loss -0.813 
2025-03-13 08:27:44.701936: val_loss -0.8196 
2025-03-13 08:27:44.704979: Pseudo dice [np.float32(0.876), np.float32(0.8502)] 
2025-03-13 08:27:44.709005: Epoch time: 6.4 s 
2025-03-13 08:27:44.712032: Yayy! New best EMA pseudo Dice: 0.8299999833106995 
2025-03-13 08:27:45.269122:  
2025-03-13 08:27:45.275202: Epoch 4 
2025-03-13 08:27:45.278268: Current learning rate: 0.00964 
2025-03-13 08:27:51.690316: train_loss -0.823 
2025-03-13 08:27:51.695864: val_loss -0.8301 
2025-03-13 08:27:51.699408: Pseudo dice [np.float32(0.8818), np.float32(0.8635)] 
2025-03-13 08:27:51.702051: Epoch time: 6.42 s 
2025-03-13 08:27:51.705563: Yayy! New best EMA pseudo Dice: 0.8342999815940857 
2025-03-13 08:27:52.391624:  
2025-03-13 08:27:52.397685: Epoch 5 
2025-03-13 08:27:52.400738: Current learning rate: 0.00955 
2025-03-13 08:27:58.817568: train_loss -0.8314 
2025-03-13 08:27:58.823124: val_loss -0.8376 
2025-03-13 08:27:58.827168: Pseudo dice [np.float32(0.8888), np.float32(0.8706)] 
2025-03-13 08:27:58.830184: Epoch time: 6.43 s 
2025-03-13 08:27:58.833693: Yayy! New best EMA pseudo Dice: 0.8388000130653381 
2025-03-13 08:27:59.387254:  
2025-03-13 08:27:59.392272: Epoch 6 
2025-03-13 08:27:59.395794: Current learning rate: 0.00946 
2025-03-13 08:28:05.780287: train_loss -0.834 
2025-03-13 08:28:05.786309: val_loss -0.8384 
2025-03-13 08:28:05.789817: Pseudo dice [np.float32(0.8855), np.float32(0.8722)] 
2025-03-13 08:28:05.793883: Epoch time: 6.39 s 
2025-03-13 08:28:05.796900: Yayy! New best EMA pseudo Dice: 0.8428000211715698 
2025-03-13 08:28:06.354827:  
2025-03-13 08:28:06.360375: Epoch 7 
2025-03-13 08:28:06.362908: Current learning rate: 0.00937 
2025-03-13 08:28:12.767455: train_loss -0.8372 
2025-03-13 08:28:12.774100: val_loss -0.839 
2025-03-13 08:28:12.777138: Pseudo dice [np.float32(0.8857), np.float32(0.874)] 
2025-03-13 08:28:12.780169: Epoch time: 6.41 s 
2025-03-13 08:28:12.783683: Yayy! New best EMA pseudo Dice: 0.8464999794960022 
2025-03-13 08:28:13.346643:  
2025-03-13 08:28:13.352214: Epoch 8 
2025-03-13 08:28:13.354800: Current learning rate: 0.00928 
2025-03-13 08:28:19.774549: train_loss -0.8365 
2025-03-13 08:28:19.783111: val_loss -0.8382 
2025-03-13 08:28:19.787171: Pseudo dice [np.float32(0.8865), np.float32(0.8688)] 
2025-03-13 08:28:19.790231: Epoch time: 6.43 s 
2025-03-13 08:28:19.792741: Yayy! New best EMA pseudo Dice: 0.8496999740600586 
2025-03-13 08:28:20.374814:  
2025-03-13 08:28:20.381429: Epoch 9 
2025-03-13 08:28:20.383981: Current learning rate: 0.00919 
2025-03-13 08:28:26.775044: train_loss -0.8424 
2025-03-13 08:28:26.781099: val_loss -0.8402 
2025-03-13 08:28:26.786117: Pseudo dice [np.float32(0.8902), np.float32(0.8702)] 
2025-03-13 08:28:26.790157: Epoch time: 6.4 s 
2025-03-13 08:28:26.793200: Yayy! New best EMA pseudo Dice: 0.8526999950408936 
2025-03-13 08:28:27.336213:  
2025-03-13 08:28:27.340247: Epoch 10 
2025-03-13 08:28:27.342764: Current learning rate: 0.0091 
2025-03-13 08:28:33.734473: train_loss -0.8487 
2025-03-13 08:28:33.740723: val_loss -0.8441 
2025-03-13 08:28:33.744309: Pseudo dice [np.float32(0.8917), np.float32(0.8729)] 
2025-03-13 08:28:33.747327: Epoch time: 6.4 s 
2025-03-13 08:28:33.749841: Yayy! New best EMA pseudo Dice: 0.8557000160217285 
2025-03-13 08:28:34.298461:  
2025-03-13 08:28:34.304015: Epoch 11 
2025-03-13 08:28:34.306551: Current learning rate: 0.009 
2025-03-13 08:28:40.704511: train_loss -0.8487 
2025-03-13 08:28:40.711097: val_loss -0.8424 
2025-03-13 08:28:40.715426: Pseudo dice [np.float32(0.8899), np.float32(0.8753)] 
2025-03-13 08:28:40.718435: Epoch time: 6.41 s 
2025-03-13 08:28:40.721943: Yayy! New best EMA pseudo Dice: 0.8583999872207642 
2025-03-13 08:28:41.284776:  
2025-03-13 08:28:41.291324: Epoch 12 
2025-03-13 08:28:41.294522: Current learning rate: 0.00891 
2025-03-13 08:28:47.692207: train_loss -0.8506 
2025-03-13 08:28:47.697747: val_loss -0.8446 
2025-03-13 08:28:47.701774: Pseudo dice [np.float32(0.893), np.float32(0.8725)] 
2025-03-13 08:28:47.704813: Epoch time: 6.41 s 
2025-03-13 08:28:47.708367: Yayy! New best EMA pseudo Dice: 0.86080002784729 
2025-03-13 08:28:48.399775:  
2025-03-13 08:28:48.404787: Epoch 13 
2025-03-13 08:28:48.408297: Current learning rate: 0.00882 
2025-03-13 08:28:54.803271: train_loss -0.8526 
2025-03-13 08:28:54.809439: val_loss -0.849 
2025-03-13 08:28:54.811991: Pseudo dice [np.float32(0.8969), np.float32(0.8766)] 
2025-03-13 08:28:54.816025: Epoch time: 6.4 s 
2025-03-13 08:28:54.818569: Yayy! New best EMA pseudo Dice: 0.8633999824523926 
2025-03-13 08:28:55.380011:  
2025-03-13 08:28:55.386070: Epoch 14 
2025-03-13 08:28:55.389121: Current learning rate: 0.00873 
2025-03-13 08:29:01.776822: train_loss -0.8531 
2025-03-13 08:29:01.781918: val_loss -0.8448 
2025-03-13 08:29:01.787022: Pseudo dice [np.float32(0.8927), np.float32(0.8741)] 
2025-03-13 08:29:01.790052: Epoch time: 6.4 s 
2025-03-13 08:29:01.793077: Yayy! New best EMA pseudo Dice: 0.8654000163078308 
2025-03-13 08:29:02.357284:  
2025-03-13 08:29:02.362805: Epoch 15 
2025-03-13 08:29:02.366320: Current learning rate: 0.00864 
2025-03-13 08:29:08.740207: train_loss -0.8569 
2025-03-13 08:29:08.746305: val_loss -0.8461 
2025-03-13 08:29:08.749853: Pseudo dice [np.float32(0.893), np.float32(0.8763)] 
2025-03-13 08:29:08.752924: Epoch time: 6.38 s 
2025-03-13 08:29:08.755986: Yayy! New best EMA pseudo Dice: 0.8672999739646912 
2025-03-13 08:29:09.327140:  
2025-03-13 08:29:09.332708: Epoch 16 
2025-03-13 08:29:09.335742: Current learning rate: 0.00855 
2025-03-13 08:29:15.732978: train_loss -0.8555 
2025-03-13 08:29:15.739499: val_loss -0.8497 
2025-03-13 08:29:15.743015: Pseudo dice [np.float32(0.8957), np.float32(0.8794)] 
2025-03-13 08:29:15.745504: Epoch time: 6.41 s 
2025-03-13 08:29:15.750051: Yayy! New best EMA pseudo Dice: 0.8693000078201294 
2025-03-13 08:29:16.334410:  
2025-03-13 08:29:16.339927: Epoch 17 
2025-03-13 08:29:16.343436: Current learning rate: 0.00846 
2025-03-13 08:29:22.767061: train_loss -0.8557 
2025-03-13 08:29:22.774716: val_loss -0.8443 
2025-03-13 08:29:22.777804: Pseudo dice [np.float32(0.892), np.float32(0.8764)] 
2025-03-13 08:29:22.781399: Epoch time: 6.43 s 
2025-03-13 08:29:22.784502: Yayy! New best EMA pseudo Dice: 0.8708000183105469 
2025-03-13 08:29:23.357420:  
2025-03-13 08:29:23.362940: Epoch 18 
2025-03-13 08:29:23.366455: Current learning rate: 0.00836 
2025-03-13 08:29:29.774719: train_loss -0.8577 
2025-03-13 08:29:29.780321: val_loss -0.843 
2025-03-13 08:29:29.783863: Pseudo dice [np.float32(0.8887), np.float32(0.8753)] 
2025-03-13 08:29:29.787449: Epoch time: 6.42 s 
2025-03-13 08:29:29.790495: Yayy! New best EMA pseudo Dice: 0.8719000220298767 
2025-03-13 08:29:30.360746:  
2025-03-13 08:29:30.366319: Epoch 19 
2025-03-13 08:29:30.369367: Current learning rate: 0.00827 
2025-03-13 08:29:36.770905: train_loss -0.8594 
2025-03-13 08:29:36.778027: val_loss -0.8493 
2025-03-13 08:29:36.781604: Pseudo dice [np.float32(0.8948), np.float32(0.8802)] 
2025-03-13 08:29:36.784638: Epoch time: 6.41 s 
2025-03-13 08:29:36.787144: Yayy! New best EMA pseudo Dice: 0.8734999895095825 
2025-03-13 08:29:37.487675:  
2025-03-13 08:29:37.493229: Epoch 20 
2025-03-13 08:29:37.495789: Current learning rate: 0.00818 
2025-03-13 08:29:43.896532: train_loss -0.8611 
2025-03-13 08:29:43.903636: val_loss -0.8449 
2025-03-13 08:29:43.907146: Pseudo dice [np.float32(0.8914), np.float32(0.8778)] 
2025-03-13 08:29:43.909652: Epoch time: 6.41 s 
2025-03-13 08:29:43.913665: Yayy! New best EMA pseudo Dice: 0.8745999932289124 
2025-03-13 08:29:44.491065:  
2025-03-13 08:29:44.496599: Epoch 21 
2025-03-13 08:29:44.502149: Current learning rate: 0.00809 
2025-03-13 08:29:50.892020: train_loss -0.8628 
2025-03-13 08:29:50.898180: val_loss -0.8508 
2025-03-13 08:29:50.901235: Pseudo dice [np.float32(0.8971), np.float32(0.8801)] 
2025-03-13 08:29:50.904268: Epoch time: 6.4 s 
2025-03-13 08:29:50.908298: Yayy! New best EMA pseudo Dice: 0.8759999871253967 
2025-03-13 08:29:51.461758:  
2025-03-13 08:29:51.467309: Epoch 22 
2025-03-13 08:29:51.469867: Current learning rate: 0.008 
2025-03-13 08:29:57.852777: train_loss -0.8647 
2025-03-13 08:29:57.858868: val_loss -0.8504 
2025-03-13 08:29:57.861408: Pseudo dice [np.float32(0.8953), np.float32(0.8795)] 
2025-03-13 08:29:57.865945: Epoch time: 6.39 s 
2025-03-13 08:29:57.868451: Yayy! New best EMA pseudo Dice: 0.8770999908447266 
2025-03-13 08:29:58.421465:  
2025-03-13 08:29:58.426538: Epoch 23 
2025-03-13 08:29:58.430124: Current learning rate: 0.0079 
2025-03-13 08:30:04.832964: train_loss -0.8643 
2025-03-13 08:30:04.839547: val_loss -0.8495 
2025-03-13 08:30:04.842595: Pseudo dice [np.float32(0.8963), np.float32(0.8777)] 
2025-03-13 08:30:04.846696: Epoch time: 6.41 s 
2025-03-13 08:30:04.849719: Yayy! New best EMA pseudo Dice: 0.8780999779701233 
2025-03-13 08:30:05.401903:  
2025-03-13 08:30:05.406917: Epoch 24 
2025-03-13 08:30:05.410424: Current learning rate: 0.00781 
2025-03-13 08:30:11.805254: train_loss -0.8655 
2025-03-13 08:30:11.810809: val_loss -0.8473 
2025-03-13 08:30:11.815358: Pseudo dice [np.float32(0.8955), np.float32(0.8768)] 
2025-03-13 08:30:11.818397: Epoch time: 6.4 s 
2025-03-13 08:30:11.822941: Yayy! New best EMA pseudo Dice: 0.8788999915122986 
2025-03-13 08:30:12.382404:  
2025-03-13 08:30:12.387955: Epoch 25 
2025-03-13 08:30:12.391543: Current learning rate: 0.00772 
2025-03-13 08:30:18.751226: train_loss -0.8672 
2025-03-13 08:30:18.758816: val_loss -0.8494 
2025-03-13 08:30:18.761870: Pseudo dice [np.float32(0.8963), np.float32(0.8775)] 
2025-03-13 08:30:18.765390: Epoch time: 6.37 s 
2025-03-13 08:30:18.768971: Yayy! New best EMA pseudo Dice: 0.8797000050544739 
2025-03-13 08:30:19.319372:  
2025-03-13 08:30:19.324901: Epoch 26 
2025-03-13 08:30:19.328497: Current learning rate: 0.00763 
2025-03-13 08:30:25.741557: train_loss -0.8683 
2025-03-13 08:30:25.747635: val_loss -0.855 
2025-03-13 08:30:25.751727: Pseudo dice [np.float32(0.8994), np.float32(0.8823)] 
2025-03-13 08:30:25.754768: Epoch time: 6.42 s 
2025-03-13 08:30:25.758343: Yayy! New best EMA pseudo Dice: 0.8808000087738037 
2025-03-13 08:30:26.319565:  
2025-03-13 08:30:26.323578: Epoch 27 
2025-03-13 08:30:26.327087: Current learning rate: 0.00753 
2025-03-13 08:30:32.734671: train_loss -0.8693 
2025-03-13 08:30:32.740232: val_loss -0.8505 
2025-03-13 08:30:32.744803: Pseudo dice [np.float32(0.8971), np.float32(0.8783)] 
2025-03-13 08:30:32.747825: Epoch time: 6.42 s 
2025-03-13 08:30:32.750854: Yayy! New best EMA pseudo Dice: 0.8815000057220459 
2025-03-13 08:30:33.441380:  
2025-03-13 08:30:33.447437: Epoch 28 
2025-03-13 08:30:33.450503: Current learning rate: 0.00744 
2025-03-13 08:30:39.823991: train_loss -0.8689 
2025-03-13 08:30:39.830664: val_loss -0.8457 
2025-03-13 08:30:39.833172: Pseudo dice [np.float32(0.8957), np.float32(0.8746)] 
2025-03-13 08:30:39.837200: Epoch time: 6.38 s 
2025-03-13 08:30:39.839777: Yayy! New best EMA pseudo Dice: 0.8819000124931335 
2025-03-13 08:30:40.393671:  
2025-03-13 08:30:40.399203: Epoch 29 
2025-03-13 08:30:40.402233: Current learning rate: 0.00735 
2025-03-13 08:30:46.800349: train_loss -0.8703 
2025-03-13 08:30:46.806446: val_loss -0.8475 
2025-03-13 08:30:46.810962: Pseudo dice [np.float32(0.8955), np.float32(0.8775)] 
2025-03-13 08:30:46.814235: Epoch time: 6.41 s 
2025-03-13 08:30:46.817741: Yayy! New best EMA pseudo Dice: 0.8823000192642212 
2025-03-13 08:30:47.378284:  
2025-03-13 08:30:47.383801: Epoch 30 
2025-03-13 08:30:47.387313: Current learning rate: 0.00725 
2025-03-13 08:30:53.789617: train_loss -0.871 
2025-03-13 08:30:53.795133: val_loss -0.8496 
2025-03-13 08:30:53.799646: Pseudo dice [np.float32(0.896), np.float32(0.8786)] 
2025-03-13 08:30:53.802657: Epoch time: 6.41 s 
2025-03-13 08:30:53.806168: Yayy! New best EMA pseudo Dice: 0.8827999830245972 
2025-03-13 08:30:54.384040:  
2025-03-13 08:30:54.389051: Epoch 31 
2025-03-13 08:30:54.393209: Current learning rate: 0.00716 
2025-03-13 08:31:00.779021: train_loss -0.8692 
2025-03-13 08:31:00.784619: val_loss -0.8555 
2025-03-13 08:31:00.788133: Pseudo dice [np.float32(0.9006), np.float32(0.8832)] 
2025-03-13 08:31:00.790215: Epoch time: 6.4 s 
2025-03-13 08:31:00.794235: Yayy! New best EMA pseudo Dice: 0.8837000131607056 
2025-03-13 08:31:01.351340:  
2025-03-13 08:31:01.356387: Epoch 32 
2025-03-13 08:31:01.359911: Current learning rate: 0.00707 
2025-03-13 08:31:07.759851: train_loss -0.8731 
2025-03-13 08:31:07.765923: val_loss -0.854 
2025-03-13 08:31:07.769532: Pseudo dice [np.float32(0.9002), np.float32(0.8793)] 
2025-03-13 08:31:07.772078: Epoch time: 6.41 s 
2025-03-13 08:31:07.776132: Yayy! New best EMA pseudo Dice: 0.8844000101089478 
2025-03-13 08:31:08.340415:  
2025-03-13 08:31:08.345931: Epoch 33 
2025-03-13 08:31:08.349440: Current learning rate: 0.00697 
2025-03-13 08:31:14.760264: train_loss -0.8732 
2025-03-13 08:31:14.766310: val_loss -0.8465 
2025-03-13 08:31:14.769842: Pseudo dice [np.float32(0.894), np.float32(0.8768)] 
2025-03-13 08:31:14.772905: Epoch time: 6.42 s 
2025-03-13 08:31:14.776489: Yayy! New best EMA pseudo Dice: 0.8845000267028809 
2025-03-13 08:31:15.338394:  
2025-03-13 08:31:15.344476: Epoch 34 
2025-03-13 08:31:15.347524: Current learning rate: 0.00688 
2025-03-13 08:31:21.764918: train_loss -0.8732 
2025-03-13 08:31:21.773026: val_loss -0.848 
2025-03-13 08:31:21.777562: Pseudo dice [np.float32(0.8968), np.float32(0.8756)] 
2025-03-13 08:31:21.780648: Epoch time: 6.43 s 
2025-03-13 08:31:21.784689: Yayy! New best EMA pseudo Dice: 0.8845999836921692 
2025-03-13 08:31:22.355607:  
2025-03-13 08:31:22.362145: Epoch 35 
2025-03-13 08:31:22.365173: Current learning rate: 0.00679 
2025-03-13 08:31:28.792459: train_loss -0.8736 
2025-03-13 08:31:28.798501: val_loss -0.8519 
2025-03-13 08:31:28.802198: Pseudo dice [np.float32(0.8998), np.float32(0.8801)] 
2025-03-13 08:31:28.805743: Epoch time: 6.44 s 
2025-03-13 08:31:28.808779: Yayy! New best EMA pseudo Dice: 0.885200023651123 
2025-03-13 08:31:29.516558:  
2025-03-13 08:31:29.522639: Epoch 36 
2025-03-13 08:31:29.525704: Current learning rate: 0.00669 
2025-03-13 08:31:35.918034: train_loss -0.8753 
2025-03-13 08:31:35.923092: val_loss -0.8533 
2025-03-13 08:31:35.926661: Pseudo dice [np.float32(0.8991), np.float32(0.8822)] 
2025-03-13 08:31:35.930181: Epoch time: 6.4 s 
2025-03-13 08:31:35.933771: Yayy! New best EMA pseudo Dice: 0.885699987411499 
2025-03-13 08:31:36.511580:  
2025-03-13 08:31:36.519148: Epoch 37 
2025-03-13 08:31:36.521736: Current learning rate: 0.0066 
2025-03-13 08:31:42.923541: train_loss -0.8768 
2025-03-13 08:31:42.929129: val_loss -0.8538 
2025-03-13 08:31:42.932168: Pseudo dice [np.float32(0.8996), np.float32(0.8822)] 
2025-03-13 08:31:42.936204: Epoch time: 6.41 s 
2025-03-13 08:31:42.938743: Yayy! New best EMA pseudo Dice: 0.8862000107765198 
2025-03-13 08:31:43.507953:  
2025-03-13 08:31:43.513503: Epoch 38 
2025-03-13 08:31:43.516537: Current learning rate: 0.0065 
2025-03-13 08:31:49.907146: train_loss -0.8767 
2025-03-13 08:31:49.913203: val_loss -0.8562 
2025-03-13 08:31:49.916758: Pseudo dice [np.float32(0.9001), np.float32(0.8836)] 
2025-03-13 08:31:49.919807: Epoch time: 6.4 s 
2025-03-13 08:31:49.922838: Yayy! New best EMA pseudo Dice: 0.8867999911308289 
2025-03-13 08:31:50.507078:  
2025-03-13 08:31:50.513112: Epoch 39 
2025-03-13 08:31:50.516108: Current learning rate: 0.00641 
2025-03-13 08:31:56.932323: train_loss -0.8791 
2025-03-13 08:31:56.937445: val_loss -0.8527 
2025-03-13 08:31:56.942005: Pseudo dice [np.float32(0.9001), np.float32(0.8816)] 
2025-03-13 08:31:56.945051: Epoch time: 6.43 s 
2025-03-13 08:31:56.948615: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-03-13 08:31:57.532467:  
2025-03-13 08:31:57.537982: Epoch 40 
2025-03-13 08:31:57.541493: Current learning rate: 0.00631 
2025-03-13 08:32:03.926627: train_loss -0.8776 
2025-03-13 08:32:03.932680: val_loss -0.8461 
2025-03-13 08:32:03.935737: Pseudo dice [np.float32(0.8951), np.float32(0.8775)] 
2025-03-13 08:32:03.939326: Epoch time: 6.4 s 
2025-03-13 08:32:04.487796:  
2025-03-13 08:32:04.493313: Epoch 41 
2025-03-13 08:32:04.495818: Current learning rate: 0.00622 
2025-03-13 08:32:10.885626: train_loss -0.8799 
2025-03-13 08:32:10.891120: val_loss -0.8502 
2025-03-13 08:32:10.894636: Pseudo dice [np.float32(0.8981), np.float32(0.8793)] 
2025-03-13 08:32:10.898152: Epoch time: 6.4 s 
2025-03-13 08:32:10.901172: Yayy! New best EMA pseudo Dice: 0.8873000144958496 
2025-03-13 08:32:11.456612:  
2025-03-13 08:32:11.462196: Epoch 42 
2025-03-13 08:32:11.465244: Current learning rate: 0.00612 
2025-03-13 08:32:17.873449: train_loss -0.8785 
2025-03-13 08:32:17.879574: val_loss -0.8502 
2025-03-13 08:32:17.883116: Pseudo dice [np.float32(0.8975), np.float32(0.8789)] 
2025-03-13 08:32:17.885630: Epoch time: 6.42 s 
2025-03-13 08:32:17.889135: Yayy! New best EMA pseudo Dice: 0.8873999714851379 
2025-03-13 08:32:18.573189:  
2025-03-13 08:32:18.579322: Epoch 43 
2025-03-13 08:32:18.581831: Current learning rate: 0.00603 
2025-03-13 08:32:24.992778: train_loss -0.8799 
2025-03-13 08:32:24.998846: val_loss -0.8461 
2025-03-13 08:32:25.002392: Pseudo dice [np.float32(0.8953), np.float32(0.876)] 
2025-03-13 08:32:25.005929: Epoch time: 6.42 s 
2025-03-13 08:32:25.528914:  
2025-03-13 08:32:25.534488: Epoch 44 
2025-03-13 08:32:25.537038: Current learning rate: 0.00593 
2025-03-13 08:32:31.920160: train_loss -0.8812 
2025-03-13 08:32:31.925834: val_loss -0.8541 
2025-03-13 08:32:31.928884: Pseudo dice [np.float32(0.9003), np.float32(0.8833)] 
2025-03-13 08:32:31.932948: Epoch time: 6.39 s 
2025-03-13 08:32:31.935970: Yayy! New best EMA pseudo Dice: 0.8877000212669373 
2025-03-13 08:32:32.487479:  
2025-03-13 08:32:32.492496: Epoch 45 
2025-03-13 08:32:32.496007: Current learning rate: 0.00584 
2025-03-13 08:32:38.902421: train_loss -0.8826 
2025-03-13 08:32:38.908946: val_loss -0.85 
2025-03-13 08:32:38.912458: Pseudo dice [np.float32(0.8968), np.float32(0.8777)] 
2025-03-13 08:32:38.916074: Epoch time: 6.42 s 
2025-03-13 08:32:39.429353:  
2025-03-13 08:32:39.434883: Epoch 46 
2025-03-13 08:32:39.437908: Current learning rate: 0.00574 
2025-03-13 08:32:45.855032: train_loss -0.8826 
2025-03-13 08:32:45.861100: val_loss -0.8508 
2025-03-13 08:32:45.864122: Pseudo dice [np.float32(0.8961), np.float32(0.8806)] 
2025-03-13 08:32:45.867146: Epoch time: 6.43 s 
2025-03-13 08:32:45.871161: Yayy! New best EMA pseudo Dice: 0.8877000212669373 
2025-03-13 08:32:46.417781:  
2025-03-13 08:32:46.421812: Epoch 47 
2025-03-13 08:32:46.424334: Current learning rate: 0.00565 
2025-03-13 08:32:52.813972: train_loss -0.8835 
2025-03-13 08:32:52.820065: val_loss -0.8517 
2025-03-13 08:32:52.823101: Pseudo dice [np.float32(0.8993), np.float32(0.8803)] 
2025-03-13 08:32:52.827125: Epoch time: 6.4 s 
2025-03-13 08:32:52.830150: Yayy! New best EMA pseudo Dice: 0.8878999948501587 
2025-03-13 08:32:53.394117:  
2025-03-13 08:32:53.400213: Epoch 48 
2025-03-13 08:32:53.404775: Current learning rate: 0.00555 
2025-03-13 08:32:59.809679: train_loss -0.8833 
2025-03-13 08:32:59.815735: val_loss -0.8511 
2025-03-13 08:32:59.819283: Pseudo dice [np.float32(0.8995), np.float32(0.878)] 
2025-03-13 08:32:59.822317: Epoch time: 6.42 s 
2025-03-13 08:32:59.825842: Yayy! New best EMA pseudo Dice: 0.8880000114440918 
2025-03-13 08:33:00.380908:  
2025-03-13 08:33:00.386971: Epoch 49 
2025-03-13 08:33:00.390028: Current learning rate: 0.00546 
2025-03-13 08:33:06.782493: train_loss -0.8854 
2025-03-13 08:33:06.788067: val_loss -0.8464 
2025-03-13 08:33:06.792136: Pseudo dice [np.float32(0.8952), np.float32(0.8776)] 
2025-03-13 08:33:06.795174: Epoch time: 6.4 s 
2025-03-13 08:33:07.348693:  
2025-03-13 08:33:07.354208: Epoch 50 
2025-03-13 08:33:07.356714: Current learning rate: 0.00536 
2025-03-13 08:33:13.753862: train_loss -0.8849 
2025-03-13 08:33:13.759937: val_loss -0.8475 
2025-03-13 08:33:13.763244: Pseudo dice [np.float32(0.8942), np.float32(0.8778)] 
2025-03-13 08:33:13.766857: Epoch time: 6.41 s 
2025-03-13 08:33:14.425930:  
2025-03-13 08:33:14.431445: Epoch 51 
2025-03-13 08:33:14.434957: Current learning rate: 0.00526 
2025-03-13 08:33:20.835690: train_loss -0.8861 
2025-03-13 08:33:20.843837: val_loss -0.8517 
2025-03-13 08:33:20.848418: Pseudo dice [np.float32(0.8989), np.float32(0.8803)] 
2025-03-13 08:33:20.851479: Epoch time: 6.41 s 
2025-03-13 08:33:21.375461:  
2025-03-13 08:33:21.381605: Epoch 52 
2025-03-13 08:33:21.384664: Current learning rate: 0.00517 
2025-03-13 08:33:27.816299: train_loss -0.886 
2025-03-13 08:33:27.823673: val_loss -0.8488 
2025-03-13 08:33:27.828711: Pseudo dice [np.float32(0.8961), np.float32(0.8795)] 
2025-03-13 08:33:27.831747: Epoch time: 6.44 s 
2025-03-13 08:33:28.351439:  
2025-03-13 08:33:28.356998: Epoch 53 
2025-03-13 08:33:28.360536: Current learning rate: 0.00507 
2025-03-13 08:33:34.833261: train_loss -0.8865 
2025-03-13 08:33:34.838753: val_loss -0.8486 
2025-03-13 08:33:34.842765: Pseudo dice [np.float32(0.8971), np.float32(0.8772)] 
2025-03-13 08:33:34.845801: Epoch time: 6.48 s 
2025-03-13 08:33:35.366052:  
2025-03-13 08:33:35.371064: Epoch 54 
2025-03-13 08:33:35.374572: Current learning rate: 0.00497 
2025-03-13 08:33:41.896950: train_loss -0.887 
2025-03-13 08:33:41.902563: val_loss -0.8479 
2025-03-13 08:33:41.906201: Pseudo dice [np.float32(0.8968), np.float32(0.8757)] 
2025-03-13 08:33:41.909245: Epoch time: 6.53 s 
2025-03-13 08:33:42.435004:  
2025-03-13 08:33:42.440560: Epoch 55 
2025-03-13 08:33:42.444091: Current learning rate: 0.00487 
2025-03-13 08:33:48.901135: train_loss -0.888 
2025-03-13 08:33:48.906726: val_loss -0.8487 
2025-03-13 08:33:48.910764: Pseudo dice [np.float32(0.8979), np.float32(0.8776)] 
2025-03-13 08:33:48.914315: Epoch time: 6.47 s 
2025-03-13 08:33:49.430944:  
2025-03-13 08:33:49.435955: Epoch 56 
2025-03-13 08:33:49.439462: Current learning rate: 0.00478 
2025-03-13 08:33:55.887317: train_loss -0.8888 
2025-03-13 08:33:55.893003: val_loss -0.8498 
2025-03-13 08:33:55.897143: Pseudo dice [np.float32(0.8987), np.float32(0.8774)] 
2025-03-13 08:33:55.899661: Epoch time: 6.46 s 
2025-03-13 08:33:56.421527:  
2025-03-13 08:33:56.427594: Epoch 57 
2025-03-13 08:33:56.430681: Current learning rate: 0.00468 
2025-03-13 08:34:02.908058: train_loss -0.8884 
2025-03-13 08:34:02.914146: val_loss -0.8518 
2025-03-13 08:34:02.918221: Pseudo dice [np.float32(0.8985), np.float32(0.8822)] 
2025-03-13 08:34:02.921250: Epoch time: 6.49 s 
2025-03-13 08:34:03.444312:  
2025-03-13 08:34:03.449327: Epoch 58 
2025-03-13 08:34:03.452336: Current learning rate: 0.00458 
2025-03-13 08:34:09.935076: train_loss -0.8878 
2025-03-13 08:34:09.941206: val_loss -0.8466 
2025-03-13 08:34:09.944834: Pseudo dice [np.float32(0.8956), np.float32(0.876)] 
2025-03-13 08:34:09.947898: Epoch time: 6.49 s 
2025-03-13 08:34:10.616827:  
2025-03-13 08:34:10.622362: Epoch 59 
2025-03-13 08:34:10.625877: Current learning rate: 0.00448 
2025-03-13 08:34:17.089108: train_loss -0.8894 
2025-03-13 08:34:17.094740: val_loss -0.8511 
2025-03-13 08:34:17.099270: Pseudo dice [np.float32(0.8989), np.float32(0.8795)] 
2025-03-13 08:34:17.102347: Epoch time: 6.47 s 
2025-03-13 08:34:17.626299:  
2025-03-13 08:34:17.631839: Epoch 60 
2025-03-13 08:34:17.634866: Current learning rate: 0.00438 
2025-03-13 08:34:24.135022: train_loss -0.8905 
2025-03-13 08:34:24.141603: val_loss -0.8489 
2025-03-13 08:34:24.145133: Pseudo dice [np.float32(0.8975), np.float32(0.8787)] 
2025-03-13 08:34:24.149108: Epoch time: 6.51 s 
2025-03-13 08:34:24.748737:  
2025-03-13 08:34:24.754823: Epoch 61 
2025-03-13 08:34:24.758392: Current learning rate: 0.00429 
2025-03-13 08:34:31.262231: train_loss -0.8903 
2025-03-13 08:34:31.267825: val_loss -0.8515 
2025-03-13 08:34:31.271370: Pseudo dice [np.float32(0.8992), np.float32(0.88)] 
2025-03-13 08:34:31.274553: Epoch time: 6.51 s 
2025-03-13 08:34:31.278110: Yayy! New best EMA pseudo Dice: 0.8881000280380249 
2025-03-13 08:34:31.848646:  
2025-03-13 08:34:31.854163: Epoch 62 
2025-03-13 08:34:31.857682: Current learning rate: 0.00419 
2025-03-13 08:34:38.305556: train_loss -0.8913 
2025-03-13 08:34:38.311571: val_loss -0.8531 
2025-03-13 08:34:38.315091: Pseudo dice [np.float32(0.8998), np.float32(0.8792)] 
2025-03-13 08:34:38.318130: Epoch time: 6.46 s 
2025-03-13 08:34:38.321190: Yayy! New best EMA pseudo Dice: 0.8881999850273132 
2025-03-13 08:34:38.892238:  
2025-03-13 08:34:38.897797: Epoch 63 
2025-03-13 08:34:38.901826: Current learning rate: 0.00409 
2025-03-13 08:34:45.391566: train_loss -0.891 
2025-03-13 08:34:45.397130: val_loss -0.8448 
2025-03-13 08:34:45.400661: Pseudo dice [np.float32(0.8935), np.float32(0.8759)] 
2025-03-13 08:34:45.404704: Epoch time: 6.5 s 
2025-03-13 08:34:45.935700:  
2025-03-13 08:34:45.940710: Epoch 64 
2025-03-13 08:34:45.944218: Current learning rate: 0.00399 
2025-03-13 08:34:52.429040: train_loss -0.892 
2025-03-13 08:34:52.435146: val_loss -0.8513 
2025-03-13 08:34:52.439228: Pseudo dice [np.float32(0.898), np.float32(0.8811)] 
2025-03-13 08:34:52.442303: Epoch time: 6.49 s 
2025-03-13 08:34:52.979659:  
2025-03-13 08:34:52.985770: Epoch 65 
2025-03-13 08:34:52.988838: Current learning rate: 0.00389 
2025-03-13 08:34:59.481437: train_loss -0.8938 
2025-03-13 08:34:59.487049: val_loss -0.8497 
2025-03-13 08:34:59.491122: Pseudo dice [np.float32(0.8964), np.float32(0.8801)] 
2025-03-13 08:34:59.494169: Epoch time: 6.5 s 
2025-03-13 08:35:00.021747:  
2025-03-13 08:35:00.027343: Epoch 66 
2025-03-13 08:35:00.031368: Current learning rate: 0.00379 
2025-03-13 08:35:06.520044: train_loss -0.8937 
2025-03-13 08:35:06.526086: val_loss -0.8477 
2025-03-13 08:35:06.529649: Pseudo dice [np.float32(0.895), np.float32(0.8787)] 
2025-03-13 08:35:06.532713: Epoch time: 6.5 s 
2025-03-13 08:35:07.207838:  
2025-03-13 08:35:07.213353: Epoch 67 
2025-03-13 08:35:07.215858: Current learning rate: 0.00369 
2025-03-13 08:35:13.725972: train_loss -0.8942 
2025-03-13 08:35:13.731067: val_loss -0.8493 
2025-03-13 08:35:13.735605: Pseudo dice [np.float32(0.8976), np.float32(0.8782)] 
2025-03-13 08:35:13.738140: Epoch time: 6.52 s 
2025-03-13 08:35:14.290836:  
2025-03-13 08:35:14.296449: Epoch 68 
2025-03-13 08:35:14.298988: Current learning rate: 0.00359 
2025-03-13 08:35:21.188419: train_loss -0.8951 
2025-03-13 08:35:21.193988: val_loss -0.8477 
2025-03-13 08:35:21.198018: Pseudo dice [np.float32(0.8958), np.float32(0.8774)] 
2025-03-13 08:35:21.201570: Epoch time: 6.9 s 
2025-03-13 08:35:21.857142:  
2025-03-13 08:35:21.862168: Epoch 69 
2025-03-13 08:35:21.865741: Current learning rate: 0.00349 
2025-03-13 08:35:30.007115: train_loss -0.8941 
2025-03-13 08:35:30.012660: val_loss -0.848 
2025-03-13 08:35:30.016186: Pseudo dice [np.float32(0.8979), np.float32(0.8767)] 
2025-03-13 08:35:30.019222: Epoch time: 8.15 s 
2025-03-13 08:35:30.562936:  
2025-03-13 08:35:30.568488: Epoch 70 
2025-03-13 08:35:30.572047: Current learning rate: 0.00338 
2025-03-13 08:35:37.100887: train_loss -0.894 
2025-03-13 08:35:37.106922: val_loss -0.8498 
2025-03-13 08:35:37.110949: Pseudo dice [np.float32(0.8981), np.float32(0.8796)] 
2025-03-13 08:35:37.113968: Epoch time: 6.54 s 
2025-03-13 08:35:37.690113:  
2025-03-13 08:35:37.695672: Epoch 71 
2025-03-13 08:35:37.698216: Current learning rate: 0.00328 
2025-03-13 08:35:44.159601: train_loss -0.8942 
2025-03-13 08:35:44.166150: val_loss -0.8481 
2025-03-13 08:35:44.170170: Pseudo dice [np.float32(0.8962), np.float32(0.8794)] 
2025-03-13 08:35:44.173195: Epoch time: 6.47 s 
2025-03-13 08:35:44.714969:  
2025-03-13 08:35:44.720500: Epoch 72 
2025-03-13 08:35:44.724042: Current learning rate: 0.00318 
2025-03-13 08:35:51.193742: train_loss -0.8954 
2025-03-13 08:35:51.199764: val_loss -0.8528 
2025-03-13 08:35:51.202774: Pseudo dice [np.float32(0.8989), np.float32(0.883)] 
2025-03-13 08:35:51.206285: Epoch time: 6.48 s 
2025-03-13 08:35:51.751943:  
2025-03-13 08:35:51.757457: Epoch 73 
2025-03-13 08:35:51.760970: Current learning rate: 0.00308 
2025-03-13 08:35:58.266464: train_loss -0.8967 
2025-03-13 08:35:58.273085: val_loss -0.8513 
2025-03-13 08:35:58.277158: Pseudo dice [np.float32(0.8983), np.float32(0.8815)] 
2025-03-13 08:35:58.280184: Epoch time: 6.51 s 
2025-03-13 08:35:58.282704: Yayy! New best EMA pseudo Dice: 0.8883000016212463 
2025-03-13 08:35:58.856439:  
2025-03-13 08:35:58.862010: Epoch 74 
2025-03-13 08:35:58.866049: Current learning rate: 0.00297 
2025-03-13 08:36:05.356569: train_loss -0.8969 
2025-03-13 08:36:05.363219: val_loss -0.849 
2025-03-13 08:36:05.366791: Pseudo dice [np.float32(0.8971), np.float32(0.8791)] 
2025-03-13 08:36:05.370319: Epoch time: 6.5 s 
2025-03-13 08:36:06.053541:  
2025-03-13 08:36:06.059117: Epoch 75 
2025-03-13 08:36:06.062751: Current learning rate: 0.00287 
2025-03-13 08:36:12.511953: train_loss -0.896 
2025-03-13 08:36:12.518540: val_loss -0.8458 
2025-03-13 08:36:12.522098: Pseudo dice [np.float32(0.8951), np.float32(0.8773)] 
2025-03-13 08:36:12.525712: Epoch time: 6.46 s 
2025-03-13 08:36:13.061298:  
2025-03-13 08:36:13.067379: Epoch 76 
2025-03-13 08:36:13.070897: Current learning rate: 0.00277 
2025-03-13 08:36:19.561570: train_loss -0.8986 
2025-03-13 08:36:19.567156: val_loss -0.8474 
2025-03-13 08:36:19.571195: Pseudo dice [np.float32(0.8964), np.float32(0.8779)] 
2025-03-13 08:36:19.574233: Epoch time: 6.5 s 
2025-03-13 08:36:20.110506:  
2025-03-13 08:36:20.116030: Epoch 77 
2025-03-13 08:36:20.118536: Current learning rate: 0.00266 
2025-03-13 08:36:26.606065: train_loss -0.8973 
2025-03-13 08:36:26.613195: val_loss -0.848 
2025-03-13 08:36:26.617795: Pseudo dice [np.float32(0.895), np.float32(0.8786)] 
2025-03-13 08:36:26.620881: Epoch time: 6.5 s 
2025-03-13 08:36:27.171381:  
2025-03-13 08:36:27.177425: Epoch 78 
2025-03-13 08:36:27.180491: Current learning rate: 0.00256 
2025-03-13 08:36:33.664977: train_loss -0.898 
2025-03-13 08:36:33.671056: val_loss -0.8501 
2025-03-13 08:36:33.674761: Pseudo dice [np.float32(0.898), np.float32(0.8808)] 
2025-03-13 08:36:33.678279: Epoch time: 6.49 s 
2025-03-13 08:36:34.224337:  
2025-03-13 08:36:34.229875: Epoch 79 
2025-03-13 08:36:34.232904: Current learning rate: 0.00245 
2025-03-13 08:36:40.733006: train_loss -0.898 
2025-03-13 08:36:40.738617: val_loss -0.8471 
2025-03-13 08:36:40.742177: Pseudo dice [np.float32(0.8956), np.float32(0.878)] 
2025-03-13 08:36:40.745739: Epoch time: 6.51 s 
2025-03-13 08:36:41.296883:  
2025-03-13 08:36:41.303402: Epoch 80 
2025-03-13 08:36:41.305908: Current learning rate: 0.00235 
2025-03-13 08:36:47.795919: train_loss -0.8999 
2025-03-13 08:36:47.801944: val_loss -0.8444 
2025-03-13 08:36:47.805513: Pseudo dice [np.float32(0.8945), np.float32(0.8772)] 
2025-03-13 08:36:47.808540: Epoch time: 6.5 s 
2025-03-13 08:36:48.360423:  
2025-03-13 08:36:48.366053: Epoch 81 
2025-03-13 08:36:48.368589: Current learning rate: 0.00224 
2025-03-13 08:36:54.846981: train_loss -0.8995 
2025-03-13 08:36:54.853104: val_loss -0.8498 
2025-03-13 08:36:54.856648: Pseudo dice [np.float32(0.8971), np.float32(0.8811)] 
2025-03-13 08:36:54.858708: Epoch time: 6.49 s 
2025-03-13 08:36:55.547167:  
2025-03-13 08:36:55.552239: Epoch 82 
2025-03-13 08:36:55.555754: Current learning rate: 0.00214 
2025-03-13 08:37:02.054187: train_loss -0.8993 
2025-03-13 08:37:02.060409: val_loss -0.851 
2025-03-13 08:37:02.064494: Pseudo dice [np.float32(0.8991), np.float32(0.8813)] 
2025-03-13 08:37:02.067535: Epoch time: 6.51 s 
2025-03-13 08:37:02.589886:  
2025-03-13 08:37:02.594909: Epoch 83 
2025-03-13 08:37:02.597439: Current learning rate: 0.00203 
2025-03-13 08:37:09.075456: train_loss -0.9013 
2025-03-13 08:37:09.082088: val_loss -0.8474 
2025-03-13 08:37:09.085640: Pseudo dice [np.float32(0.8973), np.float32(0.8796)] 
2025-03-13 08:37:09.088708: Epoch time: 6.49 s 
2025-03-13 08:37:09.603443:  
2025-03-13 08:37:09.609511: Epoch 84 
2025-03-13 08:37:09.612021: Current learning rate: 0.00192 
2025-03-13 08:37:16.094131: train_loss -0.9009 
2025-03-13 08:37:16.100688: val_loss -0.8509 
2025-03-13 08:37:16.104240: Pseudo dice [np.float32(0.8974), np.float32(0.8822)] 
2025-03-13 08:37:16.107300: Epoch time: 6.49 s 
2025-03-13 08:37:16.626088:  
2025-03-13 08:37:16.631639: Epoch 85 
2025-03-13 08:37:16.635149: Current learning rate: 0.00181 
2025-03-13 08:37:23.151711: train_loss -0.9009 
2025-03-13 08:37:23.159297: val_loss -0.8441 
2025-03-13 08:37:23.162359: Pseudo dice [np.float32(0.8934), np.float32(0.8766)] 
2025-03-13 08:37:23.165889: Epoch time: 6.53 s 
2025-03-13 08:37:23.678637:  
2025-03-13 08:37:23.684207: Epoch 86 
2025-03-13 08:37:23.687770: Current learning rate: 0.0017 
2025-03-13 08:37:30.159723: train_loss -0.9012 
2025-03-13 08:37:30.165815: val_loss -0.8486 
2025-03-13 08:37:30.168394: Pseudo dice [np.float32(0.8972), np.float32(0.8791)] 
2025-03-13 08:37:30.172938: Epoch time: 6.48 s 
2025-03-13 08:37:30.685622:  
2025-03-13 08:37:30.691174: Epoch 87 
2025-03-13 08:37:30.693710: Current learning rate: 0.00159 
2025-03-13 08:37:37.152965: train_loss -0.9011 
2025-03-13 08:37:37.159035: val_loss -0.8499 
2025-03-13 08:37:37.162708: Pseudo dice [np.float32(0.8983), np.float32(0.8804)] 
2025-03-13 08:37:37.166302: Epoch time: 6.47 s 
2025-03-13 08:37:37.692950:  
2025-03-13 08:37:37.697961: Epoch 88 
2025-03-13 08:37:37.712608: Current learning rate: 0.00148 
2025-03-13 08:37:44.219109: train_loss -0.9022 
2025-03-13 08:37:44.224742: val_loss -0.8412 
2025-03-13 08:37:44.228793: Pseudo dice [np.float32(0.8933), np.float32(0.8749)] 
2025-03-13 08:37:44.231452: Epoch time: 6.53 s 
2025-03-13 08:37:44.745545:  
2025-03-13 08:37:44.750563: Epoch 89 
2025-03-13 08:37:44.754073: Current learning rate: 0.00137 
2025-03-13 08:37:51.251116: train_loss -0.9013 
2025-03-13 08:37:51.257167: val_loss -0.8459 
2025-03-13 08:37:51.260743: Pseudo dice [np.float32(0.8938), np.float32(0.8783)] 
2025-03-13 08:37:51.263791: Epoch time: 6.51 s 
2025-03-13 08:37:51.778981:  
2025-03-13 08:37:51.784534: Epoch 90 
2025-03-13 08:37:51.788050: Current learning rate: 0.00126 
2025-03-13 08:37:58.305882: train_loss -0.9018 
2025-03-13 08:37:58.311576: val_loss -0.8515 
2025-03-13 08:37:58.315619: Pseudo dice [np.float32(0.8991), np.float32(0.8804)] 
2025-03-13 08:37:58.318718: Epoch time: 6.53 s 
2025-03-13 08:37:58.988433:  
2025-03-13 08:37:58.993460: Epoch 91 
2025-03-13 08:37:58.997489: Current learning rate: 0.00115 
2025-03-13 08:38:05.470885: train_loss -0.9018 
2025-03-13 08:38:05.476983: val_loss -0.8486 
2025-03-13 08:38:05.481056: Pseudo dice [np.float32(0.8972), np.float32(0.8799)] 
2025-03-13 08:38:05.484596: Epoch time: 6.48 s 
2025-03-13 08:38:05.995364:  
2025-03-13 08:38:06.001525: Epoch 92 
2025-03-13 08:38:06.005039: Current learning rate: 0.00103 
2025-03-13 08:38:12.486448: train_loss -0.904 
2025-03-13 08:38:12.491016: val_loss -0.8453 
2025-03-13 08:38:12.495559: Pseudo dice [np.float32(0.8949), np.float32(0.8768)] 
2025-03-13 08:38:12.499069: Epoch time: 6.49 s 
2025-03-13 08:38:13.015656:  
2025-03-13 08:38:13.020722: Epoch 93 
2025-03-13 08:38:13.024274: Current learning rate: 0.00091 
2025-03-13 08:38:19.492816: train_loss -0.9031 
2025-03-13 08:38:19.498359: val_loss -0.8462 
2025-03-13 08:38:19.501969: Pseudo dice [np.float32(0.8952), np.float32(0.8795)] 
2025-03-13 08:38:19.505577: Epoch time: 6.48 s 
2025-03-13 08:38:20.016584:  
2025-03-13 08:38:20.022126: Epoch 94 
2025-03-13 08:38:20.025640: Current learning rate: 0.00079 
2025-03-13 08:38:26.509813: train_loss -0.9027 
2025-03-13 08:38:26.517387: val_loss -0.8507 
2025-03-13 08:38:26.521416: Pseudo dice [np.float32(0.8977), np.float32(0.8807)] 
2025-03-13 08:38:26.524447: Epoch time: 6.49 s 
2025-03-13 08:38:27.044483:  
2025-03-13 08:38:27.050546: Epoch 95 
2025-03-13 08:38:27.053608: Current learning rate: 0.00067 
2025-03-13 08:38:33.530473: train_loss -0.9035 
2025-03-13 08:38:33.536016: val_loss -0.8429 
2025-03-13 08:38:33.539527: Pseudo dice [np.float32(0.8928), np.float32(0.8762)] 
2025-03-13 08:38:33.543576: Epoch time: 6.49 s 
2025-03-13 08:38:34.057423:  
2025-03-13 08:38:34.063448: Epoch 96 
2025-03-13 08:38:34.066957: Current learning rate: 0.00055 
2025-03-13 08:38:40.548581: train_loss -0.9033 
2025-03-13 08:38:40.554180: val_loss -0.8455 
2025-03-13 08:38:40.558244: Pseudo dice [np.float32(0.894), np.float32(0.878)] 
2025-03-13 08:38:40.561783: Epoch time: 6.49 s 
2025-03-13 08:38:41.091204:  
2025-03-13 08:38:41.096265: Epoch 97 
2025-03-13 08:38:41.099779: Current learning rate: 0.00043 
2025-03-13 08:38:47.588674: train_loss -0.9044 
2025-03-13 08:38:47.594790: val_loss -0.849 
2025-03-13 08:38:47.598043: Pseudo dice [np.float32(0.8985), np.float32(0.8809)] 
2025-03-13 08:38:47.601136: Epoch time: 6.5 s 
2025-03-13 08:38:48.122589:  
2025-03-13 08:38:48.128158: Epoch 98 
2025-03-13 08:38:48.132180: Current learning rate: 0.0003 
2025-03-13 08:38:54.623009: train_loss -0.9041 
2025-03-13 08:38:54.630150: val_loss -0.8501 
2025-03-13 08:38:54.633753: Pseudo dice [np.float32(0.8971), np.float32(0.8812)] 
2025-03-13 08:38:54.636790: Epoch time: 6.5 s 
2025-03-13 08:38:55.339443:  
2025-03-13 08:38:55.345495: Epoch 99 
2025-03-13 08:38:55.348549: Current learning rate: 0.00016 
2025-03-13 08:39:01.817629: train_loss -0.904 
2025-03-13 08:39:01.823810: val_loss -0.8479 
2025-03-13 08:39:01.827363: Pseudo dice [np.float32(0.8968), np.float32(0.8796)] 
2025-03-13 08:39:01.830405: Epoch time: 6.48 s 
2025-03-13 08:39:02.405681: Training done. 
2025-03-13 08:39:02.440681: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-13 08:39:02.446683: The split file contains 5 splits. 
2025-03-13 08:39:02.452683: Desired fold for training: 0 
2025-03-13 08:39:02.455681: This split has 208 training and 52 validation cases. 
2025-03-13 08:39:02.460681: predicting hippocampus_017 
2025-03-13 08:39:02.465682: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-13 08:39:02.565680: predicting hippocampus_019 
2025-03-13 08:39:02.571681: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-13 08:39:02.614680: predicting hippocampus_033 
2025-03-13 08:39:02.620682: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-13 08:39:02.646683: predicting hippocampus_035 
2025-03-13 08:39:02.652683: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-13 08:39:02.678680: predicting hippocampus_037 
2025-03-13 08:39:02.684681: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-13 08:39:02.710681: predicting hippocampus_049 
2025-03-13 08:39:02.717681: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-13 08:39:02.745680: predicting hippocampus_052 
2025-03-13 08:39:02.751681: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-13 08:39:02.777681: predicting hippocampus_065 
2025-03-13 08:39:02.784681: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-13 08:39:02.810685: predicting hippocampus_083 
2025-03-13 08:39:02.817681: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-13 08:39:02.847681: predicting hippocampus_088 
2025-03-13 08:39:02.854683: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-13 08:39:06.099846: predicting hippocampus_090 
2025-03-13 08:39:06.105850: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-13 08:39:06.142850: predicting hippocampus_092 
2025-03-13 08:39:06.149850: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-13 08:39:06.189851: predicting hippocampus_095 
2025-03-13 08:39:06.197850: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-13 08:39:06.236849: predicting hippocampus_107 
2025-03-13 08:39:06.244852: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-13 08:39:06.283851: predicting hippocampus_108 
2025-03-13 08:39:06.296851: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-13 08:39:06.332852: predicting hippocampus_123 
2025-03-13 08:39:06.340850: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-13 08:39:06.375851: predicting hippocampus_125 
2025-03-13 08:39:06.386851: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-13 08:39:06.451851: predicting hippocampus_157 
2025-03-13 08:39:06.459849: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-13 08:39:06.495850: predicting hippocampus_164 
2025-03-13 08:39:06.506851: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-13 08:39:06.592849: predicting hippocampus_169 
2025-03-13 08:39:06.600849: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-13 08:39:06.627849: predicting hippocampus_175 
2025-03-13 08:39:06.631850: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-13 08:39:06.658849: predicting hippocampus_185 
2025-03-13 08:39:06.663851: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-13 08:39:06.692849: predicting hippocampus_190 
2025-03-13 08:39:06.697850: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-13 08:39:06.723849: predicting hippocampus_194 
2025-03-13 08:39:06.728849: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-13 08:39:06.754849: predicting hippocampus_204 
2025-03-13 08:39:06.760849: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-13 08:39:06.786849: predicting hippocampus_205 
2025-03-13 08:39:06.791849: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-13 08:39:06.818849: predicting hippocampus_210 
2025-03-13 08:39:06.824851: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-13 08:39:06.851851: predicting hippocampus_217 
2025-03-13 08:39:06.858850: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-13 08:39:06.884849: predicting hippocampus_219 
2025-03-13 08:39:06.891850: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-13 08:39:06.918849: predicting hippocampus_229 
2025-03-13 08:39:06.925850: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-13 08:39:06.953849: predicting hippocampus_244 
2025-03-13 08:39:06.961850: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-13 08:39:06.988849: predicting hippocampus_261 
2025-03-13 08:39:06.994849: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-13 08:39:07.039849: predicting hippocampus_264 
2025-03-13 08:39:07.044850: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-13 08:39:07.070849: predicting hippocampus_277 
2025-03-13 08:39:07.075850: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-13 08:39:07.117849: predicting hippocampus_280 
2025-03-13 08:39:07.123849: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-13 08:39:07.149849: predicting hippocampus_286 
2025-03-13 08:39:07.154851: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-13 08:39:07.198849: predicting hippocampus_288 
2025-03-13 08:39:07.203849: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-13 08:39:07.247849: predicting hippocampus_289 
2025-03-13 08:39:07.255851: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-13 08:39:07.282849: predicting hippocampus_296 
2025-03-13 08:39:07.287850: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-13 08:39:07.313849: predicting hippocampus_305 
2025-03-13 08:39:07.318851: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-13 08:39:07.344849: predicting hippocampus_308 
2025-03-13 08:39:07.349851: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-13 08:39:07.376849: predicting hippocampus_317 
2025-03-13 08:39:07.381850: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-13 08:39:07.407849: predicting hippocampus_327 
2025-03-13 08:39:07.411851: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-13 08:39:07.439849: predicting hippocampus_330 
2025-03-13 08:39:07.445850: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-13 08:39:07.471849: predicting hippocampus_332 
2025-03-13 08:39:07.476850: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-13 08:39:07.503849: predicting hippocampus_338 
2025-03-13 08:39:07.508849: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-13 08:39:07.553849: predicting hippocampus_349 
2025-03-13 08:39:07.558851: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-13 08:39:07.584851: predicting hippocampus_350 
2025-03-13 08:39:07.589850: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-13 08:39:07.615849: predicting hippocampus_356 
2025-03-13 08:39:07.620849: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-13 08:39:07.647849: predicting hippocampus_358 
2025-03-13 08:39:07.652850: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-13 08:39:07.680849: predicting hippocampus_374 
2025-03-13 08:39:07.685850: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-13 08:39:07.712849: predicting hippocampus_394 
2025-03-13 08:39:07.717849: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-13 08:39:11.117130: Validation complete 
2025-03-13 08:39:11.122128: Mean Validation Dice:  0.8914239490876245 
