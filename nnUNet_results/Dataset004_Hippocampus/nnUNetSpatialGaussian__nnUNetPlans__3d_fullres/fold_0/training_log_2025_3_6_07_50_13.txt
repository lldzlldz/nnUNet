
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-06 07:50:13.269932: do_dummy_2d_data_aug: False 
2025-03-06 07:50:13.309497: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-06 07:50:13.316498: The split file contains 5 splits. 
2025-03-06 07:50:13.319498: Desired fold for training: 0 
2025-03-06 07:50:13.322498: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-06 07:50:19.833571: unpacking dataset... 
2025-03-06 07:50:20.150781: unpacking done... 
2025-03-06 07:50:22.264994:  
2025-03-06 07:50:22.269000: Epoch 0 
2025-03-06 07:50:22.272511: Current learning rate: 0.01 
2025-03-06 07:50:29.823950: train_loss -0.5096 
2025-03-06 07:50:29.829573: val_loss -0.8065 
2025-03-06 07:50:29.832613: Pseudo dice [np.float32(0.8626), np.float32(0.8531)] 
2025-03-06 07:50:29.836129: Epoch time: 7.56 s 
2025-03-06 07:50:29.839191: Yayy! New best EMA pseudo Dice: 0.8578000068664551 
2025-03-06 07:50:30.356310:  
2025-03-06 07:50:30.361894: Epoch 1 
2025-03-06 07:50:30.364449: Current learning rate: 0.00991 
2025-03-06 07:50:37.003027: train_loss -0.8226 
2025-03-06 07:50:37.008816: val_loss -0.8301 
2025-03-06 07:50:37.012842: Pseudo dice [np.float32(0.8808), np.float32(0.8654)] 
2025-03-06 07:50:37.015694: Epoch time: 6.65 s 
2025-03-06 07:50:37.019210: Yayy! New best EMA pseudo Dice: 0.8593000173568726 
2025-03-06 07:50:37.602683:  
2025-03-06 07:50:37.608261: Epoch 2 
2025-03-06 07:50:37.610864: Current learning rate: 0.00982 
2025-03-06 07:50:44.240931: train_loss -0.8418 
2025-03-06 07:50:44.247518: val_loss -0.8339 
2025-03-06 07:50:44.251079: Pseudo dice [np.float32(0.8846), np.float32(0.8686)] 
2025-03-06 07:50:44.253620: Epoch time: 6.64 s 
2025-03-06 07:50:44.256160: Yayy! New best EMA pseudo Dice: 0.8611000180244446 
2025-03-06 07:50:44.865120:  
2025-03-06 07:50:44.868629: Epoch 3 
2025-03-06 07:50:44.871298: Current learning rate: 0.00973 
2025-03-06 07:50:51.501321: train_loss -0.8536 
2025-03-06 07:50:51.506389: val_loss -0.8435 
2025-03-06 07:50:51.510928: Pseudo dice [np.float32(0.8915), np.float32(0.8755)] 
2025-03-06 07:50:51.513971: Epoch time: 6.64 s 
2025-03-06 07:50:51.516048: Yayy! New best EMA pseudo Dice: 0.8633000254631042 
2025-03-06 07:50:52.097404:  
2025-03-06 07:50:52.102416: Epoch 4 
2025-03-06 07:50:52.105925: Current learning rate: 0.00964 
2025-03-06 07:50:58.713679: train_loss -0.8617 
2025-03-06 07:50:58.719256: val_loss -0.8459 
2025-03-06 07:50:58.722797: Pseudo dice [np.float32(0.8931), np.float32(0.879)] 
2025-03-06 07:50:58.725950: Epoch time: 6.62 s 
2025-03-06 07:50:58.729021: Yayy! New best EMA pseudo Dice: 0.8655999898910522 
2025-03-06 07:50:59.450811:  
2025-03-06 07:50:59.456397: Epoch 5 
2025-03-06 07:50:59.459446: Current learning rate: 0.00955 
2025-03-06 07:51:06.085238: train_loss -0.8665 
2025-03-06 07:51:06.091885: val_loss -0.8433 
2025-03-06 07:51:06.094972: Pseudo dice [np.float32(0.8937), np.float32(0.8749)] 
2025-03-06 07:51:06.098503: Epoch time: 6.63 s 
2025-03-06 07:51:06.101055: Yayy! New best EMA pseudo Dice: 0.8675000071525574 
2025-03-06 07:51:06.674315:  
2025-03-06 07:51:06.680335: Epoch 6 
2025-03-06 07:51:06.683867: Current learning rate: 0.00946 
2025-03-06 07:51:13.304899: train_loss -0.8741 
2025-03-06 07:51:13.311259: val_loss -0.8521 
2025-03-06 07:51:13.314517: Pseudo dice [np.float32(0.8997), np.float32(0.8809)] 
2025-03-06 07:51:13.318128: Epoch time: 6.63 s 
2025-03-06 07:51:13.321188: Yayy! New best EMA pseudo Dice: 0.869700014591217 
2025-03-06 07:51:13.906387:  
2025-03-06 07:51:13.911967: Epoch 7 
2025-03-06 07:51:13.914517: Current learning rate: 0.00937 
2025-03-06 07:51:20.530943: train_loss -0.8779 
2025-03-06 07:51:20.537022: val_loss -0.8487 
2025-03-06 07:51:20.539596: Pseudo dice [np.float32(0.8971), np.float32(0.8788)] 
2025-03-06 07:51:20.543633: Epoch time: 6.62 s 
2025-03-06 07:51:20.546742: Yayy! New best EMA pseudo Dice: 0.8715999722480774 
2025-03-06 07:51:21.143254:  
2025-03-06 07:51:21.147306: Epoch 8 
2025-03-06 07:51:21.151957: Current learning rate: 0.00928 
2025-03-06 07:51:27.766983: train_loss -0.8839 
2025-03-06 07:51:27.772998: val_loss -0.8457 
2025-03-06 07:51:27.776007: Pseudo dice [np.float32(0.8952), np.float32(0.8773)] 
2025-03-06 07:51:27.778552: Epoch time: 6.62 s 
2025-03-06 07:51:27.781683: Yayy! New best EMA pseudo Dice: 0.8730000257492065 
2025-03-06 07:51:28.382382:  
2025-03-06 07:51:28.387899: Epoch 9 
2025-03-06 07:51:28.390405: Current learning rate: 0.00919 
2025-03-06 07:51:35.002510: train_loss -0.8852 
2025-03-06 07:51:35.008589: val_loss -0.8439 
2025-03-06 07:51:35.011744: Pseudo dice [np.float32(0.8964), np.float32(0.8746)] 
2025-03-06 07:51:35.014813: Epoch time: 6.62 s 
2025-03-06 07:51:35.017857: Yayy! New best EMA pseudo Dice: 0.8743000030517578 
2025-03-06 07:51:35.589072:  
2025-03-06 07:51:35.595133: Epoch 10 
2025-03-06 07:51:35.597678: Current learning rate: 0.0091 
2025-03-06 07:51:42.205829: train_loss -0.8907 
2025-03-06 07:51:42.211930: val_loss -0.8474 
2025-03-06 07:51:42.215590: Pseudo dice [np.float32(0.8986), np.float32(0.8773)] 
2025-03-06 07:51:42.218619: Epoch time: 6.62 s 
2025-03-06 07:51:42.221637: Yayy! New best EMA pseudo Dice: 0.8755999803543091 
2025-03-06 07:51:42.797192:  
2025-03-06 07:51:42.802757: Epoch 11 
2025-03-06 07:51:42.805277: Current learning rate: 0.009 
2025-03-06 07:51:49.415462: train_loss -0.8946 
2025-03-06 07:51:49.421047: val_loss -0.8518 
2025-03-06 07:51:49.424606: Pseudo dice [np.float32(0.9005), np.float32(0.8813)] 
2025-03-06 07:51:49.428186: Epoch time: 6.62 s 
2025-03-06 07:51:49.431244: Yayy! New best EMA pseudo Dice: 0.8772000074386597 
2025-03-06 07:51:50.010506:  
2025-03-06 07:51:50.016040: Epoch 12 
2025-03-06 07:51:50.019063: Current learning rate: 0.00891 
2025-03-06 07:51:56.632635: train_loss -0.8947 
2025-03-06 07:51:56.638222: val_loss -0.8447 
2025-03-06 07:51:56.641292: Pseudo dice [np.float32(0.8951), np.float32(0.8776)] 
2025-03-06 07:51:56.644836: Epoch time: 6.62 s 
2025-03-06 07:51:56.648379: Yayy! New best EMA pseudo Dice: 0.8780999779701233 
2025-03-06 07:51:57.386771:  
2025-03-06 07:51:57.391806: Epoch 13 
2025-03-06 07:51:57.394040: Current learning rate: 0.00882 
2025-03-06 07:52:04.009524: train_loss -0.8988 
2025-03-06 07:52:04.015037: val_loss -0.8403 
2025-03-06 07:52:04.018546: Pseudo dice [np.float32(0.8938), np.float32(0.8746)] 
2025-03-06 07:52:04.021054: Epoch time: 6.62 s 
2025-03-06 07:52:04.024559: Yayy! New best EMA pseudo Dice: 0.8787000179290771 
2025-03-06 07:52:04.613674:  
2025-03-06 07:52:04.618981: Epoch 14 
2025-03-06 07:52:04.622537: Current learning rate: 0.00873 
2025-03-06 07:52:11.233078: train_loss -0.8984 
2025-03-06 07:52:11.241870: val_loss -0.8437 
2025-03-06 07:52:11.244942: Pseudo dice [np.float32(0.8946), np.float32(0.8782)] 
2025-03-06 07:52:11.248497: Epoch time: 6.62 s 
2025-03-06 07:52:11.251515: Yayy! New best EMA pseudo Dice: 0.8794999718666077 
2025-03-06 07:52:11.843822:  
2025-03-06 07:52:11.848835: Epoch 15 
2025-03-06 07:52:11.852348: Current learning rate: 0.00864 
2025-03-06 07:52:18.454587: train_loss -0.9037 
2025-03-06 07:52:18.461776: val_loss -0.8424 
2025-03-06 07:52:18.465309: Pseudo dice [np.float32(0.8958), np.float32(0.8765)] 
2025-03-06 07:52:18.467846: Epoch time: 6.61 s 
2025-03-06 07:52:18.470883: Yayy! New best EMA pseudo Dice: 0.8801000118255615 
2025-03-06 07:52:19.063888:  
2025-03-06 07:52:19.069934: Epoch 16 
2025-03-06 07:52:19.072974: Current learning rate: 0.00855 
2025-03-06 07:52:25.683101: train_loss -0.9052 
2025-03-06 07:52:25.689224: val_loss -0.8404 
2025-03-06 07:52:25.692262: Pseudo dice [np.float32(0.8941), np.float32(0.8757)] 
2025-03-06 07:52:25.695280: Epoch time: 6.62 s 
2025-03-06 07:52:25.698067: Yayy! New best EMA pseudo Dice: 0.8805999755859375 
2025-03-06 07:52:26.305719:  
2025-03-06 07:52:26.311874: Epoch 17 
2025-03-06 07:52:26.314461: Current learning rate: 0.00846 
2025-03-06 07:52:32.926818: train_loss -0.9063 
2025-03-06 07:52:32.932440: val_loss -0.8454 
2025-03-06 07:52:32.936522: Pseudo dice [np.float32(0.8977), np.float32(0.8791)] 
2025-03-06 07:52:32.939596: Epoch time: 6.62 s 
2025-03-06 07:52:32.942726: Yayy! New best EMA pseudo Dice: 0.8813999891281128 
2025-03-06 07:52:33.543513:  
2025-03-06 07:52:33.549076: Epoch 18 
2025-03-06 07:52:33.553188: Current learning rate: 0.00836 
2025-03-06 07:52:40.154069: train_loss -0.9083 
2025-03-06 07:52:40.159639: val_loss -0.8367 
2025-03-06 07:52:40.162667: Pseudo dice [np.float32(0.8921), np.float32(0.8734)] 
2025-03-06 07:52:40.166288: Epoch time: 6.61 s 
2025-03-06 07:52:40.168807: Yayy! New best EMA pseudo Dice: 0.8815000057220459 
2025-03-06 07:52:40.774738:  
2025-03-06 07:52:40.780780: Epoch 19 
2025-03-06 07:52:40.783833: Current learning rate: 0.00827 
2025-03-06 07:52:47.390310: train_loss -0.9101 
2025-03-06 07:52:47.396561: val_loss -0.8408 
2025-03-06 07:52:47.400133: Pseudo dice [np.float32(0.8957), np.float32(0.8768)] 
2025-03-06 07:52:47.403180: Epoch time: 6.62 s 
2025-03-06 07:52:47.407223: Yayy! New best EMA pseudo Dice: 0.8820000290870667 
2025-03-06 07:52:48.015703:  
2025-03-06 07:52:48.020726: Epoch 20 
2025-03-06 07:52:48.024746: Current learning rate: 0.00818 
2025-03-06 07:52:54.635982: train_loss -0.9111 
2025-03-06 07:52:54.641585: val_loss -0.8475 
2025-03-06 07:52:54.646125: Pseudo dice [np.float32(0.8992), np.float32(0.8806)] 
2025-03-06 07:52:54.649199: Epoch time: 6.62 s 
2025-03-06 07:52:54.652747: Yayy! New best EMA pseudo Dice: 0.8827999830245972 
2025-03-06 07:52:55.407830:  
2025-03-06 07:52:55.413398: Epoch 21 
2025-03-06 07:52:55.416942: Current learning rate: 0.00809 
2025-03-06 07:53:02.019296: train_loss -0.9119 
2025-03-06 07:53:02.025347: val_loss -0.8412 
2025-03-06 07:53:02.028456: Pseudo dice [np.float32(0.8961), np.float32(0.8773)] 
2025-03-06 07:53:02.032011: Epoch time: 6.61 s 
2025-03-06 07:53:02.034573: Yayy! New best EMA pseudo Dice: 0.8831999897956848 
2025-03-06 07:53:02.613898:  
2025-03-06 07:53:02.618946: Epoch 22 
2025-03-06 07:53:02.622497: Current learning rate: 0.008 
2025-03-06 07:53:09.232009: train_loss -0.9122 
2025-03-06 07:53:09.237599: val_loss -0.8375 
2025-03-06 07:53:09.241659: Pseudo dice [np.float32(0.8926), np.float32(0.8742)] 
2025-03-06 07:53:09.244759: Epoch time: 6.62 s 
2025-03-06 07:53:09.249323: Yayy! New best EMA pseudo Dice: 0.8831999897956848 
2025-03-06 07:53:09.826494:  
2025-03-06 07:53:09.832012: Epoch 23 
2025-03-06 07:53:09.835524: Current learning rate: 0.0079 
2025-03-06 07:53:16.450514: train_loss -0.9137 
2025-03-06 07:53:16.457103: val_loss -0.8421 
2025-03-06 07:53:16.460671: Pseudo dice [np.float32(0.8972), np.float32(0.8772)] 
2025-03-06 07:53:16.463644: Epoch time: 6.63 s 
2025-03-06 07:53:16.467160: Yayy! New best EMA pseudo Dice: 0.8835999965667725 
2025-03-06 07:53:17.034605:  
2025-03-06 07:53:17.038620: Epoch 24 
2025-03-06 07:53:17.042632: Current learning rate: 0.00781 
2025-03-06 07:53:23.647531: train_loss -0.9154 
2025-03-06 07:53:23.653617: val_loss -0.8405 
2025-03-06 07:53:23.657746: Pseudo dice [np.float32(0.8955), np.float32(0.8779)] 
2025-03-06 07:53:23.660606: Epoch time: 6.61 s 
2025-03-06 07:53:23.664122: Yayy! New best EMA pseudo Dice: 0.883899986743927 
2025-03-06 07:53:24.242564:  
2025-03-06 07:53:24.248127: Epoch 25 
2025-03-06 07:53:24.252187: Current learning rate: 0.00772 
2025-03-06 07:53:30.862309: train_loss -0.9176 
2025-03-06 07:53:30.868397: val_loss -0.8403 
2025-03-06 07:53:30.871933: Pseudo dice [np.float32(0.8976), np.float32(0.8766)] 
2025-03-06 07:53:30.875454: Epoch time: 6.62 s 
2025-03-06 07:53:30.877847: Yayy! New best EMA pseudo Dice: 0.8841999769210815 
2025-03-06 07:53:31.454079:  
2025-03-06 07:53:31.459091: Epoch 26 
2025-03-06 07:53:31.463604: Current learning rate: 0.00763 
2025-03-06 07:53:38.087995: train_loss -0.9167 
2025-03-06 07:53:38.094080: val_loss -0.8391 
2025-03-06 07:53:38.098160: Pseudo dice [np.float32(0.8956), np.float32(0.8759)] 
2025-03-06 07:53:38.101778: Epoch time: 6.63 s 
2025-03-06 07:53:38.104870: Yayy! New best EMA pseudo Dice: 0.8844000101089478 
2025-03-06 07:53:38.687723:  
2025-03-06 07:53:38.693743: Epoch 27 
2025-03-06 07:53:38.697250: Current learning rate: 0.00753 
2025-03-06 07:53:45.300467: train_loss -0.917 
2025-03-06 07:53:45.307040: val_loss -0.84 
2025-03-06 07:53:45.310596: Pseudo dice [np.float32(0.8963), np.float32(0.8776)] 
2025-03-06 07:53:45.314154: Epoch time: 6.61 s 
2025-03-06 07:53:45.317790: Yayy! New best EMA pseudo Dice: 0.8845999836921692 
2025-03-06 07:53:45.901141:  
2025-03-06 07:53:45.907205: Epoch 28 
2025-03-06 07:53:45.911261: Current learning rate: 0.00744 
2025-03-06 07:53:52.510029: train_loss -0.9203 
2025-03-06 07:53:52.516142: val_loss -0.8394 
2025-03-06 07:53:52.519708: Pseudo dice [np.float32(0.8966), np.float32(0.8764)] 
2025-03-06 07:53:52.522759: Epoch time: 6.61 s 
2025-03-06 07:53:52.526824: Yayy! New best EMA pseudo Dice: 0.8848000168800354 
2025-03-06 07:53:53.240511:  
2025-03-06 07:53:53.247028: Epoch 29 
2025-03-06 07:53:53.250539: Current learning rate: 0.00735 
2025-03-06 07:53:59.865361: train_loss -0.9196 
2025-03-06 07:53:59.871445: val_loss -0.8408 
2025-03-06 07:53:59.875344: Pseudo dice [np.float32(0.8974), np.float32(0.8782)] 
2025-03-06 07:53:59.878380: Epoch time: 6.62 s 
2025-03-06 07:53:59.881891: Yayy! New best EMA pseudo Dice: 0.8851000070571899 
2025-03-06 07:54:00.461053:  
2025-03-06 07:54:00.466599: Epoch 30 
2025-03-06 07:54:00.470111: Current learning rate: 0.00725 
2025-03-06 07:54:07.083405: train_loss -0.9207 
2025-03-06 07:54:07.089425: val_loss -0.8348 
2025-03-06 07:54:07.093434: Pseudo dice [np.float32(0.8951), np.float32(0.8751)] 
2025-03-06 07:54:07.097457: Epoch time: 6.62 s 
2025-03-06 07:54:07.645695:  
2025-03-06 07:54:07.652240: Epoch 31 
2025-03-06 07:54:07.655798: Current learning rate: 0.00716 
2025-03-06 07:54:14.268412: train_loss -0.9212 
2025-03-06 07:54:14.274548: val_loss -0.8416 
2025-03-06 07:54:14.278084: Pseudo dice [np.float32(0.8979), np.float32(0.8784)] 
2025-03-06 07:54:14.281671: Epoch time: 6.62 s 
2025-03-06 07:54:14.285212: Yayy! New best EMA pseudo Dice: 0.8853999972343445 
2025-03-06 07:54:14.870345:  
2025-03-06 07:54:14.876395: Epoch 32 
2025-03-06 07:54:14.879448: Current learning rate: 0.00707 
2025-03-06 07:54:21.482036: train_loss -0.9228 
2025-03-06 07:54:21.489464: val_loss -0.8359 
2025-03-06 07:54:21.492505: Pseudo dice [np.float32(0.8951), np.float32(0.8756)] 
2025-03-06 07:54:21.498655: Epoch time: 6.61 s 
2025-03-06 07:54:22.049925:  
2025-03-06 07:54:22.056492: Epoch 33 
2025-03-06 07:54:22.060533: Current learning rate: 0.00697 
2025-03-06 07:54:28.681623: train_loss -0.9213 
2025-03-06 07:54:28.688230: val_loss -0.8291 
2025-03-06 07:54:28.691786: Pseudo dice [np.float32(0.8926), np.float32(0.8719)] 
2025-03-06 07:54:28.694926: Epoch time: 6.63 s 
2025-03-06 07:54:29.252002:  
2025-03-06 07:54:29.257518: Epoch 34 
2025-03-06 07:54:29.261030: Current learning rate: 0.00688 
2025-03-06 07:54:35.875443: train_loss -0.9233 
2025-03-06 07:54:35.881663: val_loss -0.8386 
2025-03-06 07:54:35.884820: Pseudo dice [np.float32(0.8979), np.float32(0.878)] 
2025-03-06 07:54:35.888330: Epoch time: 6.62 s 
2025-03-06 07:54:36.440202:  
2025-03-06 07:54:36.446723: Epoch 35 
2025-03-06 07:54:36.450234: Current learning rate: 0.00679 
2025-03-06 07:54:43.042962: train_loss -0.9246 
2025-03-06 07:54:43.049222: val_loss -0.8303 
2025-03-06 07:54:43.052301: Pseudo dice [np.float32(0.894), np.float32(0.8731)] 
2025-03-06 07:54:43.054941: Epoch time: 6.6 s 
2025-03-06 07:54:43.763396:  
2025-03-06 07:54:43.768983: Epoch 36 
2025-03-06 07:54:43.772074: Current learning rate: 0.00669 
2025-03-06 07:54:50.378066: train_loss -0.9248 
2025-03-06 07:54:50.383156: val_loss -0.836 
2025-03-06 07:54:50.387223: Pseudo dice [np.float32(0.8939), np.float32(0.878)] 
2025-03-06 07:54:50.390254: Epoch time: 6.61 s 
2025-03-06 07:54:50.947981:  
2025-03-06 07:54:50.953501: Epoch 37 
2025-03-06 07:54:50.957013: Current learning rate: 0.0066 
2025-03-06 07:54:57.566048: train_loss -0.9253 
2025-03-06 07:54:57.571062: val_loss -0.8337 
2025-03-06 07:54:57.574575: Pseudo dice [np.float32(0.8938), np.float32(0.8772)] 
2025-03-06 07:54:57.578083: Epoch time: 6.62 s 
2025-03-06 07:54:58.139184:  
2025-03-06 07:54:58.144701: Epoch 38 
2025-03-06 07:54:58.148213: Current learning rate: 0.0065 
2025-03-06 07:55:04.726287: train_loss -0.9255 
2025-03-06 07:55:04.733311: val_loss -0.8381 
2025-03-06 07:55:04.735332: Pseudo dice [np.float32(0.8978), np.float32(0.8782)] 
2025-03-06 07:55:04.739285: Epoch time: 6.59 s 
2025-03-06 07:55:04.741806: Yayy! New best EMA pseudo Dice: 0.8855999708175659 
2025-03-06 07:55:05.339802:  
2025-03-06 07:55:05.345330: Epoch 39 
2025-03-06 07:55:05.348847: Current learning rate: 0.00641 
2025-03-06 07:55:11.955206: train_loss -0.9278 
2025-03-06 07:55:11.961281: val_loss -0.8343 
2025-03-06 07:55:11.964786: Pseudo dice [np.float32(0.8954), np.float32(0.8774)] 
2025-03-06 07:55:11.967812: Epoch time: 6.62 s 
2025-03-06 07:55:11.970827: Yayy! New best EMA pseudo Dice: 0.885699987411499 
2025-03-06 07:55:12.576571:  
2025-03-06 07:55:12.582086: Epoch 40 
2025-03-06 07:55:12.585599: Current learning rate: 0.00631 
2025-03-06 07:55:19.189751: train_loss -0.9288 
2025-03-06 07:55:19.194726: val_loss -0.8257 
2025-03-06 07:55:19.198236: Pseudo dice [np.float32(0.8904), np.float32(0.8727)] 
2025-03-06 07:55:19.201743: Epoch time: 6.61 s 
2025-03-06 07:55:19.774153:  
2025-03-06 07:55:19.780705: Epoch 41 
2025-03-06 07:55:19.785768: Current learning rate: 0.00622 
2025-03-06 07:55:26.398341: train_loss -0.9288 
2025-03-06 07:55:26.403441: val_loss -0.833 
2025-03-06 07:55:26.407494: Pseudo dice [np.float32(0.8944), np.float32(0.8758)] 
2025-03-06 07:55:26.410551: Epoch time: 6.63 s 
2025-03-06 07:55:26.938686:  
2025-03-06 07:55:26.944264: Epoch 42 
2025-03-06 07:55:26.947377: Current learning rate: 0.00612 
2025-03-06 07:55:33.560045: train_loss -0.9282 
2025-03-06 07:55:33.566561: val_loss -0.8337 
2025-03-06 07:55:33.571617: Pseudo dice [np.float32(0.8952), np.float32(0.8757)] 
2025-03-06 07:55:33.574667: Epoch time: 6.62 s 
2025-03-06 07:55:34.109138:  
2025-03-06 07:55:34.114703: Epoch 43 
2025-03-06 07:55:34.117754: Current learning rate: 0.00603 
2025-03-06 07:55:40.733581: train_loss -0.929 
2025-03-06 07:55:40.740139: val_loss -0.8319 
2025-03-06 07:55:40.743183: Pseudo dice [np.float32(0.8938), np.float32(0.8753)] 
2025-03-06 07:55:40.745735: Epoch time: 6.62 s 
2025-03-06 07:55:41.427228:  
2025-03-06 07:55:41.432865: Epoch 44 
2025-03-06 07:55:41.436403: Current learning rate: 0.00593 
2025-03-06 07:55:48.036712: train_loss -0.9304 
2025-03-06 07:55:48.041725: val_loss -0.835 
2025-03-06 07:55:48.045740: Pseudo dice [np.float32(0.8951), np.float32(0.8782)] 
2025-03-06 07:55:48.048292: Epoch time: 6.61 s 
2025-03-06 07:55:48.619564:  
2025-03-06 07:55:48.625108: Epoch 45 
2025-03-06 07:55:48.628145: Current learning rate: 0.00584 
2025-03-06 07:55:55.224486: train_loss -0.9302 
2025-03-06 07:55:55.230047: val_loss -0.8329 
2025-03-06 07:55:55.233619: Pseudo dice [np.float32(0.8955), np.float32(0.877)] 
2025-03-06 07:55:55.237645: Epoch time: 6.61 s 
2025-03-06 07:55:55.771626:  
2025-03-06 07:55:55.777230: Epoch 46 
2025-03-06 07:55:55.780322: Current learning rate: 0.00574 
2025-03-06 07:56:02.380875: train_loss -0.9312 
2025-03-06 07:56:02.386497: val_loss -0.8358 
2025-03-06 07:56:02.389554: Pseudo dice [np.float32(0.897), np.float32(0.8792)] 
2025-03-06 07:56:02.392112: Epoch time: 6.61 s 
2025-03-06 07:56:02.395632: Yayy! New best EMA pseudo Dice: 0.885699987411499 
2025-03-06 07:56:02.962757:  
2025-03-06 07:56:02.968290: Epoch 47 
2025-03-06 07:56:02.971320: Current learning rate: 0.00565 
2025-03-06 07:56:09.572860: train_loss -0.9299 
2025-03-06 07:56:09.578450: val_loss -0.8315 
2025-03-06 07:56:09.582026: Pseudo dice [np.float32(0.8949), np.float32(0.8754)] 
2025-03-06 07:56:09.585586: Epoch time: 6.61 s 
2025-03-06 07:56:10.119116:  
2025-03-06 07:56:10.124128: Epoch 48 
2025-03-06 07:56:10.127639: Current learning rate: 0.00555 
2025-03-06 07:56:16.740258: train_loss -0.9319 
2025-03-06 07:56:16.746937: val_loss -0.8264 
2025-03-06 07:56:16.749974: Pseudo dice [np.float32(0.8934), np.float32(0.8742)] 
2025-03-06 07:56:16.753015: Epoch time: 6.62 s 
2025-03-06 07:56:17.289092:  
2025-03-06 07:56:17.294675: Epoch 49 
2025-03-06 07:56:17.297221: Current learning rate: 0.00546 
2025-03-06 07:56:23.910691: train_loss -0.9327 
2025-03-06 07:56:23.917361: val_loss -0.8315 
2025-03-06 07:56:23.920414: Pseudo dice [np.float32(0.8946), np.float32(0.8765)] 
2025-03-06 07:56:23.922441: Epoch time: 6.62 s 
2025-03-06 07:56:24.500774:  
2025-03-06 07:56:24.506338: Epoch 50 
2025-03-06 07:56:24.509928: Current learning rate: 0.00536 
2025-03-06 07:56:31.104814: train_loss -0.9326 
2025-03-06 07:56:31.110374: val_loss -0.8327 
2025-03-06 07:56:31.113887: Pseudo dice [np.float32(0.8966), np.float32(0.8777)] 
2025-03-06 07:56:31.116899: Epoch time: 6.6 s 
2025-03-06 07:56:31.655164:  
2025-03-06 07:56:31.660709: Epoch 51 
2025-03-06 07:56:31.663749: Current learning rate: 0.00526 
2025-03-06 07:56:38.273095: train_loss -0.9325 
2025-03-06 07:56:38.279776: val_loss -0.8356 
2025-03-06 07:56:38.283324: Pseudo dice [np.float32(0.8966), np.float32(0.8797)] 
2025-03-06 07:56:38.285850: Epoch time: 6.62 s 
2025-03-06 07:56:38.289881: Yayy! New best EMA pseudo Dice: 0.8859000205993652 
2025-03-06 07:56:38.896730:  
2025-03-06 07:56:38.902763: Epoch 52 
2025-03-06 07:56:38.905269: Current learning rate: 0.00517 
2025-03-06 07:56:45.513301: train_loss -0.9359 
2025-03-06 07:56:45.519404: val_loss -0.8334 
2025-03-06 07:56:45.522434: Pseudo dice [np.float32(0.8971), np.float32(0.8778)] 
2025-03-06 07:56:45.525456: Epoch time: 6.62 s 
2025-03-06 07:56:45.528484: Yayy! New best EMA pseudo Dice: 0.8860999941825867 
2025-03-06 07:56:46.262375:  
2025-03-06 07:56:46.267390: Epoch 53 
2025-03-06 07:56:46.270901: Current learning rate: 0.00507 
2025-03-06 07:56:52.865566: train_loss -0.9352 
2025-03-06 07:56:52.872100: val_loss -0.8265 
2025-03-06 07:56:52.875652: Pseudo dice [np.float32(0.8941), np.float32(0.8742)] 
2025-03-06 07:56:52.878693: Epoch time: 6.6 s 
2025-03-06 07:56:53.415725:  
2025-03-06 07:56:53.422240: Epoch 54 
2025-03-06 07:56:53.425751: Current learning rate: 0.00497 
2025-03-06 07:57:00.044103: train_loss -0.9328 
2025-03-06 07:57:00.049168: val_loss -0.8313 
2025-03-06 07:57:00.053219: Pseudo dice [np.float32(0.8963), np.float32(0.8774)] 
2025-03-06 07:57:00.056272: Epoch time: 6.63 s 
2025-03-06 07:57:00.600752:  
2025-03-06 07:57:00.605766: Epoch 55 
2025-03-06 07:57:00.609278: Current learning rate: 0.00487 
2025-03-06 07:57:07.214222: train_loss -0.9356 
2025-03-06 07:57:07.219817: val_loss -0.8254 
2025-03-06 07:57:07.223845: Pseudo dice [np.float32(0.8928), np.float32(0.8744)] 
2025-03-06 07:57:07.226895: Epoch time: 6.61 s 
2025-03-06 07:57:07.763926:  
2025-03-06 07:57:07.769940: Epoch 56 
2025-03-06 07:57:07.772953: Current learning rate: 0.00478 
2025-03-06 07:57:14.374167: train_loss -0.935 
2025-03-06 07:57:14.380763: val_loss -0.8286 
2025-03-06 07:57:14.384337: Pseudo dice [np.float32(0.894), np.float32(0.8758)] 
2025-03-06 07:57:14.386939: Epoch time: 6.61 s 
2025-03-06 07:57:14.931040:  
2025-03-06 07:57:14.936565: Epoch 57 
2025-03-06 07:57:14.939582: Current learning rate: 0.00468 
2025-03-06 07:57:21.550159: train_loss -0.9352 
2025-03-06 07:57:21.555228: val_loss -0.8278 
2025-03-06 07:57:21.558695: Pseudo dice [np.float32(0.8935), np.float32(0.8772)] 
2025-03-06 07:57:21.562331: Epoch time: 6.62 s 
2025-03-06 07:57:22.105890:  
2025-03-06 07:57:22.111480: Epoch 58 
2025-03-06 07:57:22.114559: Current learning rate: 0.00458 
2025-03-06 07:57:28.709123: train_loss -0.9362 
2025-03-06 07:57:28.714716: val_loss -0.8264 
2025-03-06 07:57:28.718309: Pseudo dice [np.float32(0.893), np.float32(0.874)] 
2025-03-06 07:57:28.721354: Epoch time: 6.6 s 
2025-03-06 07:57:29.276395:  
2025-03-06 07:57:29.282000: Epoch 59 
2025-03-06 07:57:29.284565: Current learning rate: 0.00448 
2025-03-06 07:57:35.893677: train_loss -0.9366 
2025-03-06 07:57:35.898770: val_loss -0.8297 
2025-03-06 07:57:35.903339: Pseudo dice [np.float32(0.8957), np.float32(0.8767)] 
2025-03-06 07:57:35.906394: Epoch time: 6.62 s 
2025-03-06 07:57:36.462577:  
2025-03-06 07:57:36.468192: Epoch 60 
2025-03-06 07:57:36.471238: Current learning rate: 0.00438 
2025-03-06 07:57:43.077050: train_loss -0.9367 
2025-03-06 07:57:43.083569: val_loss -0.8281 
2025-03-06 07:57:43.086093: Pseudo dice [np.float32(0.8942), np.float32(0.8765)] 
2025-03-06 07:57:43.089680: Epoch time: 6.61 s 
2025-03-06 07:57:43.786501:  
2025-03-06 07:57:43.792025: Epoch 61 
2025-03-06 07:57:43.795537: Current learning rate: 0.00429 
2025-03-06 07:57:50.410812: train_loss -0.9372 
2025-03-06 07:57:50.416368: val_loss -0.8213 
2025-03-06 07:57:50.419898: Pseudo dice [np.float32(0.8916), np.float32(0.8719)] 
2025-03-06 07:57:50.422953: Epoch time: 6.62 s 
2025-03-06 07:57:50.978712:  
2025-03-06 07:57:50.984798: Epoch 62 
2025-03-06 07:57:50.987844: Current learning rate: 0.00419 
2025-03-06 07:57:57.578278: train_loss -0.937 
2025-03-06 07:57:57.583352: val_loss -0.8293 
2025-03-06 07:57:57.586923: Pseudo dice [np.float32(0.8946), np.float32(0.8786)] 
2025-03-06 07:57:57.589918: Epoch time: 6.6 s 
2025-03-06 07:57:58.146610:  
2025-03-06 07:57:58.151641: Epoch 63 
2025-03-06 07:57:58.155177: Current learning rate: 0.00409 
2025-03-06 07:58:04.757109: train_loss -0.9382 
2025-03-06 07:58:04.763234: val_loss -0.8318 
2025-03-06 07:58:04.765974: Pseudo dice [np.float32(0.8951), np.float32(0.8789)] 
2025-03-06 07:58:04.770051: Epoch time: 6.61 s 
2025-03-06 07:58:05.324033:  
2025-03-06 07:58:05.329053: Epoch 64 
2025-03-06 07:58:05.332575: Current learning rate: 0.00399 
2025-03-06 07:58:11.928256: train_loss -0.9391 
2025-03-06 07:58:11.933346: val_loss -0.8287 
2025-03-06 07:58:11.937450: Pseudo dice [np.float32(0.8939), np.float32(0.8772)] 
2025-03-06 07:58:11.940522: Epoch time: 6.6 s 
2025-03-06 07:58:12.489043:  
2025-03-06 07:58:12.494566: Epoch 65 
2025-03-06 07:58:12.498082: Current learning rate: 0.00389 
2025-03-06 07:58:19.095772: train_loss -0.938 
2025-03-06 07:58:19.101872: val_loss -0.83 
2025-03-06 07:58:19.105033: Pseudo dice [np.float32(0.8951), np.float32(0.8776)] 
2025-03-06 07:58:19.108100: Epoch time: 6.61 s 
2025-03-06 07:58:19.669552:  
2025-03-06 07:58:19.672579: Epoch 66 
2025-03-06 07:58:19.676665: Current learning rate: 0.00379 
2025-03-06 07:58:26.304088: train_loss -0.9388 
2025-03-06 07:58:26.309181: val_loss -0.8236 
2025-03-06 07:58:26.312796: Pseudo dice [np.float32(0.8911), np.float32(0.8738)] 
2025-03-06 07:58:26.316383: Epoch time: 6.63 s 
2025-03-06 07:58:26.866545:  
2025-03-06 07:58:26.872103: Epoch 67 
2025-03-06 07:58:26.875258: Current learning rate: 0.00369 
2025-03-06 07:58:33.483118: train_loss -0.9393 
2025-03-06 07:58:33.488728: val_loss -0.8292 
2025-03-06 07:58:33.491792: Pseudo dice [np.float32(0.8958), np.float32(0.878)] 
2025-03-06 07:58:33.494818: Epoch time: 6.62 s 
2025-03-06 07:58:34.056907:  
2025-03-06 07:58:34.062499: Epoch 68 
2025-03-06 07:58:34.065633: Current learning rate: 0.00359 
2025-03-06 07:58:40.667912: train_loss -0.9397 
2025-03-06 07:58:40.673936: val_loss -0.8263 
2025-03-06 07:58:40.676897: Pseudo dice [np.float32(0.8935), np.float32(0.8761)] 
2025-03-06 07:58:40.680423: Epoch time: 6.61 s 
2025-03-06 07:58:41.382950:  
2025-03-06 07:58:41.389014: Epoch 69 
2025-03-06 07:58:41.392087: Current learning rate: 0.00349 
2025-03-06 07:58:47.990836: train_loss -0.9417 
2025-03-06 07:58:47.997003: val_loss -0.8243 
2025-03-06 07:58:48.000611: Pseudo dice [np.float32(0.8934), np.float32(0.8722)] 
2025-03-06 07:58:48.003682: Epoch time: 6.61 s 
2025-03-06 07:58:48.565679:  
2025-03-06 07:58:48.570800: Epoch 70 
2025-03-06 07:58:48.574316: Current learning rate: 0.00338 
2025-03-06 07:58:55.164199: train_loss -0.9415 
2025-03-06 07:58:55.169797: val_loss -0.8257 
2025-03-06 07:58:55.173347: Pseudo dice [np.float32(0.8945), np.float32(0.8756)] 
2025-03-06 07:58:55.176447: Epoch time: 6.6 s 
2025-03-06 07:58:55.735721:  
2025-03-06 07:58:55.741307: Epoch 71 
2025-03-06 07:58:55.743873: Current learning rate: 0.00328 
2025-03-06 07:59:02.347411: train_loss -0.94 
2025-03-06 07:59:02.353427: val_loss -0.8265 
2025-03-06 07:59:02.356439: Pseudo dice [np.float32(0.8936), np.float32(0.8756)] 
2025-03-06 07:59:02.359951: Epoch time: 6.61 s 
2025-03-06 07:59:02.920156:  
2025-03-06 07:59:02.925175: Epoch 72 
2025-03-06 07:59:02.928689: Current learning rate: 0.00318 
2025-03-06 07:59:09.524731: train_loss -0.9396 
2025-03-06 07:59:09.530263: val_loss -0.8222 
2025-03-06 07:59:09.533884: Pseudo dice [np.float32(0.8922), np.float32(0.8734)] 
2025-03-06 07:59:09.536899: Epoch time: 6.61 s 
2025-03-06 07:59:10.106898:  
2025-03-06 07:59:10.110996: Epoch 73 
2025-03-06 07:59:10.114933: Current learning rate: 0.00308 
2025-03-06 07:59:16.716402: train_loss -0.94 
2025-03-06 07:59:16.722565: val_loss -0.8258 
2025-03-06 07:59:16.725676: Pseudo dice [np.float32(0.8944), np.float32(0.8753)] 
2025-03-06 07:59:16.728834: Epoch time: 6.61 s 
2025-03-06 07:59:17.291077:  
2025-03-06 07:59:17.296131: Epoch 74 
2025-03-06 07:59:17.298795: Current learning rate: 0.00297 
2025-03-06 07:59:23.889090: train_loss -0.9433 
2025-03-06 07:59:23.894748: val_loss -0.8298 
2025-03-06 07:59:23.898439: Pseudo dice [np.float32(0.8954), np.float32(0.8787)] 
2025-03-06 07:59:23.901955: Epoch time: 6.6 s 
2025-03-06 07:59:24.465080:  
2025-03-06 07:59:24.470093: Epoch 75 
2025-03-06 07:59:24.473608: Current learning rate: 0.00287 
2025-03-06 07:59:31.071820: train_loss -0.9421 
2025-03-06 07:59:31.077383: val_loss -0.825 
2025-03-06 07:59:31.080904: Pseudo dice [np.float32(0.8939), np.float32(0.876)] 
2025-03-06 07:59:31.084481: Epoch time: 6.61 s 
2025-03-06 07:59:31.791863:  
2025-03-06 07:59:31.797402: Epoch 76 
2025-03-06 07:59:31.800437: Current learning rate: 0.00277 
2025-03-06 07:59:38.412553: train_loss -0.9411 
2025-03-06 07:59:38.418154: val_loss -0.8224 
2025-03-06 07:59:38.421681: Pseudo dice [np.float32(0.8931), np.float32(0.8749)] 
2025-03-06 07:59:38.425222: Epoch time: 6.62 s 
2025-03-06 07:59:38.983904:  
2025-03-06 07:59:38.988925: Epoch 77 
2025-03-06 07:59:38.992439: Current learning rate: 0.00266 
2025-03-06 07:59:45.591882: train_loss -0.9422 
2025-03-06 07:59:45.596835: val_loss -0.8237 
2025-03-06 07:59:45.600953: Pseudo dice [np.float32(0.8933), np.float32(0.8775)] 
2025-03-06 07:59:45.603196: Epoch time: 6.61 s 
2025-03-06 07:59:46.174602:  
2025-03-06 07:59:46.179523: Epoch 78 
2025-03-06 07:59:46.182970: Current learning rate: 0.00256 
2025-03-06 07:59:52.780871: train_loss -0.9416 
2025-03-06 07:59:52.786513: val_loss -0.8285 
2025-03-06 07:59:52.790026: Pseudo dice [np.float32(0.8945), np.float32(0.8775)] 
2025-03-06 07:59:52.794046: Epoch time: 6.61 s 
2025-03-06 07:59:53.362226:  
2025-03-06 07:59:53.367245: Epoch 79 
2025-03-06 07:59:53.370756: Current learning rate: 0.00245 
2025-03-06 07:59:59.967978: train_loss -0.9435 
2025-03-06 07:59:59.974553: val_loss -0.8259 
2025-03-06 07:59:59.977604: Pseudo dice [np.float32(0.8953), np.float32(0.8778)] 
2025-03-06 07:59:59.981145: Epoch time: 6.61 s 
2025-03-06 08:00:00.558164:  
2025-03-06 08:00:00.563732: Epoch 80 
2025-03-06 08:00:00.567324: Current learning rate: 0.00235 
2025-03-06 08:00:07.157971: train_loss -0.9439 
2025-03-06 08:00:07.164074: val_loss -0.8238 
2025-03-06 08:00:07.167193: Pseudo dice [np.float32(0.8942), np.float32(0.8765)] 
2025-03-06 08:00:07.170248: Epoch time: 6.6 s 
2025-03-06 08:00:07.749002:  
2025-03-06 08:00:07.754576: Epoch 81 
2025-03-06 08:00:07.758126: Current learning rate: 0.00224 
2025-03-06 08:00:14.381485: train_loss -0.9422 
2025-03-06 08:00:14.386813: val_loss -0.8273 
2025-03-06 08:00:14.390357: Pseudo dice [np.float32(0.895), np.float32(0.8775)] 
2025-03-06 08:00:14.393280: Epoch time: 6.63 s 
2025-03-06 08:00:14.964581:  
2025-03-06 08:00:14.967093: Epoch 82 
2025-03-06 08:00:14.972018: Current learning rate: 0.00214 
2025-03-06 08:00:21.594038: train_loss -0.9424 
2025-03-06 08:00:21.600067: val_loss -0.8261 
2025-03-06 08:00:21.604084: Pseudo dice [np.float32(0.8942), np.float32(0.8774)] 
2025-03-06 08:00:21.606639: Epoch time: 6.63 s 
2025-03-06 08:00:22.143334:  
2025-03-06 08:00:22.149929: Epoch 83 
2025-03-06 08:00:22.156530: Current learning rate: 0.00203 
2025-03-06 08:00:28.778219: train_loss -0.944 
2025-03-06 08:00:28.783889: val_loss -0.8227 
2025-03-06 08:00:28.787461: Pseudo dice [np.float32(0.893), np.float32(0.8753)] 
2025-03-06 08:00:28.790618: Epoch time: 6.64 s 
2025-03-06 08:00:29.480658:  
2025-03-06 08:00:29.487261: Epoch 84 
2025-03-06 08:00:29.489810: Current learning rate: 0.00192 
2025-03-06 08:00:36.091670: train_loss -0.9446 
2025-03-06 08:00:36.098321: val_loss -0.8213 
2025-03-06 08:00:36.100849: Pseudo dice [np.float32(0.8932), np.float32(0.8748)] 
2025-03-06 08:00:36.104374: Epoch time: 6.61 s 
2025-03-06 08:00:36.645278:  
2025-03-06 08:00:36.651328: Epoch 85 
2025-03-06 08:00:36.654474: Current learning rate: 0.00181 
2025-03-06 08:00:43.262449: train_loss -0.9432 
2025-03-06 08:00:43.268034: val_loss -0.8193 
2025-03-06 08:00:43.271066: Pseudo dice [np.float32(0.8916), np.float32(0.8732)] 
2025-03-06 08:00:43.274590: Epoch time: 6.62 s 
2025-03-06 08:00:43.807799:  
2025-03-06 08:00:43.813382: Epoch 86 
2025-03-06 08:00:43.816392: Current learning rate: 0.0017 
2025-03-06 08:00:50.404885: train_loss -0.9443 
2025-03-06 08:00:50.411405: val_loss -0.8218 
2025-03-06 08:00:50.414915: Pseudo dice [np.float32(0.8935), np.float32(0.8747)] 
2025-03-06 08:00:50.418428: Epoch time: 6.6 s 
2025-03-06 08:00:50.946781:  
2025-03-06 08:00:50.953298: Epoch 87 
2025-03-06 08:00:50.955809: Current learning rate: 0.00159 
2025-03-06 08:00:57.549981: train_loss -0.946 
2025-03-06 08:00:57.556078: val_loss -0.8211 
2025-03-06 08:00:57.559746: Pseudo dice [np.float32(0.8933), np.float32(0.8751)] 
2025-03-06 08:00:57.562853: Epoch time: 6.6 s 
2025-03-06 08:00:58.091996:  
2025-03-06 08:00:58.096511: Epoch 88 
2025-03-06 08:00:58.099520: Current learning rate: 0.00148 
2025-03-06 08:01:04.710782: train_loss -0.9438 
2025-03-06 08:01:04.716993: val_loss -0.8259 
2025-03-06 08:01:04.720069: Pseudo dice [np.float32(0.8958), np.float32(0.8783)] 
2025-03-06 08:01:04.723103: Epoch time: 6.62 s 
2025-03-06 08:01:05.254985:  
2025-03-06 08:01:05.260180: Epoch 89 
2025-03-06 08:01:05.262700: Current learning rate: 0.00137 
2025-03-06 08:01:11.865148: train_loss -0.9443 
2025-03-06 08:01:11.870247: val_loss -0.8251 
2025-03-06 08:01:11.873814: Pseudo dice [np.float32(0.8952), np.float32(0.8776)] 
2025-03-06 08:01:11.877340: Epoch time: 6.61 s 
2025-03-06 08:01:12.404113:  
2025-03-06 08:01:12.409657: Epoch 90 
2025-03-06 08:01:12.413170: Current learning rate: 0.00126 
2025-03-06 08:01:19.020307: train_loss -0.9462 
2025-03-06 08:01:19.025371: val_loss -0.8241 
2025-03-06 08:01:19.029449: Pseudo dice [np.float32(0.8941), np.float32(0.8765)] 
2025-03-06 08:01:19.032522: Epoch time: 6.62 s 
2025-03-06 08:01:19.567653:  
2025-03-06 08:01:19.573220: Epoch 91 
2025-03-06 08:01:19.575771: Current learning rate: 0.00115 
2025-03-06 08:01:26.175921: train_loss -0.945 
2025-03-06 08:01:26.181367: val_loss -0.8283 
2025-03-06 08:01:26.184885: Pseudo dice [np.float32(0.8965), np.float32(0.8789)] 
2025-03-06 08:01:26.188411: Epoch time: 6.61 s 
2025-03-06 08:01:26.729125:  
2025-03-06 08:01:26.734139: Epoch 92 
2025-03-06 08:01:26.737652: Current learning rate: 0.00103 
2025-03-06 08:01:33.516949: train_loss -0.9453 
2025-03-06 08:01:33.521962: val_loss -0.8261 
2025-03-06 08:01:33.525475: Pseudo dice [np.float32(0.8954), np.float32(0.8791)] 
2025-03-06 08:01:33.529589: Epoch time: 6.79 s 
2025-03-06 08:01:34.064703:  
2025-03-06 08:01:34.070230: Epoch 93 
2025-03-06 08:01:34.073647: Current learning rate: 0.00091 
2025-03-06 08:01:40.677592: train_loss -0.9457 
2025-03-06 08:01:40.683756: val_loss -0.822 
2025-03-06 08:01:40.686870: Pseudo dice [np.float32(0.8926), np.float32(0.8755)] 
2025-03-06 08:01:40.689975: Epoch time: 6.61 s 
2025-03-06 08:01:41.220678:  
2025-03-06 08:01:41.226696: Epoch 94 
2025-03-06 08:01:41.229708: Current learning rate: 0.00079 
2025-03-06 08:01:47.835952: train_loss -0.9457 
2025-03-06 08:01:47.841044: val_loss -0.8219 
2025-03-06 08:01:47.846139: Pseudo dice [np.float32(0.8933), np.float32(0.8762)] 
2025-03-06 08:01:47.849767: Epoch time: 6.62 s 
2025-03-06 08:01:48.381238:  
2025-03-06 08:01:48.386832: Epoch 95 
2025-03-06 08:01:48.390430: Current learning rate: 0.00067 
2025-03-06 08:01:54.994367: train_loss -0.945 
2025-03-06 08:01:55.000480: val_loss -0.8186 
2025-03-06 08:01:55.002987: Pseudo dice [np.float32(0.8917), np.float32(0.8733)] 
2025-03-06 08:01:55.007047: Epoch time: 6.61 s 
2025-03-06 08:01:55.538678:  
2025-03-06 08:01:55.544797: Epoch 96 
2025-03-06 08:01:55.547866: Current learning rate: 0.00055 
2025-03-06 08:02:02.153971: train_loss -0.947 
2025-03-06 08:02:02.160082: val_loss -0.8232 
2025-03-06 08:02:02.163140: Pseudo dice [np.float32(0.8936), np.float32(0.8763)] 
2025-03-06 08:02:02.166670: Epoch time: 6.62 s 
2025-03-06 08:02:02.706405:  
2025-03-06 08:02:02.711444: Epoch 97 
2025-03-06 08:02:02.715200: Current learning rate: 0.00043 
2025-03-06 08:02:09.316303: train_loss -0.9465 
2025-03-06 08:02:09.322370: val_loss -0.8248 
2025-03-06 08:02:09.325583: Pseudo dice [np.float32(0.8944), np.float32(0.878)] 
2025-03-06 08:02:09.328593: Epoch time: 6.61 s 
2025-03-06 08:02:09.870517:  
2025-03-06 08:02:09.876092: Epoch 98 
2025-03-06 08:02:09.879647: Current learning rate: 0.0003 
2025-03-06 08:02:16.497785: train_loss -0.9456 
2025-03-06 08:02:16.503045: val_loss -0.8196 
2025-03-06 08:02:16.506575: Pseudo dice [np.float32(0.8932), np.float32(0.8745)] 
2025-03-06 08:02:16.510254: Epoch time: 6.63 s 
2025-03-06 08:02:17.056433:  
2025-03-06 08:02:17.061505: Epoch 99 
2025-03-06 08:02:17.065022: Current learning rate: 0.00016 
2025-03-06 08:02:23.676112: train_loss -0.9475 
2025-03-06 08:02:23.683802: val_loss -0.8256 
2025-03-06 08:02:23.687308: Pseudo dice [np.float32(0.8951), np.float32(0.8782)] 
2025-03-06 08:02:23.690940: Epoch time: 6.62 s 
2025-03-06 08:02:24.286337: Training done. 
2025-03-06 08:02:24.322336: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-06 08:02:24.330337: The split file contains 5 splits. 
2025-03-06 08:02:24.336337: Desired fold for training: 0 
2025-03-06 08:02:24.340340: This split has 208 training and 52 validation cases. 
2025-03-06 08:02:24.345339: predicting hippocampus_017 
2025-03-06 08:02:24.351342: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-06 08:02:24.439848: predicting hippocampus_019 
2025-03-06 08:02:24.446850: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-06 08:02:24.480362: predicting hippocampus_033 
2025-03-06 08:02:24.486371: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-06 08:02:24.507370: predicting hippocampus_035 
2025-03-06 08:02:24.513366: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-06 08:02:24.535370: predicting hippocampus_037 
2025-03-06 08:02:24.541368: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-06 08:02:24.563369: predicting hippocampus_049 
2025-03-06 08:02:24.570371: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-06 08:02:24.593466: predicting hippocampus_052 
2025-03-06 08:02:24.599468: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-06 08:02:24.621468: predicting hippocampus_065 
2025-03-06 08:02:24.627467: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-06 08:02:24.649467: predicting hippocampus_083 
2025-03-06 08:02:24.656470: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-06 08:02:24.678472: predicting hippocampus_088 
2025-03-06 08:02:24.684982: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-06 08:02:28.237859: predicting hippocampus_090 
2025-03-06 08:02:28.244863: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-06 08:02:28.280866: predicting hippocampus_092 
2025-03-06 08:02:28.288374: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-06 08:02:28.327376: predicting hippocampus_095 
2025-03-06 08:02:28.334376: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-06 08:02:28.379380: predicting hippocampus_107 
2025-03-06 08:02:28.386886: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-06 08:02:28.430886: predicting hippocampus_108 
2025-03-06 08:02:28.438885: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-06 08:02:28.482389: predicting hippocampus_123 
2025-03-06 08:02:28.489392: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-06 08:02:28.515392: predicting hippocampus_125 
2025-03-06 08:02:28.522392: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-06 08:02:28.568394: predicting hippocampus_157 
2025-03-06 08:02:28.574395: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-06 08:02:28.600900: predicting hippocampus_164 
2025-03-06 08:02:28.605902: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-06 08:02:28.689407: predicting hippocampus_169 
2025-03-06 08:02:28.696409: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-06 08:02:28.722408: predicting hippocampus_175 
2025-03-06 08:02:28.729408: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-06 08:02:28.756407: predicting hippocampus_185 
2025-03-06 08:02:28.762408: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-06 08:02:28.788918: predicting hippocampus_190 
2025-03-06 08:02:28.793917: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-06 08:02:28.821917: predicting hippocampus_194 
2025-03-06 08:02:28.826920: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-06 08:02:28.851917: predicting hippocampus_204 
2025-03-06 08:02:28.856917: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-06 08:02:28.883425: predicting hippocampus_205 
2025-03-06 08:02:28.888425: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-06 08:02:28.912425: predicting hippocampus_210 
2025-03-06 08:02:28.917426: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-06 08:02:28.947425: predicting hippocampus_217 
2025-03-06 08:02:28.952425: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-06 08:02:28.978427: predicting hippocampus_219 
2025-03-06 08:02:28.983938: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-06 08:02:29.011938: predicting hippocampus_229 
2025-03-06 08:02:29.016937: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-06 08:02:29.042938: predicting hippocampus_244 
2025-03-06 08:02:29.049938: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-06 08:02:29.074940: predicting hippocampus_261 
2025-03-06 08:02:29.079940: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-06 08:02:29.125445: predicting hippocampus_264 
2025-03-06 08:02:29.130445: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-06 08:02:29.157445: predicting hippocampus_277 
2025-03-06 08:02:29.162445: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-06 08:02:29.203953: predicting hippocampus_280 
2025-03-06 08:02:29.209954: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-06 08:02:29.234953: predicting hippocampus_286 
2025-03-06 08:02:29.240953: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-06 08:02:29.283460: predicting hippocampus_288 
2025-03-06 08:02:29.289460: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-06 08:02:29.332460: predicting hippocampus_289 
2025-03-06 08:02:29.339463: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-06 08:02:29.367462: predicting hippocampus_296 
2025-03-06 08:02:29.373466: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-06 08:02:29.398969: predicting hippocampus_305 
2025-03-06 08:02:29.403969: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-06 08:02:29.429969: predicting hippocampus_308 
2025-03-06 08:02:29.434971: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-06 08:02:29.460969: predicting hippocampus_317 
2025-03-06 08:02:29.465971: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-06 08:02:29.491477: predicting hippocampus_327 
2025-03-06 08:02:29.496477: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-06 08:02:29.523479: predicting hippocampus_330 
2025-03-06 08:02:29.529477: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-06 08:02:29.556478: predicting hippocampus_332 
2025-03-06 08:02:29.561477: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-06 08:02:29.586987: predicting hippocampus_338 
2025-03-06 08:02:29.590987: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-06 08:02:29.635985: predicting hippocampus_349 
2025-03-06 08:02:29.642985: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-06 08:02:29.671988: predicting hippocampus_350 
2025-03-06 08:02:29.676988: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-06 08:02:29.703496: predicting hippocampus_356 
2025-03-06 08:02:29.708496: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-06 08:02:29.734496: predicting hippocampus_358 
2025-03-06 08:02:29.740497: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-06 08:02:29.766496: predicting hippocampus_374 
2025-03-06 08:02:29.771500: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-06 08:02:29.798003: predicting hippocampus_394 
2025-03-06 08:02:29.804003: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-06 08:02:33.410299: Validation complete 
2025-03-06 08:02:33.415297: Mean Validation Dice:  0.5217326459356473 
