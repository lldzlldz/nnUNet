
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-12-08 19:47:45.597184: do_dummy_2d_data_aug: False 
2024-12-08 19:47:45.597184: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2024-12-08 19:47:45.608028: The split file contains 5 splits. 
2024-12-08 19:47:45.610454: Desired fold for training: 0 
2024-12-08 19:47:45.612916: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2024-12-08 19:47:51.252554: unpacking dataset... 
2024-12-08 19:47:51.576253: unpacking done... 
2024-12-08 19:47:52.600004:  
2024-12-08 19:47:52.604552: Epoch 0 
2024-12-08 19:47:52.607646: Current learning rate: 0.01 
2024-12-08 19:47:59.575948: train_loss -0.5588 
2024-12-08 19:47:59.581086: val_loss -0.8208 
2024-12-08 19:47:59.584150: Pseudo dice [np.float32(0.8734), np.float32(0.8631)] 
2024-12-08 19:47:59.586688: Epoch time: 6.98 s 
2024-12-08 19:47:59.589216: Yayy! New best EMA pseudo Dice: 0.8682000041007996 
2024-12-08 19:48:00.071390:  
2024-12-08 19:48:00.076469: Epoch 1 
2024-12-08 19:48:00.079527: Current learning rate: 0.00991 
2024-12-08 19:48:06.195501: train_loss -0.8515 
2024-12-08 19:48:06.200591: val_loss -0.8343 
2024-12-08 19:48:06.203123: Pseudo dice [np.float32(0.8875), np.float32(0.8713)] 
2024-12-08 19:48:06.206676: Epoch time: 6.12 s 
2024-12-08 19:48:06.209267: Yayy! New best EMA pseudo Dice: 0.8694000244140625 
2024-12-08 19:48:06.741597:  
2024-12-08 19:48:06.746698: Epoch 2 
2024-12-08 19:48:06.749762: Current learning rate: 0.00982 
2024-12-08 19:48:12.861269: train_loss -0.8739 
2024-12-08 19:48:12.867840: val_loss -0.8404 
2024-12-08 19:48:12.870977: Pseudo dice [np.float32(0.8933), np.float32(0.8755)] 
2024-12-08 19:48:12.874032: Epoch time: 6.12 s 
2024-12-08 19:48:12.876049: Yayy! New best EMA pseudo Dice: 0.8708999752998352 
2024-12-08 19:48:13.444391:  
2024-12-08 19:48:13.450985: Epoch 3 
2024-12-08 19:48:13.454062: Current learning rate: 0.00973 
2024-12-08 19:48:19.552782: train_loss -0.8902 
2024-12-08 19:48:19.557885: val_loss -0.841 
2024-12-08 19:48:19.560948: Pseudo dice [np.float32(0.8938), np.float32(0.8774)] 
2024-12-08 19:48:19.564075: Epoch time: 6.11 s 
2024-12-08 19:48:19.567123: Yayy! New best EMA pseudo Dice: 0.8723000288009644 
2024-12-08 19:48:20.115852:  
2024-12-08 19:48:20.121423: Epoch 4 
2024-12-08 19:48:20.123968: Current learning rate: 0.00964 
2024-12-08 19:48:26.227414: train_loss -0.9032 
2024-12-08 19:48:26.232986: val_loss -0.8283 
2024-12-08 19:48:26.236040: Pseudo dice [np.float32(0.8877), np.float32(0.8696)] 
2024-12-08 19:48:26.239092: Epoch time: 6.11 s 
2024-12-08 19:48:26.241151: Yayy! New best EMA pseudo Dice: 0.8730000257492065 
2024-12-08 19:48:26.818521:  
2024-12-08 19:48:26.824136: Epoch 5 
2024-12-08 19:48:26.827184: Current learning rate: 0.00955 
2024-12-08 19:48:32.933374: train_loss -0.9114 
2024-12-08 19:48:32.939964: val_loss -0.8319 
2024-12-08 19:48:32.942106: Pseudo dice [np.float32(0.8912), np.float32(0.8731)] 
2024-12-08 19:48:32.946145: Epoch time: 6.11 s 
2024-12-08 19:48:32.948669: Yayy! New best EMA pseudo Dice: 0.8738999962806702 
2024-12-08 19:48:33.608747:  
2024-12-08 19:48:33.613828: Epoch 6 
2024-12-08 19:48:33.616892: Current learning rate: 0.00946 
2024-12-08 19:48:39.721565: train_loss -0.9185 
2024-12-08 19:48:39.727145: val_loss -0.8269 
2024-12-08 19:48:39.729216: Pseudo dice [np.float32(0.8886), np.float32(0.8716)] 
2024-12-08 19:48:39.732762: Epoch time: 6.11 s 
2024-12-08 19:48:39.736296: Yayy! New best EMA pseudo Dice: 0.8744999766349792 
2024-12-08 19:48:40.286109:  
2024-12-08 19:48:40.291682: Epoch 7 
2024-12-08 19:48:40.293739: Current learning rate: 0.00937 
2024-12-08 19:48:46.397832: train_loss -0.9233 
2024-12-08 19:48:46.403943: val_loss -0.8308 
2024-12-08 19:48:46.407496: Pseudo dice [np.float32(0.8901), np.float32(0.877)] 
2024-12-08 19:48:46.410029: Epoch time: 6.11 s 
2024-12-08 19:48:46.412555: Yayy! New best EMA pseudo Dice: 0.8754000067710876 
2024-12-08 19:48:46.968532:  
2024-12-08 19:48:46.973615: Epoch 8 
2024-12-08 19:48:46.976674: Current learning rate: 0.00928 
2024-12-08 19:48:53.090866: train_loss -0.9294 
2024-12-08 19:48:53.097064: val_loss -0.8332 
2024-12-08 19:48:53.100636: Pseudo dice [np.float32(0.8931), np.float32(0.8776)] 
2024-12-08 19:48:53.102669: Epoch time: 6.12 s 
2024-12-08 19:48:53.105199: Yayy! New best EMA pseudo Dice: 0.8763999938964844 
2024-12-08 19:48:53.670927:  
2024-12-08 19:48:53.676028: Epoch 9 
2024-12-08 19:48:53.678557: Current learning rate: 0.00919 
2024-12-08 19:48:59.794489: train_loss -0.9326 
2024-12-08 19:48:59.800058: val_loss -0.8235 
2024-12-08 19:48:59.802630: Pseudo dice [np.float32(0.8888), np.float32(0.8713)] 
2024-12-08 19:48:59.806183: Epoch time: 6.12 s 
2024-12-08 19:48:59.808722: Yayy! New best EMA pseudo Dice: 0.876800000667572 
2024-12-08 19:49:00.351566:  
2024-12-08 19:49:00.356613: Epoch 10 
2024-12-08 19:49:00.359150: Current learning rate: 0.0091 
2024-12-08 19:49:06.485656: train_loss -0.9373 
2024-12-08 19:49:06.490749: val_loss -0.8247 
2024-12-08 19:49:06.493797: Pseudo dice [np.float32(0.8895), np.float32(0.8741)] 
2024-12-08 19:49:06.496869: Epoch time: 6.14 s 
2024-12-08 19:49:06.498913: Yayy! New best EMA pseudo Dice: 0.8773000240325928 
2024-12-08 19:49:07.044824:  
2024-12-08 19:49:07.049871: Epoch 11 
2024-12-08 19:49:07.052404: Current learning rate: 0.009 
2024-12-08 19:49:13.152759: train_loss -0.9405 
2024-12-08 19:49:13.157826: val_loss -0.8271 
2024-12-08 19:49:13.160357: Pseudo dice [np.float32(0.8917), np.float32(0.8752)] 
2024-12-08 19:49:13.164973: Epoch time: 6.11 s 
2024-12-08 19:49:13.168021: Yayy! New best EMA pseudo Dice: 0.8779000043869019 
2024-12-08 19:49:13.710984:  
2024-12-08 19:49:13.717048: Epoch 12 
2024-12-08 19:49:13.719589: Current learning rate: 0.00891 
2024-12-08 19:49:19.822212: train_loss -0.9433 
2024-12-08 19:49:19.827292: val_loss -0.8249 
2024-12-08 19:49:19.830839: Pseudo dice [np.float32(0.8929), np.float32(0.8752)] 
2024-12-08 19:49:19.833950: Epoch time: 6.11 s 
2024-12-08 19:49:19.836997: Yayy! New best EMA pseudo Dice: 0.8784999847412109 
2024-12-08 19:49:20.524324:  
2024-12-08 19:49:20.529390: Epoch 13 
2024-12-08 19:49:20.532528: Current learning rate: 0.00882 
2024-12-08 19:49:26.636710: train_loss -0.9458 
2024-12-08 19:49:26.642776: val_loss -0.8215 
2024-12-08 19:49:26.646838: Pseudo dice [np.float32(0.8908), np.float32(0.8736)] 
2024-12-08 19:49:26.649411: Epoch time: 6.11 s 
2024-12-08 19:49:26.652457: Yayy! New best EMA pseudo Dice: 0.8788999915122986 
2024-12-08 19:49:27.202685:  
2024-12-08 19:49:27.208273: Epoch 14 
2024-12-08 19:49:27.210815: Current learning rate: 0.00873 
2024-12-08 19:49:33.316902: train_loss -0.949 
2024-12-08 19:49:33.321971: val_loss -0.8179 
2024-12-08 19:49:33.324526: Pseudo dice [np.float32(0.8871), np.float32(0.8721)] 
2024-12-08 19:49:33.327565: Epoch time: 6.11 s 
2024-12-08 19:49:33.329594: Yayy! New best EMA pseudo Dice: 0.8790000081062317 
2024-12-08 19:49:33.891834:  
2024-12-08 19:49:33.897527: Epoch 15 
2024-12-08 19:49:33.900059: Current learning rate: 0.00864 
2024-12-08 19:49:40.013030: train_loss -0.9507 
2024-12-08 19:49:40.018623: val_loss -0.8197 
2024-12-08 19:49:40.022195: Pseudo dice [np.float32(0.8901), np.float32(0.874)] 
2024-12-08 19:49:40.025253: Epoch time: 6.12 s 
2024-12-08 19:49:40.027297: Yayy! New best EMA pseudo Dice: 0.8792999982833862 
2024-12-08 19:49:40.593939:  
2024-12-08 19:49:40.599576: Epoch 16 
2024-12-08 19:49:40.601601: Current learning rate: 0.00855 
2024-12-08 19:49:46.715019: train_loss -0.9528 
2024-12-08 19:49:46.721594: val_loss -0.8152 
2024-12-08 19:49:46.724130: Pseudo dice [np.float32(0.8883), np.float32(0.8731)] 
2024-12-08 19:49:46.727721: Epoch time: 6.12 s 
2024-12-08 19:49:46.730782: Yayy! New best EMA pseudo Dice: 0.8794000148773193 
2024-12-08 19:49:47.302628:  
2024-12-08 19:49:47.308255: Epoch 17 
2024-12-08 19:49:47.310796: Current learning rate: 0.00846 
2024-12-08 19:49:53.419858: train_loss -0.9548 
2024-12-08 19:49:53.427487: val_loss -0.8214 
2024-12-08 19:49:53.430541: Pseudo dice [np.float32(0.8925), np.float32(0.8764)] 
2024-12-08 19:49:53.433073: Epoch time: 6.12 s 
2024-12-08 19:49:53.435144: Yayy! New best EMA pseudo Dice: 0.8798999786376953 
2024-12-08 19:49:54.003527:  
2024-12-08 19:49:54.008605: Epoch 18 
2024-12-08 19:49:54.012152: Current learning rate: 0.00836 
2024-12-08 19:50:00.121815: train_loss -0.9567 
2024-12-08 19:50:00.127441: val_loss -0.8185 
2024-12-08 19:50:00.130017: Pseudo dice [np.float32(0.891), np.float32(0.8751)] 
2024-12-08 19:50:00.134574: Epoch time: 6.12 s 
2024-12-08 19:50:00.137644: Yayy! New best EMA pseudo Dice: 0.8802000284194946 
2024-12-08 19:50:00.702553:  
2024-12-08 19:50:00.708181: Epoch 19 
2024-12-08 19:50:00.710720: Current learning rate: 0.00827 
2024-12-08 19:50:06.826286: train_loss -0.9579 
2024-12-08 19:50:06.830852: val_loss -0.8181 
2024-12-08 19:50:06.835434: Pseudo dice [np.float32(0.892), np.float32(0.8749)] 
2024-12-08 19:50:06.838482: Epoch time: 6.12 s 
2024-12-08 19:50:06.841015: Yayy! New best EMA pseudo Dice: 0.8805999755859375 
2024-12-08 19:50:07.402281:  
2024-12-08 19:50:07.407866: Epoch 20 
2024-12-08 19:50:07.410407: Current learning rate: 0.00818 
2024-12-08 19:50:13.527098: train_loss -0.9593 
2024-12-08 19:50:13.531740: val_loss -0.817 
2024-12-08 19:50:13.535792: Pseudo dice [np.float32(0.8904), np.float32(0.8758)] 
2024-12-08 19:50:13.538860: Epoch time: 6.13 s 
2024-12-08 19:50:13.541408: Yayy! New best EMA pseudo Dice: 0.8808000087738037 
2024-12-08 19:50:14.248775:  
2024-12-08 19:50:14.254375: Epoch 21 
2024-12-08 19:50:14.256913: Current learning rate: 0.00809 
2024-12-08 19:50:20.357467: train_loss -0.9611 
2024-12-08 19:50:20.364094: val_loss -0.8103 
2024-12-08 19:50:20.367157: Pseudo dice [np.float32(0.8888), np.float32(0.8745)] 
2024-12-08 19:50:20.370234: Epoch time: 6.11 s 
2024-12-08 19:50:20.372787: Yayy! New best EMA pseudo Dice: 0.8809000253677368 
2024-12-08 19:50:20.924531:  
2024-12-08 19:50:20.930096: Epoch 22 
2024-12-08 19:50:20.933149: Current learning rate: 0.008 
2024-12-08 19:50:27.036745: train_loss -0.961 
2024-12-08 19:50:27.041376: val_loss -0.8109 
2024-12-08 19:50:27.044935: Pseudo dice [np.float32(0.8882), np.float32(0.8731)] 
2024-12-08 19:50:27.048056: Epoch time: 6.11 s 
2024-12-08 19:50:27.559872:  
2024-12-08 19:50:27.564948: Epoch 23 
2024-12-08 19:50:27.567546: Current learning rate: 0.0079 
2024-12-08 19:50:33.677520: train_loss -0.963 
2024-12-08 19:50:33.682106: val_loss -0.8136 
2024-12-08 19:50:33.686652: Pseudo dice [np.float32(0.8896), np.float32(0.8749)] 
2024-12-08 19:50:33.689756: Epoch time: 6.12 s 
2024-12-08 19:50:33.692822: Yayy! New best EMA pseudo Dice: 0.8809999823570251 
2024-12-08 19:50:34.228247:  
2024-12-08 19:50:34.234311: Epoch 24 
2024-12-08 19:50:34.236848: Current learning rate: 0.00781 
2024-12-08 19:50:40.339742: train_loss -0.9634 
2024-12-08 19:50:40.345383: val_loss -0.8116 
2024-12-08 19:50:40.347930: Pseudo dice [np.float32(0.8897), np.float32(0.8744)] 
2024-12-08 19:50:40.351487: Epoch time: 6.11 s 
2024-12-08 19:50:40.353538: Yayy! New best EMA pseudo Dice: 0.8810999989509583 
2024-12-08 19:50:40.899126:  
2024-12-08 19:50:40.904233: Epoch 25 
2024-12-08 19:50:40.907288: Current learning rate: 0.00772 
2024-12-08 19:50:47.019935: train_loss -0.964 
2024-12-08 19:50:47.025530: val_loss -0.8138 
2024-12-08 19:50:47.028637: Pseudo dice [np.float32(0.892), np.float32(0.8758)] 
2024-12-08 19:50:47.030672: Epoch time: 6.12 s 
2024-12-08 19:50:47.033216: Yayy! New best EMA pseudo Dice: 0.8813999891281128 
2024-12-08 19:50:47.585019:  
2024-12-08 19:50:47.590652: Epoch 26 
2024-12-08 19:50:47.593197: Current learning rate: 0.00763 
2024-12-08 19:50:53.708307: train_loss -0.9651 
2024-12-08 19:50:53.714873: val_loss -0.8114 
2024-12-08 19:50:53.717964: Pseudo dice [np.float32(0.8892), np.float32(0.8744)] 
2024-12-08 19:50:53.721027: Epoch time: 6.12 s 
2024-12-08 19:50:53.723119: Yayy! New best EMA pseudo Dice: 0.8813999891281128 
2024-12-08 19:50:54.266962:  
2024-12-08 19:50:54.272105: Epoch 27 
2024-12-08 19:50:54.275181: Current learning rate: 0.00753 
2024-12-08 19:51:00.380622: train_loss -0.9664 
2024-12-08 19:51:00.386207: val_loss -0.8078 
2024-12-08 19:51:00.389269: Pseudo dice [np.float32(0.8888), np.float32(0.8707)] 
2024-12-08 19:51:00.392875: Epoch time: 6.11 s 
2024-12-08 19:51:00.905243:  
2024-12-08 19:51:00.911338: Epoch 28 
2024-12-08 19:51:00.914443: Current learning rate: 0.00744 
2024-12-08 19:51:07.040044: train_loss -0.9659 
2024-12-08 19:51:07.047233: val_loss -0.8116 
2024-12-08 19:51:07.050289: Pseudo dice [np.float32(0.8907), np.float32(0.8739)] 
2024-12-08 19:51:07.052827: Epoch time: 6.13 s 
2024-12-08 19:51:07.702647:  
2024-12-08 19:51:07.708246: Epoch 29 
2024-12-08 19:51:07.710789: Current learning rate: 0.00735 
2024-12-08 19:51:13.829686: train_loss -0.9676 
2024-12-08 19:51:13.834774: val_loss -0.7994 
2024-12-08 19:51:13.837317: Pseudo dice [np.float32(0.8861), np.float32(0.8686)] 
2024-12-08 19:51:13.839860: Epoch time: 6.13 s 
2024-12-08 19:51:14.369124:  
2024-12-08 19:51:14.374219: Epoch 30 
2024-12-08 19:51:14.377279: Current learning rate: 0.00725 
2024-12-08 19:51:20.494045: train_loss -0.9685 
2024-12-08 19:51:20.499125: val_loss -0.8087 
2024-12-08 19:51:20.503683: Pseudo dice [np.float32(0.8898), np.float32(0.8742)] 
2024-12-08 19:51:20.506759: Epoch time: 6.13 s 
2024-12-08 19:51:21.023401:  
2024-12-08 19:51:21.028475: Epoch 31 
2024-12-08 19:51:21.031008: Current learning rate: 0.00716 
2024-12-08 19:51:27.134366: train_loss -0.9687 
2024-12-08 19:51:27.139489: val_loss -0.7995 
2024-12-08 19:51:27.143078: Pseudo dice [np.float32(0.8865), np.float32(0.8689)] 
2024-12-08 19:51:27.145638: Epoch time: 6.11 s 
2024-12-08 19:51:27.664401:  
2024-12-08 19:51:27.670052: Epoch 32 
2024-12-08 19:51:27.672619: Current learning rate: 0.00707 
2024-12-08 19:51:33.777252: train_loss -0.9704 
2024-12-08 19:51:33.782363: val_loss -0.8069 
2024-12-08 19:51:33.786410: Pseudo dice [np.float32(0.89), np.float32(0.873)] 
2024-12-08 19:51:33.789465: Epoch time: 6.11 s 
2024-12-08 19:51:34.300807:  
2024-12-08 19:51:34.306372: Epoch 33 
2024-12-08 19:51:34.308922: Current learning rate: 0.00697 
2024-12-08 19:51:40.423078: train_loss -0.97 
2024-12-08 19:51:40.428245: val_loss -0.8007 
2024-12-08 19:51:40.431307: Pseudo dice [np.float32(0.8869), np.float32(0.8698)] 
2024-12-08 19:51:40.433859: Epoch time: 6.12 s 
2024-12-08 19:51:40.956165:  
2024-12-08 19:51:40.961223: Epoch 34 
2024-12-08 19:51:40.964298: Current learning rate: 0.00688 
2024-12-08 19:51:48.483186: train_loss -0.9703 
2024-12-08 19:51:48.488271: val_loss -0.8001 
2024-12-08 19:51:48.493347: Pseudo dice [np.float32(0.8872), np.float32(0.8684)] 
2024-12-08 19:51:48.495880: Epoch time: 7.53 s 
2024-12-08 19:51:49.152316:  
2024-12-08 19:51:49.157499: Epoch 35 
2024-12-08 19:51:49.162653: Current learning rate: 0.00679 
2024-12-08 19:51:59.210858: train_loss -0.9706 
2024-12-08 19:51:59.216632: val_loss -0.8007 
2024-12-08 19:51:59.219667: Pseudo dice [np.float32(0.8875), np.float32(0.8706)] 
2024-12-08 19:51:59.223193: Epoch time: 10.06 s 
2024-12-08 19:51:59.765162:  
2024-12-08 19:51:59.770180: Epoch 36 
2024-12-08 19:51:59.773276: Current learning rate: 0.00669 
2024-12-08 19:52:06.177388: train_loss -0.9722 
2024-12-08 19:52:06.182935: val_loss -0.8053 
2024-12-08 19:52:06.186448: Pseudo dice [np.float32(0.8901), np.float32(0.8728)] 
2024-12-08 19:52:06.189465: Epoch time: 6.41 s 
2024-12-08 19:52:06.889673:  
2024-12-08 19:52:06.894767: Epoch 37 
2024-12-08 19:52:06.898360: Current learning rate: 0.0066 
2024-12-08 19:52:13.037526: train_loss -0.9723 
2024-12-08 19:52:13.042648: val_loss -0.8078 
2024-12-08 19:52:13.045183: Pseudo dice [np.float32(0.8916), np.float32(0.8752)] 
2024-12-08 19:52:13.047710: Epoch time: 6.15 s 
2024-12-08 19:52:13.587945:  
2024-12-08 19:52:13.593591: Epoch 38 
2024-12-08 19:52:13.596671: Current learning rate: 0.0065 
2024-12-08 19:52:19.696750: train_loss -0.9734 
2024-12-08 19:52:19.701312: val_loss -0.8005 
2024-12-08 19:52:19.703848: Pseudo dice [np.float32(0.8876), np.float32(0.8706)] 
2024-12-08 19:52:19.708456: Epoch time: 6.11 s 
2024-12-08 19:52:20.246838:  
2024-12-08 19:52:20.252924: Epoch 39 
2024-12-08 19:52:20.256012: Current learning rate: 0.00641 
2024-12-08 19:52:26.361732: train_loss -0.9745 
2024-12-08 19:52:26.367317: val_loss -0.8027 
2024-12-08 19:52:26.370911: Pseudo dice [np.float32(0.8883), np.float32(0.8716)] 
2024-12-08 19:52:26.373975: Epoch time: 6.11 s 
2024-12-08 19:52:26.916348:  
2024-12-08 19:52:26.922489: Epoch 40 
2024-12-08 19:52:26.925543: Current learning rate: 0.00631 
2024-12-08 19:52:33.025123: train_loss -0.975 
2024-12-08 19:52:33.030188: val_loss -0.7985 
2024-12-08 19:52:33.035254: Pseudo dice [np.float32(0.8881), np.float32(0.8702)] 
2024-12-08 19:52:33.037789: Epoch time: 6.11 s 
2024-12-08 19:52:33.583067:  
2024-12-08 19:52:33.589176: Epoch 41 
2024-12-08 19:52:33.594229: Current learning rate: 0.00622 
2024-12-08 19:52:39.711339: train_loss -0.9756 
2024-12-08 19:52:39.716956: val_loss -0.7989 
2024-12-08 19:52:39.718996: Pseudo dice [np.float32(0.8878), np.float32(0.8698)] 
2024-12-08 19:52:39.723562: Epoch time: 6.13 s 
2024-12-08 19:52:40.237092:  
2024-12-08 19:52:40.243193: Epoch 42 
2024-12-08 19:52:40.245738: Current learning rate: 0.00612 
2024-12-08 19:52:46.346203: train_loss -0.9754 
2024-12-08 19:52:46.352919: val_loss -0.8002 
2024-12-08 19:52:46.355963: Pseudo dice [np.float32(0.8885), np.float32(0.8713)] 
2024-12-08 19:52:46.359015: Epoch time: 6.11 s 
2024-12-08 19:52:46.873157:  
2024-12-08 19:52:46.878228: Epoch 43 
2024-12-08 19:52:46.882792: Current learning rate: 0.00603 
2024-12-08 19:52:52.992054: train_loss -0.9757 
2024-12-08 19:52:52.999175: val_loss -0.7995 
2024-12-08 19:52:53.004769: Pseudo dice [np.float32(0.8892), np.float32(0.8708)] 
2024-12-08 19:52:53.007822: Epoch time: 6.12 s 
2024-12-08 19:52:53.671963:  
2024-12-08 19:52:53.678566: Epoch 44 
2024-12-08 19:52:53.682167: Current learning rate: 0.00593 
2024-12-08 19:52:59.790247: train_loss -0.9766 
2024-12-08 19:52:59.795855: val_loss -0.8028 
2024-12-08 19:52:59.798389: Pseudo dice [np.float32(0.8906), np.float32(0.8716)] 
2024-12-08 19:52:59.802937: Epoch time: 6.12 s 
2024-12-08 19:53:00.318005:  
2024-12-08 19:53:00.323117: Epoch 45 
2024-12-08 19:53:00.326721: Current learning rate: 0.00584 
2024-12-08 19:53:06.434471: train_loss -0.9769 
2024-12-08 19:53:06.440058: val_loss -0.7977 
2024-12-08 19:53:06.442593: Pseudo dice [np.float32(0.886), np.float32(0.869)] 
2024-12-08 19:53:06.446635: Epoch time: 6.12 s 
2024-12-08 19:53:06.958869:  
2024-12-08 19:53:06.965039: Epoch 46 
2024-12-08 19:53:06.969106: Current learning rate: 0.00574 
2024-12-08 19:53:13.079006: train_loss -0.9779 
2024-12-08 19:53:13.084089: val_loss -0.8047 
2024-12-08 19:53:13.088645: Pseudo dice [np.float32(0.8913), np.float32(0.8733)] 
2024-12-08 19:53:13.091184: Epoch time: 6.12 s 
2024-12-08 19:53:13.606759:  
2024-12-08 19:53:13.611822: Epoch 47 
2024-12-08 19:53:13.614920: Current learning rate: 0.00565 
2024-12-08 19:53:19.726305: train_loss -0.9776 
2024-12-08 19:53:19.731903: val_loss -0.8015 
2024-12-08 19:53:19.735447: Pseudo dice [np.float32(0.8895), np.float32(0.8727)] 
2024-12-08 19:53:19.738511: Epoch time: 6.12 s 
2024-12-08 19:53:20.257040:  
2024-12-08 19:53:20.262104: Epoch 48 
2024-12-08 19:53:20.265194: Current learning rate: 0.00555 
2024-12-08 19:53:26.368866: train_loss -0.9779 
2024-12-08 19:53:26.373967: val_loss -0.7966 
2024-12-08 19:53:26.377008: Pseudo dice [np.float32(0.8869), np.float32(0.8713)] 
2024-12-08 19:53:26.380547: Epoch time: 6.11 s 
2024-12-08 19:53:26.898978:  
2024-12-08 19:53:26.904547: Epoch 49 
2024-12-08 19:53:26.908126: Current learning rate: 0.00546 
2024-12-08 19:53:33.004982: train_loss -0.9786 
2024-12-08 19:53:33.010589: val_loss -0.8001 
2024-12-08 19:53:33.014126: Pseudo dice [np.float32(0.889), np.float32(0.8716)] 
2024-12-08 19:53:33.017180: Epoch time: 6.11 s 
2024-12-08 19:53:33.575293:  
2024-12-08 19:53:33.581358: Epoch 50 
2024-12-08 19:53:33.584413: Current learning rate: 0.00536 
2024-12-08 19:53:39.694172: train_loss -0.9792 
2024-12-08 19:53:39.699833: val_loss -0.7988 
2024-12-08 19:53:39.702371: Pseudo dice [np.float32(0.8892), np.float32(0.8715)] 
2024-12-08 19:53:39.705921: Epoch time: 6.12 s 
2024-12-08 19:53:40.229330:  
2024-12-08 19:53:40.233397: Epoch 51 
2024-12-08 19:53:40.237467: Current learning rate: 0.00526 
2024-12-08 19:53:46.345966: train_loss -0.9793 
2024-12-08 19:53:46.350521: val_loss -0.7921 
2024-12-08 19:53:46.354078: Pseudo dice [np.float32(0.8852), np.float32(0.868)] 
2024-12-08 19:53:46.357627: Epoch time: 6.12 s 
2024-12-08 19:53:46.896901:  
2024-12-08 19:53:46.901967: Epoch 52 
2024-12-08 19:53:46.905043: Current learning rate: 0.00517 
2024-12-08 19:53:53.022453: train_loss -0.9799 
2024-12-08 19:53:53.029543: val_loss -0.8011 
2024-12-08 19:53:53.033170: Pseudo dice [np.float32(0.8903), np.float32(0.8732)] 
2024-12-08 19:53:53.036745: Epoch time: 6.13 s 
2024-12-08 19:53:53.701682:  
2024-12-08 19:53:53.707275: Epoch 53 
2024-12-08 19:53:53.709805: Current learning rate: 0.00507 
2024-12-08 19:53:59.947262: train_loss -0.9802 
2024-12-08 19:53:59.953851: val_loss -0.8046 
2024-12-08 19:53:59.956892: Pseudo dice [np.float32(0.8925), np.float32(0.8752)] 
2024-12-08 19:53:59.959418: Epoch time: 6.25 s 
2024-12-08 19:54:00.482617:  
2024-12-08 19:54:00.487684: Epoch 54 
2024-12-08 19:54:00.490724: Current learning rate: 0.00497 
2024-12-08 19:54:06.612799: train_loss -0.9806 
2024-12-08 19:54:06.619901: val_loss -0.7946 
2024-12-08 19:54:06.622967: Pseudo dice [np.float32(0.8882), np.float32(0.8698)] 
2024-12-08 19:54:06.625512: Epoch time: 6.13 s 
2024-12-08 19:54:07.144027:  
2024-12-08 19:54:07.149136: Epoch 55 
2024-12-08 19:54:07.153180: Current learning rate: 0.00487 
2024-12-08 19:54:13.306859: train_loss -0.9806 
2024-12-08 19:54:13.312480: val_loss -0.7951 
2024-12-08 19:54:13.315025: Pseudo dice [np.float32(0.8877), np.float32(0.8695)] 
2024-12-08 19:54:13.318591: Epoch time: 6.16 s 
2024-12-08 19:54:13.838109:  
2024-12-08 19:54:13.843160: Epoch 56 
2024-12-08 19:54:13.846220: Current learning rate: 0.00478 
2024-12-08 19:54:19.974850: train_loss -0.9812 
2024-12-08 19:54:19.980450: val_loss -0.7959 
2024-12-08 19:54:19.983518: Pseudo dice [np.float32(0.8884), np.float32(0.87)] 
2024-12-08 19:54:19.985554: Epoch time: 6.14 s 
2024-12-08 19:54:20.503760:  
2024-12-08 19:54:20.509344: Epoch 57 
2024-12-08 19:54:20.512384: Current learning rate: 0.00468 
2024-12-08 19:54:26.645758: train_loss -0.9818 
2024-12-08 19:54:26.650368: val_loss -0.7986 
2024-12-08 19:54:26.654409: Pseudo dice [np.float32(0.8895), np.float32(0.8734)] 
2024-12-08 19:54:26.657507: Epoch time: 6.14 s 
2024-12-08 19:54:27.172968:  
2024-12-08 19:54:27.179022: Epoch 58 
2024-12-08 19:54:27.182579: Current learning rate: 0.00458 
2024-12-08 19:54:33.300628: train_loss -0.982 
2024-12-08 19:54:33.305248: val_loss -0.7987 
2024-12-08 19:54:33.309790: Pseudo dice [np.float32(0.8897), np.float32(0.8715)] 
2024-12-08 19:54:33.312850: Epoch time: 6.13 s 
2024-12-08 19:54:33.835370:  
2024-12-08 19:54:33.840488: Epoch 59 
2024-12-08 19:54:33.844571: Current learning rate: 0.00448 
2024-12-08 19:54:39.996791: train_loss -0.9822 
2024-12-08 19:54:40.001953: val_loss -0.7943 
2024-12-08 19:54:40.005520: Pseudo dice [np.float32(0.8887), np.float32(0.8707)] 
2024-12-08 19:54:40.008565: Epoch time: 6.16 s 
2024-12-08 19:54:40.532186:  
2024-12-08 19:54:40.537286: Epoch 60 
2024-12-08 19:54:40.539825: Current learning rate: 0.00438 
2024-12-08 19:54:46.670587: train_loss -0.9823 
2024-12-08 19:54:46.675696: val_loss -0.8003 
2024-12-08 19:54:46.679241: Pseudo dice [np.float32(0.8912), np.float32(0.8735)] 
2024-12-08 19:54:46.682288: Epoch time: 6.14 s 
2024-12-08 19:54:47.351703:  
2024-12-08 19:54:47.356797: Epoch 61 
2024-12-08 19:54:47.360348: Current learning rate: 0.00429 
2024-12-08 19:54:53.479115: train_loss -0.9829 
2024-12-08 19:54:53.487226: val_loss -0.796 
2024-12-08 19:54:53.491823: Pseudo dice [np.float32(0.8887), np.float32(0.873)] 
2024-12-08 19:54:53.494401: Epoch time: 6.13 s 
2024-12-08 19:54:54.021243:  
2024-12-08 19:54:54.026844: Epoch 62 
2024-12-08 19:54:54.029894: Current learning rate: 0.00419 
2024-12-08 19:55:00.163401: train_loss -0.9831 
2024-12-08 19:55:00.169476: val_loss -0.7908 
2024-12-08 19:55:00.172073: Pseudo dice [np.float32(0.8862), np.float32(0.8687)] 
2024-12-08 19:55:00.175660: Epoch time: 6.14 s 
2024-12-08 19:55:00.703319:  
2024-12-08 19:55:00.708385: Epoch 63 
2024-12-08 19:55:00.711942: Current learning rate: 0.00409 
2024-12-08 19:55:06.842873: train_loss -0.9834 
2024-12-08 19:55:06.848007: val_loss -0.7938 
2024-12-08 19:55:06.851570: Pseudo dice [np.float32(0.8881), np.float32(0.8709)] 
2024-12-08 19:55:06.854111: Epoch time: 6.14 s 
2024-12-08 19:55:07.382597:  
2024-12-08 19:55:07.386647: Epoch 64 
2024-12-08 19:55:07.390190: Current learning rate: 0.00399 
2024-12-08 19:55:13.516818: train_loss -0.9838 
2024-12-08 19:55:13.521929: val_loss -0.792 
2024-12-08 19:55:13.538180: Pseudo dice [np.float32(0.8874), np.float32(0.8697)] 
2024-12-08 19:55:13.540717: Epoch time: 6.14 s 
2024-12-08 19:55:14.068357:  
2024-12-08 19:55:14.074947: Epoch 65 
2024-12-08 19:55:14.079020: Current learning rate: 0.00389 
2024-12-08 19:55:20.204081: train_loss -0.9839 
2024-12-08 19:55:20.209155: val_loss -0.7933 
2024-12-08 19:55:20.212722: Pseudo dice [np.float32(0.8878), np.float32(0.8705)] 
2024-12-08 19:55:20.214755: Epoch time: 6.14 s 
2024-12-08 19:55:20.739801:  
2024-12-08 19:55:20.744863: Epoch 66 
2024-12-08 19:55:20.747396: Current learning rate: 0.00379 
2024-12-08 19:55:26.877000: train_loss -0.9835 
2024-12-08 19:55:26.882614: val_loss -0.797 
2024-12-08 19:55:26.886206: Pseudo dice [np.float32(0.8896), np.float32(0.8714)] 
2024-12-08 19:55:26.888744: Epoch time: 6.14 s 
2024-12-08 19:55:27.422429:  
2024-12-08 19:55:27.427530: Epoch 67 
2024-12-08 19:55:27.430597: Current learning rate: 0.00369 
2024-12-08 19:55:33.554831: train_loss -0.9838 
2024-12-08 19:55:33.561450: val_loss -0.7894 
2024-12-08 19:55:33.565001: Pseudo dice [np.float32(0.8855), np.float32(0.868)] 
2024-12-08 19:55:33.567542: Epoch time: 6.13 s 
2024-12-08 19:55:34.099343:  
2024-12-08 19:55:34.105525: Epoch 68 
2024-12-08 19:55:34.108059: Current learning rate: 0.00359 
2024-12-08 19:55:40.233537: train_loss -0.9837 
2024-12-08 19:55:40.239654: val_loss -0.7954 
2024-12-08 19:55:40.243217: Pseudo dice [np.float32(0.8893), np.float32(0.8715)] 
2024-12-08 19:55:40.246263: Epoch time: 6.13 s 
2024-12-08 19:55:40.918412:  
2024-12-08 19:55:40.923996: Epoch 69 
2024-12-08 19:55:40.926540: Current learning rate: 0.00349 
2024-12-08 19:55:47.049429: train_loss -0.9842 
2024-12-08 19:55:47.054575: val_loss -0.795 
2024-12-08 19:55:47.057126: Pseudo dice [np.float32(0.8896), np.float32(0.8727)] 
2024-12-08 19:55:47.060703: Epoch time: 6.13 s 
2024-12-08 19:55:47.592231:  
2024-12-08 19:55:47.597814: Epoch 70 
2024-12-08 19:55:47.600350: Current learning rate: 0.00338 
2024-12-08 19:55:53.721867: train_loss -0.9847 
2024-12-08 19:55:53.728968: val_loss -0.7987 
2024-12-08 19:55:53.732570: Pseudo dice [np.float32(0.8911), np.float32(0.8728)] 
2024-12-08 19:55:53.737124: Epoch time: 6.13 s 
2024-12-08 19:55:54.276128:  
2024-12-08 19:55:54.281788: Epoch 71 
2024-12-08 19:55:54.284867: Current learning rate: 0.00328 
2024-12-08 19:56:00.411806: train_loss -0.9846 
2024-12-08 19:56:00.416876: val_loss -0.7923 
2024-12-08 19:56:00.419418: Pseudo dice [np.float32(0.8877), np.float32(0.8697)] 
2024-12-08 19:56:00.423972: Epoch time: 6.14 s 
2024-12-08 19:56:00.963438:  
2024-12-08 19:56:00.968509: Epoch 72 
2024-12-08 19:56:00.971035: Current learning rate: 0.00318 
2024-12-08 19:56:07.107352: train_loss -0.9851 
2024-12-08 19:56:07.112955: val_loss -0.7923 
2024-12-08 19:56:07.116568: Pseudo dice [np.float32(0.8881), np.float32(0.8707)] 
2024-12-08 19:56:07.119101: Epoch time: 6.14 s 
2024-12-08 19:56:07.654088:  
2024-12-08 19:56:07.660656: Epoch 73 
2024-12-08 19:56:07.664279: Current learning rate: 0.00308 
2024-12-08 19:56:13.791327: train_loss -0.9852 
2024-12-08 19:56:13.797894: val_loss -0.7876 
2024-12-08 19:56:13.800956: Pseudo dice [np.float32(0.8862), np.float32(0.8675)] 
2024-12-08 19:56:13.803992: Epoch time: 6.14 s 
2024-12-08 19:56:14.331959:  
2024-12-08 19:56:14.339571: Epoch 74 
2024-12-08 19:56:14.343150: Current learning rate: 0.00297 
2024-12-08 19:56:20.463510: train_loss -0.9855 
2024-12-08 19:56:20.469091: val_loss -0.7938 
2024-12-08 19:56:20.471625: Pseudo dice [np.float32(0.888), np.float32(0.8709)] 
2024-12-08 19:56:20.474158: Epoch time: 6.13 s 
2024-12-08 19:56:21.006711:  
2024-12-08 19:56:21.011832: Epoch 75 
2024-12-08 19:56:21.016922: Current learning rate: 0.00287 
2024-12-08 19:56:27.138557: train_loss -0.9855 
2024-12-08 19:56:27.144179: val_loss -0.7938 
2024-12-08 19:56:27.147243: Pseudo dice [np.float32(0.8887), np.float32(0.8707)] 
2024-12-08 19:56:27.150313: Epoch time: 6.13 s 
2024-12-08 19:56:27.683438:  
2024-12-08 19:56:27.689519: Epoch 76 
2024-12-08 19:56:27.692587: Current learning rate: 0.00277 
2024-12-08 19:56:33.813716: train_loss -0.9858 
2024-12-08 19:56:33.819802: val_loss -0.7873 
2024-12-08 19:56:33.823914: Pseudo dice [np.float32(0.887), np.float32(0.8695)] 
2024-12-08 19:56:33.826462: Epoch time: 6.13 s 
2024-12-08 19:56:34.493568:  
2024-12-08 19:56:34.498678: Epoch 77 
2024-12-08 19:56:34.502251: Current learning rate: 0.00266 
2024-12-08 19:56:40.624603: train_loss -0.9861 
2024-12-08 19:56:40.630199: val_loss -0.7913 
2024-12-08 19:56:40.633266: Pseudo dice [np.float32(0.8885), np.float32(0.8694)] 
2024-12-08 19:56:40.636375: Epoch time: 6.13 s 
2024-12-08 19:56:41.170853:  
2024-12-08 19:56:41.175912: Epoch 78 
2024-12-08 19:56:41.178957: Current learning rate: 0.00256 
2024-12-08 19:56:47.312235: train_loss -0.9869 
2024-12-08 19:56:47.318356: val_loss -0.7872 
2024-12-08 19:56:47.322440: Pseudo dice [np.float32(0.8863), np.float32(0.8698)] 
2024-12-08 19:56:47.325485: Epoch time: 6.14 s 
2024-12-08 19:56:47.862966:  
2024-12-08 19:56:47.869605: Epoch 79 
2024-12-08 19:56:47.873186: Current learning rate: 0.00245 
2024-12-08 19:56:54.003011: train_loss -0.9861 
2024-12-08 19:56:54.009586: val_loss -0.7921 
2024-12-08 19:56:54.012649: Pseudo dice [np.float32(0.8877), np.float32(0.8706)] 
2024-12-08 19:56:54.015185: Epoch time: 6.14 s 
2024-12-08 19:56:54.557739:  
2024-12-08 19:56:54.564361: Epoch 80 
2024-12-08 19:56:54.567454: Current learning rate: 0.00235 
2024-12-08 19:57:00.704014: train_loss -0.9871 
2024-12-08 19:57:00.709621: val_loss -0.7979 
2024-12-08 19:57:00.711692: Pseudo dice [np.float32(0.8914), np.float32(0.8738)] 
2024-12-08 19:57:00.715749: Epoch time: 6.15 s 
2024-12-08 19:57:01.258560:  
2024-12-08 19:57:01.263660: Epoch 81 
2024-12-08 19:57:01.267228: Current learning rate: 0.00224 
2024-12-08 19:57:07.399409: train_loss -0.9865 
2024-12-08 19:57:07.406007: val_loss -0.786 
2024-12-08 19:57:07.409069: Pseudo dice [np.float32(0.8859), np.float32(0.868)] 
2024-12-08 19:57:07.412132: Epoch time: 6.14 s 
2024-12-08 19:57:07.954234:  
2024-12-08 19:57:07.960310: Epoch 82 
2024-12-08 19:57:07.963346: Current learning rate: 0.00214 
2024-12-08 19:57:14.087493: train_loss -0.9868 
2024-12-08 19:57:14.093089: val_loss -0.7957 
2024-12-08 19:57:14.096660: Pseudo dice [np.float32(0.8901), np.float32(0.8723)] 
2024-12-08 19:57:14.100195: Epoch time: 6.13 s 
2024-12-08 19:57:14.720498:  
2024-12-08 19:57:14.724053: Epoch 83 
2024-12-08 19:57:14.728114: Current learning rate: 0.00203 
2024-12-08 19:57:20.852727: train_loss -0.9872 
2024-12-08 19:57:20.857791: val_loss -0.782 
2024-12-08 19:57:20.860320: Pseudo dice [np.float32(0.8838), np.float32(0.8645)] 
2024-12-08 19:57:20.863927: Epoch time: 6.13 s 
2024-12-08 19:57:21.380349:  
2024-12-08 19:57:21.386416: Epoch 84 
2024-12-08 19:57:21.388951: Current learning rate: 0.00192 
2024-12-08 19:57:27.525243: train_loss -0.9868 
2024-12-08 19:57:27.531822: val_loss -0.793 
2024-12-08 19:57:27.534946: Pseudo dice [np.float32(0.8889), np.float32(0.8715)] 
2024-12-08 19:57:27.536973: Epoch time: 6.14 s 
2024-12-08 19:57:28.194611:  
2024-12-08 19:57:28.200188: Epoch 85 
2024-12-08 19:57:28.202725: Current learning rate: 0.00181 
2024-12-08 19:57:34.339429: train_loss -0.9876 
2024-12-08 19:57:34.345047: val_loss -0.7879 
2024-12-08 19:57:34.348138: Pseudo dice [np.float32(0.8871), np.float32(0.8686)] 
2024-12-08 19:57:34.350685: Epoch time: 6.14 s 
2024-12-08 19:57:34.869236:  
2024-12-08 19:57:34.874875: Epoch 86 
2024-12-08 19:57:34.877937: Current learning rate: 0.0017 
2024-12-08 19:57:40.997952: train_loss -0.9872 
2024-12-08 19:57:41.003623: val_loss -0.7958 
2024-12-08 19:57:41.006684: Pseudo dice [np.float32(0.8906), np.float32(0.873)] 
2024-12-08 19:57:41.009239: Epoch time: 6.13 s 
2024-12-08 19:57:41.520702:  
2024-12-08 19:57:41.525773: Epoch 87 
2024-12-08 19:57:41.529332: Current learning rate: 0.00159 
2024-12-08 19:57:47.653004: train_loss -0.9873 
2024-12-08 19:57:47.659109: val_loss -0.7862 
2024-12-08 19:57:47.662153: Pseudo dice [np.float32(0.8867), np.float32(0.8689)] 
2024-12-08 19:57:47.665260: Epoch time: 6.13 s 
2024-12-08 19:57:48.196913:  
2024-12-08 19:57:48.202487: Epoch 88 
2024-12-08 19:57:48.205025: Current learning rate: 0.00148 
2024-12-08 19:57:54.331512: train_loss -0.9878 
2024-12-08 19:57:54.338106: val_loss -0.7923 
2024-12-08 19:57:54.344747: Pseudo dice [np.float32(0.8887), np.float32(0.8706)] 
2024-12-08 19:57:54.346770: Epoch time: 6.14 s 
2024-12-08 19:57:54.860296:  
2024-12-08 19:57:54.865361: Epoch 89 
2024-12-08 19:57:54.868402: Current learning rate: 0.00137 
2024-12-08 19:58:00.992255: train_loss -0.9871 
2024-12-08 19:58:00.998820: val_loss -0.7887 
2024-12-08 19:58:01.001898: Pseudo dice [np.float32(0.888), np.float32(0.87)] 
2024-12-08 19:58:01.005448: Epoch time: 6.13 s 
2024-12-08 19:58:01.523451:  
2024-12-08 19:58:01.528544: Epoch 90 
2024-12-08 19:58:01.532091: Current learning rate: 0.00126 
2024-12-08 19:58:07.666337: train_loss -0.9879 
2024-12-08 19:58:07.672964: val_loss -0.7827 
2024-12-08 19:58:07.675500: Pseudo dice [np.float32(0.8844), np.float32(0.8668)] 
2024-12-08 19:58:07.678537: Epoch time: 6.14 s 
2024-12-08 19:58:08.188011:  
2024-12-08 19:58:08.193100: Epoch 91 
2024-12-08 19:58:08.195634: Current learning rate: 0.00115 
2024-12-08 19:58:14.331793: train_loss -0.9875 
2024-12-08 19:58:14.337374: val_loss -0.7901 
2024-12-08 19:58:14.339408: Pseudo dice [np.float32(0.8874), np.float32(0.8698)] 
2024-12-08 19:58:14.343452: Epoch time: 6.15 s 
2024-12-08 19:58:14.853837:  
2024-12-08 19:58:14.858906: Epoch 92 
2024-12-08 19:58:14.861960: Current learning rate: 0.00103 
2024-12-08 19:58:20.987173: train_loss -0.9874 
2024-12-08 19:58:20.993750: val_loss -0.7862 
2024-12-08 19:58:20.997354: Pseudo dice [np.float32(0.8858), np.float32(0.8684)] 
2024-12-08 19:58:21.000401: Epoch time: 6.13 s 
2024-12-08 19:58:21.646152:  
2024-12-08 19:58:21.652761: Epoch 93 
2024-12-08 19:58:21.655808: Current learning rate: 0.00091 
2024-12-08 19:58:27.777699: train_loss -0.9886 
2024-12-08 19:58:27.783296: val_loss -0.7894 
2024-12-08 19:58:27.786889: Pseudo dice [np.float32(0.8883), np.float32(0.8704)] 
2024-12-08 19:58:27.789944: Epoch time: 6.13 s 
2024-12-08 19:58:28.296372:  
2024-12-08 19:58:28.301931: Epoch 94 
2024-12-08 19:58:28.305003: Current learning rate: 0.00079 
2024-12-08 19:58:34.438303: train_loss -0.9883 
2024-12-08 19:58:34.442883: val_loss -0.7944 
2024-12-08 19:58:34.446924: Pseudo dice [np.float32(0.8901), np.float32(0.8723)] 
2024-12-08 19:58:34.449989: Epoch time: 6.14 s 
2024-12-08 19:58:34.957519:  
2024-12-08 19:58:34.962623: Epoch 95 
2024-12-08 19:58:34.965693: Current learning rate: 0.00067 
2024-12-08 19:58:41.088878: train_loss -0.988 
2024-12-08 19:58:41.093990: val_loss -0.7912 
2024-12-08 19:58:41.097555: Pseudo dice [np.float32(0.8885), np.float32(0.8711)] 
2024-12-08 19:58:41.100091: Epoch time: 6.13 s 
2024-12-08 19:58:41.616357:  
2024-12-08 19:58:41.622003: Epoch 96 
2024-12-08 19:58:41.624562: Current learning rate: 0.00055 
2024-12-08 19:58:47.747936: train_loss -0.9881 
2024-12-08 19:58:47.753591: val_loss -0.7834 
2024-12-08 19:58:47.755628: Pseudo dice [np.float32(0.8845), np.float32(0.8664)] 
2024-12-08 19:58:47.758175: Epoch time: 6.13 s 
2024-12-08 19:58:48.278222:  
2024-12-08 19:58:48.284862: Epoch 97 
2024-12-08 19:58:48.288407: Current learning rate: 0.00043 
2024-12-08 19:58:54.410644: train_loss -0.9887 
2024-12-08 19:58:54.417272: val_loss -0.7861 
2024-12-08 19:58:54.422882: Pseudo dice [np.float32(0.8858), np.float32(0.8681)] 
2024-12-08 19:58:54.424906: Epoch time: 6.13 s 
2024-12-08 19:58:54.949986:  
2024-12-08 19:58:54.957082: Epoch 98 
2024-12-08 19:58:54.962654: Current learning rate: 0.0003 
2024-12-08 19:59:01.081546: train_loss -0.9885 
2024-12-08 19:59:01.087636: val_loss -0.7893 
2024-12-08 19:59:01.089682: Pseudo dice [np.float32(0.8876), np.float32(0.869)] 
2024-12-08 19:59:01.093764: Epoch time: 6.13 s 
2024-12-08 19:59:01.616957:  
2024-12-08 19:59:01.622552: Epoch 99 
2024-12-08 19:59:01.625627: Current learning rate: 0.00016 
2024-12-08 19:59:07.918428: train_loss -0.989 
2024-12-08 19:59:07.923510: val_loss -0.7841 
2024-12-08 19:59:07.927554: Pseudo dice [np.float32(0.8863), np.float32(0.8669)] 
2024-12-08 19:59:07.931166: Epoch time: 6.3 s 
2024-12-08 19:59:08.499228: Training done. 
2024-12-08 19:59:08.538577: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2024-12-08 19:59:08.541586: The split file contains 5 splits. 
2024-12-08 19:59:08.547602: Desired fold for training: 0 
2024-12-08 19:59:08.547602: This split has 208 training and 52 validation cases. 
2024-12-08 19:59:08.555959: predicting hippocampus_017 
2024-12-08 19:59:08.564363: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2024-12-08 19:59:08.654232: predicting hippocampus_019 
2024-12-08 19:59:08.654232: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2024-12-08 19:59:08.670336: predicting hippocampus_033 
2024-12-08 19:59:08.678563: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2024-12-08 19:59:08.686660: predicting hippocampus_035 
2024-12-08 19:59:08.694789: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2024-12-08 19:59:08.702901: predicting hippocampus_037 
2024-12-08 19:59:08.711353: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2024-12-08 19:59:08.723414: predicting hippocampus_049 
2024-12-08 19:59:08.727420: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2024-12-08 19:59:08.739737: predicting hippocampus_052 
2024-12-08 19:59:08.745771: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2024-12-08 19:59:08.756113: predicting hippocampus_065 
2024-12-08 19:59:08.762124: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2024-12-08 19:59:08.768140: predicting hippocampus_083 
2024-12-08 19:59:08.776520: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2024-12-08 19:59:08.791004: predicting hippocampus_088 
2024-12-08 19:59:08.793008: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2024-12-08 19:59:12.443302: predicting hippocampus_090 
2024-12-08 19:59:12.451318: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2024-12-08 19:59:12.463337: predicting hippocampus_092 
2024-12-08 19:59:12.473588: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2024-12-08 19:59:12.491616: predicting hippocampus_095 
2024-12-08 19:59:12.499845: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2024-12-08 19:59:12.521897: predicting hippocampus_107 
2024-12-08 19:59:12.529898: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2024-12-08 19:59:12.547926: predicting hippocampus_108 
2024-12-08 19:59:12.553933: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2024-12-08 19:59:12.569953: predicting hippocampus_123 
2024-12-08 19:59:12.575962: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2024-12-08 19:59:12.591984: predicting hippocampus_125 
2024-12-08 19:59:12.595989: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2024-12-08 19:59:12.622028: predicting hippocampus_157 
2024-12-08 19:59:12.628038: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2024-12-08 19:59:12.652074: predicting hippocampus_164 
2024-12-08 19:59:12.676105: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2024-12-08 19:59:12.714148: predicting hippocampus_169 
2024-12-08 19:59:12.720156: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2024-12-08 19:59:12.736173: predicting hippocampus_175 
2024-12-08 19:59:12.740178: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2024-12-08 19:59:12.756199: predicting hippocampus_185 
2024-12-08 19:59:12.762209: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2024-12-08 19:59:12.776226: predicting hippocampus_190 
2024-12-08 19:59:12.782232: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2024-12-08 19:59:12.800257: predicting hippocampus_194 
2024-12-08 19:59:12.804263: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2024-12-08 19:59:12.818280: predicting hippocampus_204 
2024-12-08 19:59:12.824288: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2024-12-08 19:59:12.838311: predicting hippocampus_205 
2024-12-08 19:59:12.844320: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2024-12-08 19:59:12.858338: predicting hippocampus_210 
2024-12-08 19:59:12.864344: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2024-12-08 19:59:12.880361: predicting hippocampus_217 
2024-12-08 19:59:12.884368: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2024-12-08 19:59:12.898386: predicting hippocampus_219 
2024-12-08 19:59:12.904393: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2024-12-08 19:59:12.918409: predicting hippocampus_229 
2024-12-08 19:59:12.922414: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2024-12-08 19:59:12.936430: predicting hippocampus_244 
2024-12-08 19:59:12.940435: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2024-12-08 19:59:12.952451: predicting hippocampus_261 
2024-12-08 19:59:12.958459: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2024-12-08 19:59:12.978484: predicting hippocampus_264 
2024-12-08 19:59:12.982490: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2024-12-08 19:59:12.996508: predicting hippocampus_277 
2024-12-08 19:59:13.002258: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2024-12-08 19:59:13.022281: predicting hippocampus_280 
2024-12-08 19:59:13.026287: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2024-12-08 19:59:13.040304: predicting hippocampus_286 
2024-12-08 19:59:13.044311: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2024-12-08 19:59:13.062332: predicting hippocampus_288 
2024-12-08 19:59:13.066337: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2024-12-08 19:59:13.086360: predicting hippocampus_289 
2024-12-08 19:59:13.090365: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2024-12-08 19:59:13.104382: predicting hippocampus_296 
2024-12-08 19:59:13.108387: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2024-12-08 19:59:13.122404: predicting hippocampus_305 
2024-12-08 19:59:13.126409: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2024-12-08 19:59:13.138423: predicting hippocampus_308 
2024-12-08 19:59:13.142427: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2024-12-08 19:59:13.156444: predicting hippocampus_317 
2024-12-08 19:59:13.160450: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2024-12-08 19:59:13.174472: predicting hippocampus_327 
2024-12-08 19:59:13.178478: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2024-12-08 19:59:13.192497: predicting hippocampus_330 
2024-12-08 19:59:13.196502: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2024-12-08 19:59:13.208517: predicting hippocampus_332 
2024-12-08 19:59:13.214526: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2024-12-08 19:59:13.226544: predicting hippocampus_338 
2024-12-08 19:59:13.230550: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2024-12-08 19:59:13.250575: predicting hippocampus_349 
2024-12-08 19:59:13.256581: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2024-12-08 19:59:13.268595: predicting hippocampus_350 
2024-12-08 19:59:13.274603: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2024-12-08 19:59:13.286621: predicting hippocampus_356 
2024-12-08 19:59:13.292632: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2024-12-08 19:59:13.304650: predicting hippocampus_358 
2024-12-08 19:59:13.308654: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2024-12-08 19:59:13.322672: predicting hippocampus_374 
2024-12-08 19:59:13.326677: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2024-12-08 19:59:13.338693: predicting hippocampus_394 
2024-12-08 19:59:13.344702: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2024-12-08 19:59:16.810472: Validation complete 
2024-12-08 19:59:16.820635: Mean Validation Dice:  0.8779224710526914 
