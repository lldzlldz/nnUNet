
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-17 07:48:14.717616: do_dummy_2d_data_aug: False 
2025-03-17 07:48:14.736616: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-17 07:48:14.742617: The split file contains 5 splits. 
2025-03-17 07:48:14.745617: Desired fold for training: 0 
2025-03-17 07:48:14.748618: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-17 07:48:20.770084: unpacking dataset... 
2025-03-17 07:48:20.970788: unpacking done... 
2025-03-17 07:48:22.190977:  
2025-03-17 07:48:22.194991: Epoch 0 
2025-03-17 07:48:22.199004: Current learning rate: 0.01 
2025-03-17 07:48:29.523018: train_loss -0.441 
2025-03-17 07:48:29.529169: val_loss -0.8126 
2025-03-17 07:48:29.531720: Pseudo dice [np.float32(0.8638), np.float32(0.8563)] 
2025-03-17 07:48:29.535278: Epoch time: 7.33 s 
2025-03-17 07:48:29.538347: Yayy! New best EMA pseudo Dice: 0.8600999712944031 
2025-03-17 07:48:30.079453:  
2025-03-17 07:48:30.085119: Epoch 1 
2025-03-17 07:48:30.087667: Current learning rate: 0.00991 
2025-03-17 07:48:36.541423: train_loss -0.8147 
2025-03-17 07:48:36.549029: val_loss -0.8319 
2025-03-17 07:48:36.552071: Pseudo dice [np.float32(0.8825), np.float32(0.8646)] 
2025-03-17 07:48:36.555099: Epoch time: 6.46 s 
2025-03-17 07:48:36.559212: Yayy! New best EMA pseudo Dice: 0.8614000082015991 
2025-03-17 07:48:37.162954:  
2025-03-17 07:48:37.169036: Epoch 2 
2025-03-17 07:48:37.172599: Current learning rate: 0.00982 
2025-03-17 07:48:43.616787: train_loss -0.8368 
2025-03-17 07:48:43.622807: val_loss -0.8425 
2025-03-17 07:48:43.625860: Pseudo dice [np.float32(0.8912), np.float32(0.8736)] 
2025-03-17 07:48:43.628935: Epoch time: 6.45 s 
2025-03-17 07:48:43.632009: Yayy! New best EMA pseudo Dice: 0.8634999990463257 
2025-03-17 07:48:44.257984:  
2025-03-17 07:48:44.264019: Epoch 3 
2025-03-17 07:48:44.266336: Current learning rate: 0.00973 
2025-03-17 07:48:50.702198: train_loss -0.8485 
2025-03-17 07:48:50.707772: val_loss -0.8441 
2025-03-17 07:48:50.711333: Pseudo dice [np.float32(0.8944), np.float32(0.8755)] 
2025-03-17 07:48:50.714364: Epoch time: 6.44 s 
2025-03-17 07:48:50.717380: Yayy! New best EMA pseudo Dice: 0.8657000064849854 
2025-03-17 07:48:51.326982:  
2025-03-17 07:48:51.332495: Epoch 4 
2025-03-17 07:48:51.335003: Current learning rate: 0.00964 
2025-03-17 07:48:57.774227: train_loss -0.8552 
2025-03-17 07:48:57.780053: val_loss -0.8379 
2025-03-17 07:48:57.784107: Pseudo dice [np.float32(0.8903), np.float32(0.8667)] 
2025-03-17 07:48:57.787127: Epoch time: 6.45 s 
2025-03-17 07:48:57.790168: Yayy! New best EMA pseudo Dice: 0.8669000267982483 
2025-03-17 07:48:58.534787:  
2025-03-17 07:48:58.540817: Epoch 5 
2025-03-17 07:48:58.543852: Current learning rate: 0.00955 
2025-03-17 07:49:04.992054: train_loss -0.8595 
2025-03-17 07:49:04.998024: val_loss -0.8467 
2025-03-17 07:49:05.001128: Pseudo dice [np.float32(0.8968), np.float32(0.8754)] 
2025-03-17 07:49:05.003634: Epoch time: 6.46 s 
2025-03-17 07:49:05.007652: Yayy! New best EMA pseudo Dice: 0.8689000010490417 
2025-03-17 07:49:05.600640:  
2025-03-17 07:49:05.606201: Epoch 6 
2025-03-17 07:49:05.608755: Current learning rate: 0.00946 
2025-03-17 07:49:12.038275: train_loss -0.8656 
2025-03-17 07:49:12.043344: val_loss -0.8479 
2025-03-17 07:49:12.047492: Pseudo dice [np.float32(0.8973), np.float32(0.8772)] 
2025-03-17 07:49:12.050041: Epoch time: 6.44 s 
2025-03-17 07:49:12.053598: Yayy! New best EMA pseudo Dice: 0.8707000017166138 
2025-03-17 07:49:12.659445:  
2025-03-17 07:49:12.664456: Epoch 7 
2025-03-17 07:49:12.667968: Current learning rate: 0.00937 
2025-03-17 07:49:19.109852: train_loss -0.8692 
2025-03-17 07:49:19.115431: val_loss -0.8441 
2025-03-17 07:49:19.119556: Pseudo dice [np.float32(0.8935), np.float32(0.8755)] 
2025-03-17 07:49:19.122106: Epoch time: 6.45 s 
2025-03-17 07:49:19.125635: Yayy! New best EMA pseudo Dice: 0.8720999956130981 
2025-03-17 07:49:19.740811:  
2025-03-17 07:49:19.746874: Epoch 8 
2025-03-17 07:49:19.749958: Current learning rate: 0.00928 
2025-03-17 07:49:26.191025: train_loss -0.8737 
2025-03-17 07:49:26.198123: val_loss -0.8478 
2025-03-17 07:49:26.202696: Pseudo dice [np.float32(0.8968), np.float32(0.8768)] 
2025-03-17 07:49:26.205618: Epoch time: 6.45 s 
2025-03-17 07:49:26.209151: Yayy! New best EMA pseudo Dice: 0.8736000061035156 
2025-03-17 07:49:26.830687:  
2025-03-17 07:49:26.835778: Epoch 9 
2025-03-17 07:49:26.839290: Current learning rate: 0.00919 
2025-03-17 07:49:33.284057: train_loss -0.8768 
2025-03-17 07:49:33.290019: val_loss -0.8436 
2025-03-17 07:49:33.293040: Pseudo dice [np.float32(0.8941), np.float32(0.8746)] 
2025-03-17 07:49:33.296551: Epoch time: 6.45 s 
2025-03-17 07:49:33.300107: Yayy! New best EMA pseudo Dice: 0.8745999932289124 
2025-03-17 07:49:33.888730:  
2025-03-17 07:49:33.895036: Epoch 10 
2025-03-17 07:49:33.897557: Current learning rate: 0.0091 
2025-03-17 07:49:40.327826: train_loss -0.88 
2025-03-17 07:49:40.333696: val_loss -0.8503 
2025-03-17 07:49:40.336721: Pseudo dice [np.float32(0.8988), np.float32(0.8801)] 
2025-03-17 07:49:40.340263: Epoch time: 6.44 s 
2025-03-17 07:49:40.343315: Yayy! New best EMA pseudo Dice: 0.8761000037193298 
2025-03-17 07:49:40.934097:  
2025-03-17 07:49:40.940166: Epoch 11 
2025-03-17 07:49:40.944216: Current learning rate: 0.009 
2025-03-17 07:49:47.382723: train_loss -0.8823 
2025-03-17 07:49:47.388500: val_loss -0.8495 
2025-03-17 07:49:47.391006: Pseudo dice [np.float32(0.8978), np.float32(0.8792)] 
2025-03-17 07:49:47.394517: Epoch time: 6.45 s 
2025-03-17 07:49:47.398023: Yayy! New best EMA pseudo Dice: 0.8773999810218811 
2025-03-17 07:49:48.001600:  
2025-03-17 07:49:48.007116: Epoch 12 
2025-03-17 07:49:48.009623: Current learning rate: 0.00891 
2025-03-17 07:49:54.443640: train_loss -0.885 
2025-03-17 07:49:54.449192: val_loss -0.847 
2025-03-17 07:49:54.452606: Pseudo dice [np.float32(0.8972), np.float32(0.877)] 
2025-03-17 07:49:54.456122: Epoch time: 6.44 s 
2025-03-17 07:49:54.459175: Yayy! New best EMA pseudo Dice: 0.8783000111579895 
2025-03-17 07:49:55.213279:  
2025-03-17 07:49:55.219058: Epoch 13 
2025-03-17 07:49:55.222569: Current learning rate: 0.00882 
2025-03-17 07:50:01.645585: train_loss -0.8883 
2025-03-17 07:50:01.651656: val_loss -0.8509 
2025-03-17 07:50:01.654698: Pseudo dice [np.float32(0.8996), np.float32(0.8811)] 
2025-03-17 07:50:01.658231: Epoch time: 6.43 s 
2025-03-17 07:50:01.660969: Yayy! New best EMA pseudo Dice: 0.8794999718666077 
2025-03-17 07:50:02.268756:  
2025-03-17 07:50:02.274275: Epoch 14 
2025-03-17 07:50:02.277786: Current learning rate: 0.00873 
2025-03-17 07:50:08.709316: train_loss -0.8908 
2025-03-17 07:50:08.715398: val_loss -0.8465 
2025-03-17 07:50:08.718934: Pseudo dice [np.float32(0.8968), np.float32(0.8772)] 
2025-03-17 07:50:08.722512: Epoch time: 6.44 s 
2025-03-17 07:50:08.725060: Yayy! New best EMA pseudo Dice: 0.880299985408783 
2025-03-17 07:50:09.338257:  
2025-03-17 07:50:09.344276: Epoch 15 
2025-03-17 07:50:09.347786: Current learning rate: 0.00864 
2025-03-17 07:50:15.763269: train_loss -0.8909 
2025-03-17 07:50:15.768839: val_loss -0.8463 
2025-03-17 07:50:15.772384: Pseudo dice [np.float32(0.8961), np.float32(0.8773)] 
2025-03-17 07:50:15.775461: Epoch time: 6.43 s 
2025-03-17 07:50:15.779091: Yayy! New best EMA pseudo Dice: 0.8809000253677368 
2025-03-17 07:50:16.393647:  
2025-03-17 07:50:16.399783: Epoch 16 
2025-03-17 07:50:16.403319: Current learning rate: 0.00855 
2025-03-17 07:50:22.819882: train_loss -0.8928 
2025-03-17 07:50:22.825901: val_loss -0.8441 
2025-03-17 07:50:22.829462: Pseudo dice [np.float32(0.8972), np.float32(0.8741)] 
2025-03-17 07:50:22.832512: Epoch time: 6.43 s 
2025-03-17 07:50:22.836027: Yayy! New best EMA pseudo Dice: 0.8813999891281128 
2025-03-17 07:50:23.465865:  
2025-03-17 07:50:23.471952: Epoch 17 
2025-03-17 07:50:23.474980: Current learning rate: 0.00846 
2025-03-17 07:50:29.904827: train_loss -0.8952 
2025-03-17 07:50:29.911432: val_loss -0.8451 
2025-03-17 07:50:29.915049: Pseudo dice [np.float32(0.8971), np.float32(0.8771)] 
2025-03-17 07:50:29.918606: Epoch time: 6.44 s 
2025-03-17 07:50:29.921138: Yayy! New best EMA pseudo Dice: 0.8820000290870667 
2025-03-17 07:50:30.546117:  
2025-03-17 07:50:30.551685: Epoch 18 
2025-03-17 07:50:30.554736: Current learning rate: 0.00836 
2025-03-17 07:50:36.993259: train_loss -0.8982 
2025-03-17 07:50:36.999347: val_loss -0.8464 
2025-03-17 07:50:37.003482: Pseudo dice [np.float32(0.8976), np.float32(0.8806)] 
2025-03-17 07:50:37.006534: Epoch time: 6.45 s 
2025-03-17 07:50:37.009557: Yayy! New best EMA pseudo Dice: 0.8827000260353088 
2025-03-17 07:50:37.680082:  
2025-03-17 07:50:37.684152: Epoch 19 
2025-03-17 07:50:37.687692: Current learning rate: 0.00827 
2025-03-17 07:50:44.125105: train_loss -0.8984 
2025-03-17 07:50:44.131155: val_loss -0.8476 
2025-03-17 07:50:44.134187: Pseudo dice [np.float32(0.898), np.float32(0.8801)] 
2025-03-17 07:50:44.136708: Epoch time: 6.45 s 
2025-03-17 07:50:44.140735: Yayy! New best EMA pseudo Dice: 0.8833000063896179 
2025-03-17 07:50:44.758575:  
2025-03-17 07:50:44.763077: Epoch 20 
2025-03-17 07:50:44.766092: Current learning rate: 0.00818 
2025-03-17 07:50:51.200386: train_loss -0.8995 
2025-03-17 07:50:51.205957: val_loss -0.8442 
2025-03-17 07:50:51.209561: Pseudo dice [np.float32(0.8961), np.float32(0.8767)] 
2025-03-17 07:50:51.212615: Epoch time: 6.44 s 
2025-03-17 07:50:51.215652: Yayy! New best EMA pseudo Dice: 0.8835999965667725 
2025-03-17 07:50:51.986036:  
2025-03-17 07:50:51.990076: Epoch 21 
2025-03-17 07:50:51.994672: Current learning rate: 0.00809 
2025-03-17 07:50:58.424007: train_loss -0.9007 
2025-03-17 07:50:58.430248: val_loss -0.8406 
2025-03-17 07:50:58.433319: Pseudo dice [np.float32(0.8938), np.float32(0.8755)] 
2025-03-17 07:50:58.436873: Epoch time: 6.44 s 
2025-03-17 07:50:58.439930: Yayy! New best EMA pseudo Dice: 0.8837000131607056 
2025-03-17 07:50:59.030157:  
2025-03-17 07:50:59.036209: Epoch 22 
2025-03-17 07:50:59.039261: Current learning rate: 0.008 
2025-03-17 07:51:05.465683: train_loss -0.902 
2025-03-17 07:51:05.472783: val_loss -0.8488 
2025-03-17 07:51:05.475912: Pseudo dice [np.float32(0.8986), np.float32(0.8807)] 
2025-03-17 07:51:05.479457: Epoch time: 6.44 s 
2025-03-17 07:51:05.482004: Yayy! New best EMA pseudo Dice: 0.8842999935150146 
2025-03-17 07:51:06.077054:  
2025-03-17 07:51:06.082228: Epoch 23 
2025-03-17 07:51:06.086172: Current learning rate: 0.0079 
2025-03-17 07:51:12.516135: train_loss -0.9022 
2025-03-17 07:51:12.521683: val_loss -0.8421 
2025-03-17 07:51:12.525695: Pseudo dice [np.float32(0.8955), np.float32(0.877)] 
2025-03-17 07:51:12.528246: Epoch time: 6.44 s 
2025-03-17 07:51:12.532272: Yayy! New best EMA pseudo Dice: 0.8845000267028809 
2025-03-17 07:51:13.111583:  
2025-03-17 07:51:13.115616: Epoch 24 
2025-03-17 07:51:13.119151: Current learning rate: 0.00781 
2025-03-17 07:51:19.564732: train_loss -0.9046 
2025-03-17 07:51:19.570298: val_loss -0.8434 
2025-03-17 07:51:19.573336: Pseudo dice [np.float32(0.8965), np.float32(0.8789)] 
2025-03-17 07:51:19.576872: Epoch time: 6.45 s 
2025-03-17 07:51:19.579901: Yayy! New best EMA pseudo Dice: 0.8848000168800354 
2025-03-17 07:51:20.172040:  
2025-03-17 07:51:20.177653: Epoch 25 
2025-03-17 07:51:20.181161: Current learning rate: 0.00772 
2025-03-17 07:51:26.605387: train_loss -0.9054 
2025-03-17 07:51:26.611426: val_loss -0.8422 
2025-03-17 07:51:26.614495: Pseudo dice [np.float32(0.8948), np.float32(0.8786)] 
2025-03-17 07:51:26.617531: Epoch time: 6.43 s 
2025-03-17 07:51:26.621071: Yayy! New best EMA pseudo Dice: 0.8849999904632568 
2025-03-17 07:51:27.218024:  
2025-03-17 07:51:27.223588: Epoch 26 
2025-03-17 07:51:27.227165: Current learning rate: 0.00763 
2025-03-17 07:51:33.655934: train_loss -0.9039 
2025-03-17 07:51:33.661953: val_loss -0.8432 
2025-03-17 07:51:33.665513: Pseudo dice [np.float32(0.895), np.float32(0.8768)] 
2025-03-17 07:51:33.668545: Epoch time: 6.44 s 
2025-03-17 07:51:33.672558: Yayy! New best EMA pseudo Dice: 0.8851000070571899 
2025-03-17 07:51:34.268853:  
2025-03-17 07:51:34.274977: Epoch 27 
2025-03-17 07:51:34.278553: Current learning rate: 0.00753 
2025-03-17 07:51:40.709213: train_loss -0.9074 
2025-03-17 07:51:40.715315: val_loss -0.8391 
2025-03-17 07:51:40.718367: Pseudo dice [np.float32(0.8948), np.float32(0.8743)] 
2025-03-17 07:51:40.722521: Epoch time: 6.44 s 
2025-03-17 07:51:41.289947:  
2025-03-17 07:51:41.293990: Epoch 28 
2025-03-17 07:51:41.296498: Current learning rate: 0.00744 
2025-03-17 07:51:47.732782: train_loss -0.906 
2025-03-17 07:51:47.738797: val_loss -0.8426 
2025-03-17 07:51:47.741808: Pseudo dice [np.float32(0.8946), np.float32(0.8786)] 
2025-03-17 07:51:47.744401: Epoch time: 6.44 s 
2025-03-17 07:51:47.748461: Yayy! New best EMA pseudo Dice: 0.885200023651123 
2025-03-17 07:51:48.499761:  
2025-03-17 07:51:48.505293: Epoch 29 
2025-03-17 07:51:48.507800: Current learning rate: 0.00735 
2025-03-17 07:51:54.924344: train_loss -0.9082 
2025-03-17 07:51:54.929919: val_loss -0.8414 
2025-03-17 07:51:54.933476: Pseudo dice [np.float32(0.8951), np.float32(0.8788)] 
2025-03-17 07:51:54.935799: Epoch time: 6.43 s 
2025-03-17 07:51:54.939932: Yayy! New best EMA pseudo Dice: 0.8853999972343445 
2025-03-17 07:51:55.548728:  
2025-03-17 07:51:55.554291: Epoch 30 
2025-03-17 07:51:55.558349: Current learning rate: 0.00725 
2025-03-17 07:52:01.984268: train_loss -0.9091 
2025-03-17 07:52:01.990378: val_loss -0.8383 
2025-03-17 07:52:01.994432: Pseudo dice [np.float32(0.8937), np.float32(0.877)] 
2025-03-17 07:52:01.997477: Epoch time: 6.44 s 
2025-03-17 07:52:02.572821:  
2025-03-17 07:52:02.578440: Epoch 31 
2025-03-17 07:52:02.583011: Current learning rate: 0.00716 
2025-03-17 07:52:09.011399: train_loss -0.9103 
2025-03-17 07:52:09.017513: val_loss -0.8481 
2025-03-17 07:52:09.021076: Pseudo dice [np.float32(0.9008), np.float32(0.8827)] 
2025-03-17 07:52:09.024614: Epoch time: 6.44 s 
2025-03-17 07:52:09.028177: Yayy! New best EMA pseudo Dice: 0.8859999775886536 
2025-03-17 07:52:09.691125:  
2025-03-17 07:52:09.696164: Epoch 32 
2025-03-17 07:52:09.698883: Current learning rate: 0.00707 
2025-03-17 07:52:16.133312: train_loss -0.9115 
2025-03-17 07:52:16.140733: val_loss -0.8426 
2025-03-17 07:52:16.143838: Pseudo dice [np.float32(0.8956), np.float32(0.8786)] 
2025-03-17 07:52:16.147363: Epoch time: 6.44 s 
2025-03-17 07:52:16.149885: Yayy! New best EMA pseudo Dice: 0.8860999941825867 
2025-03-17 07:52:16.766979:  
2025-03-17 07:52:16.771956: Epoch 33 
2025-03-17 07:52:16.776472: Current learning rate: 0.00697 
2025-03-17 07:52:23.207767: train_loss -0.9109 
2025-03-17 07:52:23.213914: val_loss -0.8399 
2025-03-17 07:52:23.216955: Pseudo dice [np.float32(0.8959), np.float32(0.8778)] 
2025-03-17 07:52:23.220503: Epoch time: 6.44 s 
2025-03-17 07:52:23.223025: Yayy! New best EMA pseudo Dice: 0.8862000107765198 
2025-03-17 07:52:23.833646:  
2025-03-17 07:52:23.839213: Epoch 34 
2025-03-17 07:52:23.842811: Current learning rate: 0.00688 
2025-03-17 07:52:30.304257: train_loss -0.914 
2025-03-17 07:52:30.309876: val_loss -0.8361 
2025-03-17 07:52:30.313400: Pseudo dice [np.float32(0.8932), np.float32(0.8751)] 
2025-03-17 07:52:30.316425: Epoch time: 6.47 s 
2025-03-17 07:52:30.894481:  
2025-03-17 07:52:30.900015: Epoch 35 
2025-03-17 07:52:30.903550: Current learning rate: 0.00679 
2025-03-17 07:52:37.354322: train_loss -0.9146 
2025-03-17 07:52:37.361896: val_loss -0.8377 
2025-03-17 07:52:37.364936: Pseudo dice [np.float32(0.8931), np.float32(0.8777)] 
2025-03-17 07:52:37.368477: Epoch time: 6.46 s 
2025-03-17 07:52:38.108535:  
2025-03-17 07:52:38.114711: Epoch 36 
2025-03-17 07:52:38.117782: Current learning rate: 0.00669 
2025-03-17 07:52:44.549582: train_loss -0.9145 
2025-03-17 07:52:44.555686: val_loss -0.839 
2025-03-17 07:52:44.558786: Pseudo dice [np.float32(0.8949), np.float32(0.8772)] 
2025-03-17 07:52:44.561875: Epoch time: 6.44 s 
2025-03-17 07:52:45.141448:  
2025-03-17 07:52:45.147034: Epoch 37 
2025-03-17 07:52:45.150630: Current learning rate: 0.0066 
2025-03-17 07:52:51.582680: train_loss -0.9143 
2025-03-17 07:52:51.587327: val_loss -0.8455 
2025-03-17 07:52:51.590397: Pseudo dice [np.float32(0.8992), np.float32(0.8818)] 
2025-03-17 07:52:51.594006: Epoch time: 6.44 s 
2025-03-17 07:52:51.597082: Yayy! New best EMA pseudo Dice: 0.8863999843597412 
2025-03-17 07:52:52.222954:  
2025-03-17 07:52:52.226969: Epoch 38 
2025-03-17 07:52:52.230478: Current learning rate: 0.0065 
2025-03-17 07:52:58.666289: train_loss -0.9129 
2025-03-17 07:52:58.671807: val_loss -0.8382 
2025-03-17 07:52:58.674358: Pseudo dice [np.float32(0.8953), np.float32(0.8761)] 
2025-03-17 07:52:58.678396: Epoch time: 6.44 s 
2025-03-17 07:52:59.265646:  
2025-03-17 07:52:59.268679: Epoch 39 
2025-03-17 07:52:59.273831: Current learning rate: 0.00641 
2025-03-17 07:53:05.740663: train_loss -0.9151 
2025-03-17 07:53:05.745270: val_loss -0.8346 
2025-03-17 07:53:05.748451: Pseudo dice [np.float32(0.8933), np.float32(0.8756)] 
2025-03-17 07:53:05.750967: Epoch time: 6.48 s 
2025-03-17 07:53:06.342191:  
2025-03-17 07:53:06.345229: Epoch 40 
2025-03-17 07:53:06.349780: Current learning rate: 0.00631 
2025-03-17 07:53:12.810259: train_loss -0.9143 
2025-03-17 07:53:12.813830: val_loss -0.8386 
2025-03-17 07:53:12.817898: Pseudo dice [np.float32(0.8951), np.float32(0.877)] 
2025-03-17 07:53:12.820945: Epoch time: 6.47 s 
2025-03-17 07:53:13.417318:  
2025-03-17 07:53:13.421854: Epoch 41 
2025-03-17 07:53:13.424922: Current learning rate: 0.00622 
2025-03-17 07:53:19.863428: train_loss -0.9174 
2025-03-17 07:53:19.868005: val_loss -0.8339 
2025-03-17 07:53:19.871066: Pseudo dice [np.float32(0.8918), np.float32(0.8751)] 
2025-03-17 07:53:19.874085: Epoch time: 6.45 s 
2025-03-17 07:53:20.434036:  
2025-03-17 07:53:20.439611: Epoch 42 
2025-03-17 07:53:20.443207: Current learning rate: 0.00612 
2025-03-17 07:53:26.895449: train_loss -0.9169 
2025-03-17 07:53:26.899964: val_loss -0.8357 
2025-03-17 07:53:26.903492: Pseudo dice [np.float32(0.8937), np.float32(0.8765)] 
2025-03-17 07:53:26.906524: Epoch time: 6.46 s 
2025-03-17 07:53:27.489298:  
2025-03-17 07:53:27.493317: Epoch 43 
2025-03-17 07:53:27.496833: Current learning rate: 0.00603 
2025-03-17 07:53:33.962140: train_loss -0.9189 
2025-03-17 07:53:33.966652: val_loss -0.8326 
2025-03-17 07:53:33.969710: Pseudo dice [np.float32(0.8924), np.float32(0.8753)] 
2025-03-17 07:53:33.974281: Epoch time: 6.47 s 
2025-03-17 07:53:34.709013:  
2025-03-17 07:53:34.714072: Epoch 44 
2025-03-17 07:53:34.717143: Current learning rate: 0.00593 
2025-03-17 07:53:41.162535: train_loss -0.919 
2025-03-17 07:53:41.167126: val_loss -0.8434 
2025-03-17 07:53:41.170644: Pseudo dice [np.float32(0.8985), np.float32(0.8827)] 
2025-03-17 07:53:41.174149: Epoch time: 6.45 s 
2025-03-17 07:53:41.776244:  
2025-03-17 07:53:41.780259: Epoch 45 
2025-03-17 07:53:41.783768: Current learning rate: 0.00584 
2025-03-17 07:53:48.232460: train_loss -0.9208 
2025-03-17 07:53:48.236613: val_loss -0.8407 
2025-03-17 07:53:48.240648: Pseudo dice [np.float32(0.8963), np.float32(0.8822)] 
2025-03-17 07:53:48.244191: Epoch time: 6.46 s 
2025-03-17 07:53:48.246213: Yayy! New best EMA pseudo Dice: 0.8863999843597412 
2025-03-17 07:53:48.845247:  
2025-03-17 07:53:48.849264: Epoch 46 
2025-03-17 07:53:48.852781: Current learning rate: 0.00574 
2025-03-17 07:53:55.305226: train_loss -0.92 
2025-03-17 07:53:55.309814: val_loss -0.832 
2025-03-17 07:53:55.312876: Pseudo dice [np.float32(0.8927), np.float32(0.8754)] 
2025-03-17 07:53:55.315442: Epoch time: 6.46 s 
2025-03-17 07:53:55.867866:  
2025-03-17 07:53:55.871917: Epoch 47 
2025-03-17 07:53:55.874460: Current learning rate: 0.00565 
2025-03-17 07:54:02.320325: train_loss -0.9205 
2025-03-17 07:54:02.324893: val_loss -0.8349 
2025-03-17 07:54:02.328468: Pseudo dice [np.float32(0.8942), np.float32(0.8772)] 
2025-03-17 07:54:02.332020: Epoch time: 6.45 s 
2025-03-17 07:54:02.886738:  
2025-03-17 07:54:02.890775: Epoch 48 
2025-03-17 07:54:02.894069: Current learning rate: 0.00555 
2025-03-17 07:54:09.341148: train_loss -0.9205 
2025-03-17 07:54:09.346241: val_loss -0.8342 
2025-03-17 07:54:09.349313: Pseudo dice [np.float32(0.8933), np.float32(0.8777)] 
2025-03-17 07:54:09.352916: Epoch time: 6.46 s 
2025-03-17 07:54:09.910202:  
2025-03-17 07:54:09.914246: Epoch 49 
2025-03-17 07:54:09.916775: Current learning rate: 0.00546 
2025-03-17 07:54:16.376182: train_loss -0.9231 
2025-03-17 07:54:16.380224: val_loss -0.8328 
2025-03-17 07:54:16.383292: Pseudo dice [np.float32(0.8922), np.float32(0.8763)] 
2025-03-17 07:54:16.386344: Epoch time: 6.47 s 
2025-03-17 07:54:16.982444:  
2025-03-17 07:54:16.985469: Epoch 50 
2025-03-17 07:54:16.989536: Current learning rate: 0.00536 
2025-03-17 07:54:23.444700: train_loss -0.9214 
2025-03-17 07:54:23.449284: val_loss -0.8411 
2025-03-17 07:54:23.452394: Pseudo dice [np.float32(0.8971), np.float32(0.8803)] 
2025-03-17 07:54:23.456000: Epoch time: 6.46 s 
2025-03-17 07:54:24.020030:  
2025-03-17 07:54:24.023055: Epoch 51 
2025-03-17 07:54:24.027650: Current learning rate: 0.00526 
2025-03-17 07:54:30.485917: train_loss -0.923 
2025-03-17 07:54:30.490603: val_loss -0.8379 
2025-03-17 07:54:30.493688: Pseudo dice [np.float32(0.8971), np.float32(0.8784)] 
2025-03-17 07:54:30.496735: Epoch time: 6.47 s 
2025-03-17 07:54:31.217645:  
2025-03-17 07:54:31.221659: Epoch 52 
2025-03-17 07:54:31.225674: Current learning rate: 0.00517 
2025-03-17 07:54:37.677044: train_loss -0.9219 
2025-03-17 07:54:37.684207: val_loss -0.8364 
2025-03-17 07:54:37.688248: Pseudo dice [np.float32(0.8949), np.float32(0.8775)] 
2025-03-17 07:54:37.691306: Epoch time: 6.46 s 
2025-03-17 07:54:38.386151:  
2025-03-17 07:54:38.391164: Epoch 53 
2025-03-17 07:54:38.394675: Current learning rate: 0.00507 
2025-03-17 07:54:44.843625: train_loss -0.9213 
2025-03-17 07:54:44.849185: val_loss -0.839 
2025-03-17 07:54:44.853225: Pseudo dice [np.float32(0.8972), np.float32(0.879)] 
2025-03-17 07:54:44.856270: Epoch time: 6.46 s 
2025-03-17 07:54:44.859314: Yayy! New best EMA pseudo Dice: 0.8865000009536743 
2025-03-17 07:54:45.464327:  
2025-03-17 07:54:45.470363: Epoch 54 
2025-03-17 07:54:45.473054: Current learning rate: 0.00497 
2025-03-17 07:54:51.911858: train_loss -0.9224 
2025-03-17 07:54:51.917876: val_loss -0.8351 
2025-03-17 07:54:51.920909: Pseudo dice [np.float32(0.8942), np.float32(0.8789)] 
2025-03-17 07:54:51.924435: Epoch time: 6.45 s 
2025-03-17 07:54:51.928059: Yayy! New best EMA pseudo Dice: 0.8865000009536743 
2025-03-17 07:54:52.548169:  
2025-03-17 07:54:52.553733: Epoch 55 
2025-03-17 07:54:52.556791: Current learning rate: 0.00487 
2025-03-17 07:54:58.978285: train_loss -0.923 
2025-03-17 07:54:58.983880: val_loss -0.8286 
2025-03-17 07:54:58.987443: Pseudo dice [np.float32(0.8925), np.float32(0.8752)] 
2025-03-17 07:54:58.991051: Epoch time: 6.43 s 
2025-03-17 07:54:59.572760:  
2025-03-17 07:54:59.578798: Epoch 56 
2025-03-17 07:54:59.581845: Current learning rate: 0.00478 
2025-03-17 07:55:06.008917: train_loss -0.9233 
2025-03-17 07:55:06.014467: val_loss -0.839 
2025-03-17 07:55:06.018028: Pseudo dice [np.float32(0.8967), np.float32(0.8813)] 
2025-03-17 07:55:06.021059: Epoch time: 6.44 s 
2025-03-17 07:55:06.023582: Yayy! New best EMA pseudo Dice: 0.8865000009536743 
2025-03-17 07:55:06.632613:  
2025-03-17 07:55:06.638127: Epoch 57 
2025-03-17 07:55:06.641638: Current learning rate: 0.00468 
2025-03-17 07:55:13.065349: train_loss -0.9248 
2025-03-17 07:55:13.071473: val_loss -0.8297 
2025-03-17 07:55:13.075003: Pseudo dice [np.float32(0.8918), np.float32(0.874)] 
2025-03-17 07:55:13.078085: Epoch time: 6.43 s 
2025-03-17 07:55:13.696886:  
2025-03-17 07:55:13.703013: Epoch 58 
2025-03-17 07:55:13.706088: Current learning rate: 0.00458 
2025-03-17 07:55:20.140592: train_loss -0.9252 
2025-03-17 07:55:20.146658: val_loss -0.8322 
2025-03-17 07:55:20.149171: Pseudo dice [np.float32(0.894), np.float32(0.8776)] 
2025-03-17 07:55:20.152688: Epoch time: 6.44 s 
2025-03-17 07:55:20.740738:  
2025-03-17 07:55:20.746718: Epoch 59 
2025-03-17 07:55:20.750731: Current learning rate: 0.00448 
2025-03-17 07:55:27.185663: train_loss -0.9254 
2025-03-17 07:55:27.191230: val_loss -0.8407 
2025-03-17 07:55:27.195333: Pseudo dice [np.float32(0.8985), np.float32(0.8816)] 
2025-03-17 07:55:27.198420: Epoch time: 6.45 s 
2025-03-17 07:55:27.786978:  
2025-03-17 07:55:27.792530: Epoch 60 
2025-03-17 07:55:27.795059: Current learning rate: 0.00438 
2025-03-17 07:55:34.216111: train_loss -0.9241 
2025-03-17 07:55:34.222663: val_loss -0.8341 
2025-03-17 07:55:34.225796: Pseudo dice [np.float32(0.8939), np.float32(0.8784)] 
2025-03-17 07:55:34.228867: Epoch time: 6.43 s 
2025-03-17 07:55:34.964683:  
2025-03-17 07:55:34.970798: Epoch 61 
2025-03-17 07:55:34.973860: Current learning rate: 0.00429 
2025-03-17 07:55:41.400566: train_loss -0.9264 
2025-03-17 07:55:41.405639: val_loss -0.8352 
2025-03-17 07:55:41.409693: Pseudo dice [np.float32(0.8952), np.float32(0.8784)] 
2025-03-17 07:55:41.413248: Epoch time: 6.44 s 
2025-03-17 07:55:41.992069:  
2025-03-17 07:55:41.998652: Epoch 62 
2025-03-17 07:55:42.001197: Current learning rate: 0.00419 
2025-03-17 07:55:48.424237: train_loss -0.9254 
2025-03-17 07:55:48.430256: val_loss -0.8371 
2025-03-17 07:55:48.432787: Pseudo dice [np.float32(0.8953), np.float32(0.8805)] 
2025-03-17 07:55:48.436101: Epoch time: 6.43 s 
2025-03-17 07:55:48.440190: Yayy! New best EMA pseudo Dice: 0.8866999745368958 
2025-03-17 07:55:49.057693:  
2025-03-17 07:55:49.063206: Epoch 63 
2025-03-17 07:55:49.066719: Current learning rate: 0.00409 
2025-03-17 07:55:55.491529: train_loss -0.9276 
2025-03-17 07:55:55.497111: val_loss -0.8306 
2025-03-17 07:55:55.500178: Pseudo dice [np.float32(0.8934), np.float32(0.8771)] 
2025-03-17 07:55:55.503778: Epoch time: 6.43 s 
2025-03-17 07:55:56.080106:  
2025-03-17 07:55:56.085677: Epoch 64 
2025-03-17 07:55:56.089812: Current learning rate: 0.00399 
2025-03-17 07:56:02.522302: train_loss -0.9267 
2025-03-17 07:56:02.528871: val_loss -0.831 
2025-03-17 07:56:02.531927: Pseudo dice [np.float32(0.8938), np.float32(0.8775)] 
2025-03-17 07:56:02.534980: Epoch time: 6.44 s 
2025-03-17 07:56:03.118245:  
2025-03-17 07:56:03.123265: Epoch 65 
2025-03-17 07:56:03.126782: Current learning rate: 0.00389 
2025-03-17 07:56:09.561006: train_loss -0.9282 
2025-03-17 07:56:09.567168: val_loss -0.831 
2025-03-17 07:56:09.570267: Pseudo dice [np.float32(0.894), np.float32(0.8777)] 
2025-03-17 07:56:09.573722: Epoch time: 6.44 s 
2025-03-17 07:56:10.144193:  
2025-03-17 07:56:10.149714: Epoch 66 
2025-03-17 07:56:10.152221: Current learning rate: 0.00379 
2025-03-17 07:56:16.594465: train_loss -0.9273 
2025-03-17 07:56:16.600036: val_loss -0.834 
2025-03-17 07:56:16.604127: Pseudo dice [np.float32(0.8939), np.float32(0.8787)] 
2025-03-17 07:56:16.607285: Epoch time: 6.45 s 
2025-03-17 07:56:17.184074:  
2025-03-17 07:56:17.189589: Epoch 67 
2025-03-17 07:56:17.194098: Current learning rate: 0.00369 
2025-03-17 07:56:23.618766: train_loss -0.9292 
2025-03-17 07:56:23.624953: val_loss -0.8254 
2025-03-17 07:56:23.627984: Pseudo dice [np.float32(0.8904), np.float32(0.8736)] 
2025-03-17 07:56:23.630668: Epoch time: 6.44 s 
2025-03-17 07:56:24.219461:  
2025-03-17 07:56:24.225038: Epoch 68 
2025-03-17 07:56:24.228552: Current learning rate: 0.00359 
2025-03-17 07:56:30.665314: train_loss -0.9284 
2025-03-17 07:56:30.672936: val_loss -0.8275 
2025-03-17 07:56:30.675482: Pseudo dice [np.float32(0.8912), np.float32(0.8753)] 
2025-03-17 07:56:30.678041: Epoch time: 6.45 s 
2025-03-17 07:56:31.427302:  
2025-03-17 07:56:31.432898: Epoch 69 
2025-03-17 07:56:31.435461: Current learning rate: 0.00349 
2025-03-17 07:56:37.871365: train_loss -0.9282 
2025-03-17 07:56:37.878447: val_loss -0.8264 
2025-03-17 07:56:37.881962: Pseudo dice [np.float32(0.8912), np.float32(0.8731)] 
2025-03-17 07:56:37.885466: Epoch time: 6.44 s 
2025-03-17 07:56:38.472983:  
2025-03-17 07:56:38.478779: Epoch 70 
2025-03-17 07:56:38.482289: Current learning rate: 0.00338 
2025-03-17 07:56:44.920408: train_loss -0.9298 
2025-03-17 07:56:44.926297: val_loss -0.8307 
2025-03-17 07:56:44.928853: Pseudo dice [np.float32(0.8937), np.float32(0.8787)] 
2025-03-17 07:56:44.932361: Epoch time: 6.45 s 
2025-03-17 07:56:45.565555:  
2025-03-17 07:56:45.571609: Epoch 71 
2025-03-17 07:56:45.574668: Current learning rate: 0.00328 
2025-03-17 07:56:51.998563: train_loss -0.9294 
2025-03-17 07:56:52.005125: val_loss -0.8304 
2025-03-17 07:56:52.007663: Pseudo dice [np.float32(0.894), np.float32(0.8774)] 
2025-03-17 07:56:52.011204: Epoch time: 6.43 s 
2025-03-17 07:56:52.602031:  
2025-03-17 07:56:52.607596: Epoch 72 
2025-03-17 07:56:52.610126: Current learning rate: 0.00318 
2025-03-17 07:56:59.043556: train_loss -0.9289 
2025-03-17 07:56:59.048684: val_loss -0.8323 
2025-03-17 07:56:59.052729: Pseudo dice [np.float32(0.8945), np.float32(0.8779)] 
2025-03-17 07:56:59.055779: Epoch time: 6.44 s 
2025-03-17 07:56:59.643632:  
2025-03-17 07:56:59.649152: Epoch 73 
2025-03-17 07:56:59.652661: Current learning rate: 0.00308 
2025-03-17 07:57:06.092037: train_loss -0.9307 
2025-03-17 07:57:06.098143: val_loss -0.8294 
2025-03-17 07:57:06.101700: Pseudo dice [np.float32(0.8938), np.float32(0.8768)] 
2025-03-17 07:57:06.104242: Epoch time: 6.45 s 
2025-03-17 07:57:06.702158:  
2025-03-17 07:57:06.707691: Epoch 74 
2025-03-17 07:57:06.711246: Current learning rate: 0.00297 
2025-03-17 07:57:13.146872: train_loss -0.9314 
2025-03-17 07:57:13.153418: val_loss -0.8305 
2025-03-17 07:57:13.156976: Pseudo dice [np.float32(0.8931), np.float32(0.8768)] 
2025-03-17 07:57:13.159533: Epoch time: 6.45 s 
2025-03-17 07:57:13.754927:  
2025-03-17 07:57:13.759977: Epoch 75 
2025-03-17 07:57:13.763526: Current learning rate: 0.00287 
2025-03-17 07:57:20.198288: train_loss -0.9301 
2025-03-17 07:57:20.203856: val_loss -0.8316 
2025-03-17 07:57:20.207369: Pseudo dice [np.float32(0.8945), np.float32(0.8773)] 
2025-03-17 07:57:20.211384: Epoch time: 6.44 s 
2025-03-17 07:57:20.949159:  
2025-03-17 07:57:20.955328: Epoch 76 
2025-03-17 07:57:20.957347: Current learning rate: 0.00277 
2025-03-17 07:57:27.375374: train_loss -0.9316 
2025-03-17 07:57:27.382127: val_loss -0.8269 
2025-03-17 07:57:27.385653: Pseudo dice [np.float32(0.8919), np.float32(0.8757)] 
2025-03-17 07:57:27.388756: Epoch time: 6.43 s 
2025-03-17 07:57:27.982150:  
2025-03-17 07:57:27.987163: Epoch 77 
2025-03-17 07:57:27.990676: Current learning rate: 0.00266 
2025-03-17 07:57:34.421203: train_loss -0.9312 
2025-03-17 07:57:34.427431: val_loss -0.829 
2025-03-17 07:57:34.430983: Pseudo dice [np.float32(0.8929), np.float32(0.8779)] 
2025-03-17 07:57:34.434511: Epoch time: 6.44 s 
2025-03-17 07:57:35.046139:  
2025-03-17 07:57:35.051366: Epoch 78 
2025-03-17 07:57:35.055880: Current learning rate: 0.00256 
2025-03-17 07:57:41.589767: train_loss -0.9312 
2025-03-17 07:57:41.596342: val_loss -0.8309 
2025-03-17 07:57:41.599681: Pseudo dice [np.float32(0.8947), np.float32(0.8788)] 
2025-03-17 07:57:41.603192: Epoch time: 6.54 s 
2025-03-17 07:57:42.200425:  
2025-03-17 07:57:42.205939: Epoch 79 
2025-03-17 07:57:42.209447: Current learning rate: 0.00245 
2025-03-17 07:57:48.836014: train_loss -0.9322 
2025-03-17 07:57:48.841027: val_loss -0.8284 
2025-03-17 07:57:48.845083: Pseudo dice [np.float32(0.8931), np.float32(0.8781)] 
2025-03-17 07:57:48.847631: Epoch time: 6.64 s 
2025-03-17 07:57:49.441526:  
2025-03-17 07:57:49.447091: Epoch 80 
2025-03-17 07:57:49.450685: Current learning rate: 0.00235 
2025-03-17 07:57:55.875637: train_loss -0.9313 
2025-03-17 07:57:55.881711: val_loss -0.8262 
2025-03-17 07:57:55.885306: Pseudo dice [np.float32(0.8916), np.float32(0.8751)] 
2025-03-17 07:57:55.888872: Epoch time: 6.43 s 
2025-03-17 07:57:56.517077:  
2025-03-17 07:57:56.523193: Epoch 81 
2025-03-17 07:57:56.526701: Current learning rate: 0.00224 
2025-03-17 07:58:02.949450: train_loss -0.9323 
2025-03-17 07:58:02.955489: val_loss -0.8329 
2025-03-17 07:58:02.958541: Pseudo dice [np.float32(0.8955), np.float32(0.8794)] 
2025-03-17 07:58:02.962117: Epoch time: 6.43 s 
2025-03-17 07:58:03.556607:  
2025-03-17 07:58:03.561620: Epoch 82 
2025-03-17 07:58:03.565132: Current learning rate: 0.00214 
2025-03-17 07:58:09.982085: train_loss -0.9326 
2025-03-17 07:58:09.988274: val_loss -0.8343 
2025-03-17 07:58:09.992316: Pseudo dice [np.float32(0.8963), np.float32(0.8798)] 
2025-03-17 07:58:09.995377: Epoch time: 6.43 s 
2025-03-17 07:58:10.569648:  
2025-03-17 07:58:10.574463: Epoch 83 
2025-03-17 07:58:10.577481: Current learning rate: 0.00203 
2025-03-17 07:58:17.014113: train_loss -0.9317 
2025-03-17 07:58:17.019181: val_loss -0.833 
2025-03-17 07:58:17.025284: Pseudo dice [np.float32(0.8954), np.float32(0.8797)] 
2025-03-17 07:58:17.028301: Epoch time: 6.45 s 
2025-03-17 07:58:17.617825:  
2025-03-17 07:58:17.623431: Epoch 84 
2025-03-17 07:58:17.627495: Current learning rate: 0.00192 
2025-03-17 07:58:24.243002: train_loss -0.9329 
2025-03-17 07:58:24.249057: val_loss -0.8335 
2025-03-17 07:58:24.252088: Pseudo dice [np.float32(0.8947), np.float32(0.879)] 
2025-03-17 07:58:24.255124: Epoch time: 6.63 s 
2025-03-17 07:58:24.819499:  
2025-03-17 07:58:24.825561: Epoch 85 
2025-03-17 07:58:24.828654: Current learning rate: 0.00181 
2025-03-17 07:58:31.268833: train_loss -0.9342 
2025-03-17 07:58:31.274406: val_loss -0.8296 
2025-03-17 07:58:31.277939: Pseudo dice [np.float32(0.8942), np.float32(0.8766)] 
2025-03-17 07:58:31.280979: Epoch time: 6.45 s 
2025-03-17 07:58:31.833104:  
2025-03-17 07:58:31.839182: Epoch 86 
2025-03-17 07:58:31.842272: Current learning rate: 0.0017 
2025-03-17 07:58:38.269772: train_loss -0.9337 
2025-03-17 07:58:38.278403: val_loss -0.8333 
2025-03-17 07:58:38.281003: Pseudo dice [np.float32(0.896), np.float32(0.88)] 
2025-03-17 07:58:38.285077: Epoch time: 6.44 s 
2025-03-17 07:58:38.845929:  
2025-03-17 07:58:38.851460: Epoch 87 
2025-03-17 07:58:38.855010: Current learning rate: 0.00159 
2025-03-17 07:58:45.289725: train_loss -0.934 
2025-03-17 07:58:45.296306: val_loss -0.8333 
2025-03-17 07:58:45.299452: Pseudo dice [np.float32(0.8968), np.float32(0.88)] 
2025-03-17 07:58:45.302983: Epoch time: 6.44 s 
2025-03-17 07:58:45.858096:  
2025-03-17 07:58:45.863657: Epoch 88 
2025-03-17 07:58:45.866203: Current learning rate: 0.00148 
2025-03-17 07:58:52.292053: train_loss -0.9345 
2025-03-17 07:58:52.298614: val_loss -0.8293 
2025-03-17 07:58:52.302154: Pseudo dice [np.float32(0.8937), np.float32(0.878)] 
2025-03-17 07:58:52.305179: Epoch time: 6.43 s 
2025-03-17 07:58:52.875403:  
2025-03-17 07:58:52.880922: Epoch 89 
2025-03-17 07:58:52.883429: Current learning rate: 0.00137 
2025-03-17 07:58:59.319301: train_loss -0.936 
2025-03-17 07:58:59.322925: val_loss -0.8251 
2025-03-17 07:58:59.326982: Pseudo dice [np.float32(0.8919), np.float32(0.8747)] 
2025-03-17 07:58:59.330035: Epoch time: 6.44 s 
2025-03-17 07:58:59.880270:  
2025-03-17 07:58:59.886857: Epoch 90 
2025-03-17 07:58:59.890482: Current learning rate: 0.00126 
2025-03-17 07:59:06.321201: train_loss -0.9345 
2025-03-17 07:59:06.327428: val_loss -0.8293 
2025-03-17 07:59:06.331457: Pseudo dice [np.float32(0.8941), np.float32(0.8786)] 
2025-03-17 07:59:06.333980: Epoch time: 6.44 s 
2025-03-17 07:59:06.891387:  
2025-03-17 07:59:06.898026: Epoch 91 
2025-03-17 07:59:06.900571: Current learning rate: 0.00115 
2025-03-17 07:59:13.348898: train_loss -0.9334 
2025-03-17 07:59:13.353926: val_loss -0.8297 
2025-03-17 07:59:13.357061: Pseudo dice [np.float32(0.8943), np.float32(0.8785)] 
2025-03-17 07:59:13.361093: Epoch time: 6.46 s 
2025-03-17 07:59:13.919039:  
2025-03-17 07:59:13.924051: Epoch 92 
2025-03-17 07:59:13.927565: Current learning rate: 0.00103 
2025-03-17 07:59:20.362044: train_loss -0.9343 
2025-03-17 07:59:20.367618: val_loss -0.8295 
2025-03-17 07:59:20.371153: Pseudo dice [np.float32(0.8944), np.float32(0.8777)] 
2025-03-17 07:59:20.374708: Epoch time: 6.44 s 
2025-03-17 07:59:21.090384:  
2025-03-17 07:59:21.095962: Epoch 93 
2025-03-17 07:59:21.099030: Current learning rate: 0.00091 
2025-03-17 07:59:27.530791: train_loss -0.9349 
2025-03-17 07:59:27.536887: val_loss -0.8264 
2025-03-17 07:59:27.540424: Pseudo dice [np.float32(0.8928), np.float32(0.8757)] 
2025-03-17 07:59:27.543979: Epoch time: 6.44 s 
2025-03-17 07:59:28.104023:  
2025-03-17 07:59:28.109117: Epoch 94 
2025-03-17 07:59:28.112652: Current learning rate: 0.00079 
2025-03-17 07:59:34.551901: train_loss -0.9346 
2025-03-17 07:59:34.557989: val_loss -0.8284 
2025-03-17 07:59:34.561572: Pseudo dice [np.float32(0.8933), np.float32(0.8766)] 
2025-03-17 07:59:34.565152: Epoch time: 6.45 s 
2025-03-17 07:59:35.126444:  
2025-03-17 07:59:35.132016: Epoch 95 
2025-03-17 07:59:35.134565: Current learning rate: 0.00067 
2025-03-17 07:59:41.561409: train_loss -0.9361 
2025-03-17 07:59:41.568045: val_loss -0.8315 
2025-03-17 07:59:41.572133: Pseudo dice [np.float32(0.8957), np.float32(0.8795)] 
2025-03-17 07:59:41.575227: Epoch time: 6.43 s 
2025-03-17 07:59:42.131705:  
2025-03-17 07:59:42.137271: Epoch 96 
2025-03-17 07:59:42.141338: Current learning rate: 0.00055 
2025-03-17 07:59:48.569513: train_loss -0.9352 
2025-03-17 07:59:48.576032: val_loss -0.8304 
2025-03-17 07:59:48.579587: Pseudo dice [np.float32(0.8946), np.float32(0.8786)] 
2025-03-17 07:59:48.583105: Epoch time: 6.44 s 
2025-03-17 07:59:49.181449:  
2025-03-17 07:59:49.186461: Epoch 97 
2025-03-17 07:59:49.189979: Current learning rate: 0.00043 
2025-03-17 07:59:55.608512: train_loss -0.935 
2025-03-17 07:59:55.614622: val_loss -0.8303 
2025-03-17 07:59:55.617704: Pseudo dice [np.float32(0.8947), np.float32(0.8779)] 
2025-03-17 07:59:55.621241: Epoch time: 6.43 s 
2025-03-17 07:59:56.186035:  
2025-03-17 07:59:56.192094: Epoch 98 
2025-03-17 07:59:56.195217: Current learning rate: 0.0003 
2025-03-17 08:00:02.636910: train_loss -0.9366 
2025-03-17 08:00:02.642493: val_loss -0.8272 
2025-03-17 08:00:02.646614: Pseudo dice [np.float32(0.8929), np.float32(0.8765)] 
2025-03-17 08:00:02.649685: Epoch time: 6.45 s 
2025-03-17 08:00:03.275404:  
2025-03-17 08:00:03.280545: Epoch 99 
2025-03-17 08:00:03.283096: Current learning rate: 0.00016 
2025-03-17 08:00:09.723993: train_loss -0.9361 
2025-03-17 08:00:09.730077: val_loss -0.8309 
2025-03-17 08:00:09.733665: Pseudo dice [np.float32(0.8965), np.float32(0.8792)] 
2025-03-17 08:00:09.736706: Epoch time: 6.45 s 
2025-03-17 08:00:10.353967: Training done. 
2025-03-17 08:00:10.389966: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-17 08:00:10.396968: The split file contains 5 splits. 
2025-03-17 08:00:10.403972: Desired fold for training: 0 
2025-03-17 08:00:10.408971: This split has 208 training and 52 validation cases. 
2025-03-17 08:00:10.412973: predicting hippocampus_017 
2025-03-17 08:00:10.418471: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-17 08:00:10.511470: predicting hippocampus_019 
2025-03-17 08:00:10.517985: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-17 08:00:10.554983: predicting hippocampus_033 
2025-03-17 08:00:10.560984: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-17 08:00:10.583982: predicting hippocampus_035 
2025-03-17 08:00:10.590984: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-17 08:00:10.613494: predicting hippocampus_037 
2025-03-17 08:00:10.620501: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-17 08:00:10.644502: predicting hippocampus_049 
2025-03-17 08:00:10.650502: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-17 08:00:10.673499: predicting hippocampus_052 
2025-03-17 08:00:10.680502: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-17 08:00:10.703507: predicting hippocampus_065 
2025-03-17 08:00:10.710509: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-17 08:00:10.735017: predicting hippocampus_083 
2025-03-17 08:00:10.742015: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-17 08:00:10.767016: predicting hippocampus_088 
2025-03-17 08:00:10.773015: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-17 08:00:14.114739: predicting hippocampus_090 
2025-03-17 08:00:14.122249: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-17 08:00:14.166251: predicting hippocampus_092 
2025-03-17 08:00:14.174249: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-17 08:00:14.223766: predicting hippocampus_095 
2025-03-17 08:00:14.230766: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-17 08:00:14.279766: predicting hippocampus_107 
2025-03-17 08:00:14.287764: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-17 08:00:14.336283: predicting hippocampus_108 
2025-03-17 08:00:14.343282: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-17 08:00:14.394284: predicting hippocampus_123 
2025-03-17 08:00:14.402289: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-17 08:00:14.444793: predicting hippocampus_125 
2025-03-17 08:00:14.451793: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-17 08:00:14.515297: predicting hippocampus_157 
2025-03-17 08:00:14.531300: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-17 08:00:14.581301: predicting hippocampus_164 
2025-03-17 08:00:14.593301: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-17 08:00:14.681813: predicting hippocampus_169 
2025-03-17 08:00:14.688815: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-17 08:00:14.722322: predicting hippocampus_175 
2025-03-17 08:00:14.728324: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-17 08:00:14.756321: predicting hippocampus_185 
2025-03-17 08:00:14.761322: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-17 08:00:14.793321: predicting hippocampus_190 
2025-03-17 08:00:14.799324: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-17 08:00:14.826832: predicting hippocampus_194 
2025-03-17 08:00:14.832834: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-17 08:00:14.860832: predicting hippocampus_204 
2025-03-17 08:00:14.872832: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-17 08:00:14.898832: predicting hippocampus_205 
2025-03-17 08:00:14.905834: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-17 08:00:14.936343: predicting hippocampus_210 
2025-03-17 08:00:14.941343: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-17 08:00:14.974343: predicting hippocampus_217 
2025-03-17 08:00:14.979345: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-17 08:00:15.008345: predicting hippocampus_219 
2025-03-17 08:00:15.015853: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-17 08:00:15.043854: predicting hippocampus_229 
2025-03-17 08:00:15.049853: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-17 08:00:15.080853: predicting hippocampus_244 
2025-03-17 08:00:15.085852: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-17 08:00:15.116359: predicting hippocampus_261 
2025-03-17 08:00:15.122362: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-17 08:00:15.166359: predicting hippocampus_264 
2025-03-17 08:00:15.172361: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-17 08:00:15.200362: predicting hippocampus_277 
2025-03-17 08:00:15.207365: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-17 08:00:15.250867: predicting hippocampus_280 
2025-03-17 08:00:15.259868: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-17 08:00:15.290868: predicting hippocampus_286 
2025-03-17 08:00:15.296869: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-17 08:00:15.343375: predicting hippocampus_288 
2025-03-17 08:00:15.350377: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-17 08:00:15.397376: predicting hippocampus_289 
2025-03-17 08:00:15.405378: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-17 08:00:15.431883: predicting hippocampus_296 
2025-03-17 08:00:15.437883: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-17 08:00:15.465884: predicting hippocampus_305 
2025-03-17 08:00:15.472883: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-17 08:00:15.503886: predicting hippocampus_308 
2025-03-17 08:00:15.511887: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-17 08:00:15.542393: predicting hippocampus_317 
2025-03-17 08:00:15.549393: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-17 08:00:15.579393: predicting hippocampus_327 
2025-03-17 08:00:15.586393: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-17 08:00:15.613395: predicting hippocampus_330 
2025-03-17 08:00:15.618903: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-17 08:00:15.645902: predicting hippocampus_332 
2025-03-17 08:00:15.650903: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-17 08:00:15.678901: predicting hippocampus_338 
2025-03-17 08:00:15.684902: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-17 08:00:15.733409: predicting hippocampus_349 
2025-03-17 08:00:15.738411: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-17 08:00:15.767411: predicting hippocampus_350 
2025-03-17 08:00:15.773412: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-17 08:00:15.800415: predicting hippocampus_356 
2025-03-17 08:00:15.806414: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-17 08:00:15.833923: predicting hippocampus_358 
2025-03-17 08:00:15.838924: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-17 08:00:15.865922: predicting hippocampus_374 
2025-03-17 08:00:15.871924: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-17 08:00:15.897922: predicting hippocampus_394 
2025-03-17 08:00:15.903925: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-17 08:00:19.444709: Validation complete 
2025-03-17 08:00:19.450709: Mean Validation Dice:  0.3850589378079776 
