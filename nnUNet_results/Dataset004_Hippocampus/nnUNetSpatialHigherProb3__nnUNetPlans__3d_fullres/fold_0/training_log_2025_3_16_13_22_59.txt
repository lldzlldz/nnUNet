
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-16 13:22:59.621922: do_dummy_2d_data_aug: False 
2025-03-16 13:22:59.638059: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-16 13:22:59.645913: The split file contains 5 splits. 
2025-03-16 13:22:59.648912: Desired fold for training: 0 
2025-03-16 13:22:59.651912: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-16 13:23:05.970678: unpacking dataset... 
2025-03-16 13:23:06.305745: unpacking done... 
2025-03-16 13:23:07.484284:  
2025-03-16 13:23:07.489324: Epoch 0 
2025-03-16 13:23:07.492367: Current learning rate: 0.01 
2025-03-16 13:23:14.811072: train_loss -0.2723 
2025-03-16 13:23:14.816019: val_loss -0.5308 
2025-03-16 13:23:14.820315: Pseudo dice [np.float32(0.5765), np.float32(0.7042)] 
2025-03-16 13:23:14.822815: Epoch time: 7.33 s 
2025-03-16 13:23:14.826823: Yayy! New best EMA pseudo Dice: 0.6402999758720398 
2025-03-16 13:23:15.328990:  
2025-03-16 13:23:15.335052: Epoch 1 
2025-03-16 13:23:15.338129: Current learning rate: 0.00991 
2025-03-16 13:23:21.796720: train_loss -0.6274 
2025-03-16 13:23:21.802869: val_loss -0.7619 
2025-03-16 13:23:21.805763: Pseudo dice [np.float32(0.8237), np.float32(0.8544)] 
2025-03-16 13:23:21.809793: Epoch time: 6.47 s 
2025-03-16 13:23:21.812816: Yayy! New best EMA pseudo Dice: 0.6601999998092651 
2025-03-16 13:23:22.365823:  
2025-03-16 13:23:22.371365: Epoch 2 
2025-03-16 13:23:22.375444: Current learning rate: 0.00982 
2025-03-16 13:23:28.851398: train_loss -0.8113 
2025-03-16 13:23:28.856921: val_loss -0.8367 
2025-03-16 13:23:28.860433: Pseudo dice [np.float32(0.8875), np.float32(0.8703)] 
2025-03-16 13:23:28.864486: Epoch time: 6.49 s 
2025-03-16 13:23:28.867025: Yayy! New best EMA pseudo Dice: 0.6820999979972839 
2025-03-16 13:23:29.448027:  
2025-03-16 13:23:29.453558: Epoch 3 
2025-03-16 13:23:29.456623: Current learning rate: 0.00973 
2025-03-16 13:23:35.908930: train_loss -0.8441 
2025-03-16 13:23:35.914518: val_loss -0.8341 
2025-03-16 13:23:35.919607: Pseudo dice [np.float32(0.8849), np.float32(0.8645)] 
2025-03-16 13:23:35.922643: Epoch time: 6.46 s 
2025-03-16 13:23:35.925191: Yayy! New best EMA pseudo Dice: 0.7013000249862671 
2025-03-16 13:23:36.491879:  
2025-03-16 13:23:36.496959: Epoch 4 
2025-03-16 13:23:36.500546: Current learning rate: 0.00964 
2025-03-16 13:23:42.988928: train_loss -0.8513 
2025-03-16 13:23:42.994449: val_loss -0.8422 
2025-03-16 13:23:42.997957: Pseudo dice [np.float32(0.8927), np.float32(0.8713)] 
2025-03-16 13:23:43.001976: Epoch time: 6.5 s 
2025-03-16 13:23:43.005491: Yayy! New best EMA pseudo Dice: 0.7193999886512756 
2025-03-16 13:23:43.703935:  
2025-03-16 13:23:43.709974: Epoch 5 
2025-03-16 13:23:43.713004: Current learning rate: 0.00955 
2025-03-16 13:23:50.173274: train_loss -0.8573 
2025-03-16 13:23:50.179878: val_loss -0.8453 
2025-03-16 13:23:50.183390: Pseudo dice [np.float32(0.8954), np.float32(0.8742)] 
2025-03-16 13:23:50.186431: Epoch time: 6.47 s 
2025-03-16 13:23:50.189963: Yayy! New best EMA pseudo Dice: 0.7358999848365784 
2025-03-16 13:23:50.740935:  
2025-03-16 13:23:50.746461: Epoch 6 
2025-03-16 13:23:50.750011: Current learning rate: 0.00946 
2025-03-16 13:23:57.191722: train_loss -0.8645 
2025-03-16 13:23:57.197389: val_loss -0.8534 
2025-03-16 13:23:57.201475: Pseudo dice [np.float32(0.8998), np.float32(0.881)] 
2025-03-16 13:23:57.204520: Epoch time: 6.45 s 
2025-03-16 13:23:57.207604: Yayy! New best EMA pseudo Dice: 0.7513999938964844 
2025-03-16 13:23:57.784283:  
2025-03-16 13:23:57.789825: Epoch 7 
2025-03-16 13:23:57.793387: Current learning rate: 0.00937 
2025-03-16 13:24:04.249335: train_loss -0.8684 
2025-03-16 13:24:04.254942: val_loss -0.8507 
2025-03-16 13:24:04.258487: Pseudo dice [np.float32(0.8982), np.float32(0.8803)] 
2025-03-16 13:24:04.262322: Epoch time: 6.47 s 
2025-03-16 13:24:04.265948: Yayy! New best EMA pseudo Dice: 0.7652000188827515 
2025-03-16 13:24:04.841037:  
2025-03-16 13:24:04.846096: Epoch 8 
2025-03-16 13:24:04.850138: Current learning rate: 0.00928 
2025-03-16 13:24:11.313560: train_loss -0.8741 
2025-03-16 13:24:11.319645: val_loss -0.8507 
2025-03-16 13:24:11.321836: Pseudo dice [np.float32(0.896), np.float32(0.8797)] 
2025-03-16 13:24:11.328469: Epoch time: 6.47 s 
2025-03-16 13:24:11.332033: Yayy! New best EMA pseudo Dice: 0.777400016784668 
2025-03-16 13:24:11.917061:  
2025-03-16 13:24:11.922610: Epoch 9 
2025-03-16 13:24:11.925140: Current learning rate: 0.00919 
2025-03-16 13:24:18.394856: train_loss -0.8756 
2025-03-16 13:24:18.401428: val_loss -0.8498 
2025-03-16 13:24:18.404997: Pseudo dice [np.float32(0.8969), np.float32(0.8797)] 
2025-03-16 13:24:18.407515: Epoch time: 6.48 s 
2025-03-16 13:24:18.411545: Yayy! New best EMA pseudo Dice: 0.7885000109672546 
2025-03-16 13:24:18.963815:  
2025-03-16 13:24:18.969831: Epoch 10 
2025-03-16 13:24:18.972333: Current learning rate: 0.0091 
2025-03-16 13:24:25.455032: train_loss -0.8804 
2025-03-16 13:24:25.463197: val_loss -0.8507 
2025-03-16 13:24:25.467737: Pseudo dice [np.float32(0.8991), np.float32(0.8792)] 
2025-03-16 13:24:25.470253: Epoch time: 6.49 s 
2025-03-16 13:24:25.474577: Yayy! New best EMA pseudo Dice: 0.7986000180244446 
2025-03-16 13:24:26.031343:  
2025-03-16 13:24:26.036913: Epoch 11 
2025-03-16 13:24:26.039448: Current learning rate: 0.009 
2025-03-16 13:24:32.513680: train_loss -0.8836 
2025-03-16 13:24:32.520849: val_loss -0.8565 
2025-03-16 13:24:32.523916: Pseudo dice [np.float32(0.9029), np.float32(0.8862)] 
2025-03-16 13:24:32.527991: Epoch time: 6.48 s 
2025-03-16 13:24:32.531645: Yayy! New best EMA pseudo Dice: 0.8082000017166138 
2025-03-16 13:24:33.090455:  
2025-03-16 13:24:33.097011: Epoch 12 
2025-03-16 13:24:33.100029: Current learning rate: 0.00891 
2025-03-16 13:24:39.538220: train_loss -0.8864 
2025-03-16 13:24:39.543756: val_loss -0.8466 
2025-03-16 13:24:39.548295: Pseudo dice [np.float32(0.8956), np.float32(0.8775)] 
2025-03-16 13:24:39.551352: Epoch time: 6.45 s 
2025-03-16 13:24:39.554859: Yayy! New best EMA pseudo Dice: 0.8159999847412109 
2025-03-16 13:24:40.266595:  
2025-03-16 13:24:40.271613: Epoch 13 
2025-03-16 13:24:40.274117: Current learning rate: 0.00882 
2025-03-16 13:24:46.735356: train_loss -0.8864 
2025-03-16 13:24:46.741966: val_loss -0.8499 
2025-03-16 13:24:46.745004: Pseudo dice [np.float32(0.8999), np.float32(0.8789)] 
2025-03-16 13:24:46.748621: Epoch time: 6.47 s 
2025-03-16 13:24:46.752685: Yayy! New best EMA pseudo Dice: 0.8234000205993652 
2025-03-16 13:24:47.318712:  
2025-03-16 13:24:47.324842: Epoch 14 
2025-03-16 13:24:47.327896: Current learning rate: 0.00873 
2025-03-16 13:24:53.799233: train_loss -0.8882 
2025-03-16 13:24:53.805352: val_loss -0.8467 
2025-03-16 13:24:53.809932: Pseudo dice [np.float32(0.8957), np.float32(0.8788)] 
2025-03-16 13:24:53.812959: Epoch time: 6.48 s 
2025-03-16 13:24:53.816106: Yayy! New best EMA pseudo Dice: 0.8296999931335449 
2025-03-16 13:24:54.396342:  
2025-03-16 13:24:54.401976: Epoch 15 
2025-03-16 13:24:54.405035: Current learning rate: 0.00864 
2025-03-16 13:25:00.834039: train_loss -0.8904 
2025-03-16 13:25:00.840089: val_loss -0.8476 
2025-03-16 13:25:00.843140: Pseudo dice [np.float32(0.8971), np.float32(0.8781)] 
2025-03-16 13:25:00.847224: Epoch time: 6.44 s 
2025-03-16 13:25:00.850774: Yayy! New best EMA pseudo Dice: 0.8355000019073486 
2025-03-16 13:25:01.430285:  
2025-03-16 13:25:01.436856: Epoch 16 
2025-03-16 13:25:01.439443: Current learning rate: 0.00855 
2025-03-16 13:25:07.907336: train_loss -0.8917 
2025-03-16 13:25:07.913497: val_loss -0.8467 
2025-03-16 13:25:07.916634: Pseudo dice [np.float32(0.8962), np.float32(0.8791)] 
2025-03-16 13:25:07.920690: Epoch time: 6.48 s 
2025-03-16 13:25:07.923728: Yayy! New best EMA pseudo Dice: 0.8406999707221985 
2025-03-16 13:25:08.518924:  
2025-03-16 13:25:08.525505: Epoch 17 
2025-03-16 13:25:08.528031: Current learning rate: 0.00846 
2025-03-16 13:25:14.987025: train_loss -0.893 
2025-03-16 13:25:14.992609: val_loss -0.85 
2025-03-16 13:25:14.996691: Pseudo dice [np.float32(0.8974), np.float32(0.8822)] 
2025-03-16 13:25:15.000237: Epoch time: 6.47 s 
2025-03-16 13:25:15.003291: Yayy! New best EMA pseudo Dice: 0.8456000089645386 
2025-03-16 13:25:15.583053:  
2025-03-16 13:25:15.589679: Epoch 18 
2025-03-16 13:25:15.592220: Current learning rate: 0.00836 
2025-03-16 13:25:22.042461: train_loss -0.8943 
2025-03-16 13:25:22.048051: val_loss -0.8452 
2025-03-16 13:25:22.052719: Pseudo dice [np.float32(0.8954), np.float32(0.8779)] 
2025-03-16 13:25:22.056746: Epoch time: 6.46 s 
2025-03-16 13:25:22.060354: Yayy! New best EMA pseudo Dice: 0.8496999740600586 
2025-03-16 13:25:22.640860:  
2025-03-16 13:25:22.647390: Epoch 19 
2025-03-16 13:25:22.650932: Current learning rate: 0.00827 
2025-03-16 13:25:29.120594: train_loss -0.8987 
2025-03-16 13:25:29.127265: val_loss -0.8492 
2025-03-16 13:25:29.130847: Pseudo dice [np.float32(0.898), np.float32(0.8805)] 
2025-03-16 13:25:29.134873: Epoch time: 6.48 s 
2025-03-16 13:25:29.137883: Yayy! New best EMA pseudo Dice: 0.8536999821662903 
2025-03-16 13:25:29.716247:  
2025-03-16 13:25:29.722327: Epoch 20 
2025-03-16 13:25:29.725893: Current learning rate: 0.00818 
2025-03-16 13:25:36.186270: train_loss -0.8983 
2025-03-16 13:25:36.192368: val_loss -0.8491 
2025-03-16 13:25:36.195889: Pseudo dice [np.float32(0.8985), np.float32(0.8813)] 
2025-03-16 13:25:36.200548: Epoch time: 6.47 s 
2025-03-16 13:25:36.204112: Yayy! New best EMA pseudo Dice: 0.8572999835014343 
2025-03-16 13:25:36.931863:  
2025-03-16 13:25:36.937428: Epoch 21 
2025-03-16 13:25:36.941486: Current learning rate: 0.00809 
2025-03-16 13:25:43.389243: train_loss -0.9005 
2025-03-16 13:25:43.394343: val_loss -0.8486 
2025-03-16 13:25:43.399414: Pseudo dice [np.float32(0.8996), np.float32(0.8801)] 
2025-03-16 13:25:43.402508: Epoch time: 6.46 s 
2025-03-16 13:25:43.406077: Yayy! New best EMA pseudo Dice: 0.8605999946594238 
2025-03-16 13:25:43.962461:  
2025-03-16 13:25:43.968477: Epoch 22 
2025-03-16 13:25:43.970980: Current learning rate: 0.008 
2025-03-16 13:25:50.415815: train_loss -0.9002 
2025-03-16 13:25:50.421434: val_loss -0.8465 
2025-03-16 13:25:50.424963: Pseudo dice [np.float32(0.8975), np.float32(0.8785)] 
2025-03-16 13:25:50.428515: Epoch time: 6.45 s 
2025-03-16 13:25:50.432100: Yayy! New best EMA pseudo Dice: 0.8633000254631042 
2025-03-16 13:25:50.996307:  
2025-03-16 13:25:51.001385: Epoch 23 
2025-03-16 13:25:51.005413: Current learning rate: 0.0079 
2025-03-16 13:25:57.464727: train_loss -0.9018 
2025-03-16 13:25:57.470795: val_loss -0.8486 
2025-03-16 13:25:57.473829: Pseudo dice [np.float32(0.8992), np.float32(0.8807)] 
2025-03-16 13:25:57.477856: Epoch time: 6.47 s 
2025-03-16 13:25:57.480887: Yayy! New best EMA pseudo Dice: 0.8659999966621399 
2025-03-16 13:25:58.027674:  
2025-03-16 13:25:58.034229: Epoch 24 
2025-03-16 13:25:58.037254: Current learning rate: 0.00781 
2025-03-16 13:26:04.492103: train_loss -0.9018 
2025-03-16 13:26:04.497622: val_loss -0.8451 
2025-03-16 13:26:04.501134: Pseudo dice [np.float32(0.8975), np.float32(0.8787)] 
2025-03-16 13:26:04.505175: Epoch time: 6.46 s 
2025-03-16 13:26:04.508226: Yayy! New best EMA pseudo Dice: 0.8682000041007996 
2025-03-16 13:26:05.068290:  
2025-03-16 13:26:05.074428: Epoch 25 
2025-03-16 13:26:05.077486: Current learning rate: 0.00772 
2025-03-16 13:26:11.529758: train_loss -0.9025 
2025-03-16 13:26:11.535939: val_loss -0.8478 
2025-03-16 13:26:11.539469: Pseudo dice [np.float32(0.8989), np.float32(0.8796)] 
2025-03-16 13:26:11.543010: Epoch time: 6.46 s 
2025-03-16 13:26:11.546035: Yayy! New best EMA pseudo Dice: 0.8702999949455261 
2025-03-16 13:26:12.102436:  
2025-03-16 13:26:12.106454: Epoch 26 
2025-03-16 13:26:12.109637: Current learning rate: 0.00763 
2025-03-16 13:26:18.583184: train_loss -0.9052 
2025-03-16 13:26:18.588787: val_loss -0.8484 
2025-03-16 13:26:18.592971: Pseudo dice [np.float32(0.8994), np.float32(0.8804)] 
2025-03-16 13:26:18.596006: Epoch time: 6.48 s 
2025-03-16 13:26:18.598541: Yayy! New best EMA pseudo Dice: 0.8723000288009644 
2025-03-16 13:26:19.158754:  
2025-03-16 13:26:19.164267: Epoch 27 
2025-03-16 13:26:19.167781: Current learning rate: 0.00753 
2025-03-16 13:26:25.643428: train_loss -0.9054 
2025-03-16 13:26:25.653022: val_loss -0.848 
2025-03-16 13:26:25.658054: Pseudo dice [np.float32(0.9003), np.float32(0.8816)] 
2025-03-16 13:26:25.661106: Epoch time: 6.49 s 
2025-03-16 13:26:25.664689: Yayy! New best EMA pseudo Dice: 0.8741000294685364 
2025-03-16 13:26:26.223814:  
2025-03-16 13:26:26.229326: Epoch 28 
2025-03-16 13:26:26.232834: Current learning rate: 0.00744 
2025-03-16 13:26:32.664436: train_loss -0.9049 
2025-03-16 13:26:32.670749: val_loss -0.8413 
2025-03-16 13:26:32.674349: Pseudo dice [np.float32(0.8939), np.float32(0.8757)] 
2025-03-16 13:26:32.676912: Epoch time: 6.44 s 
2025-03-16 13:26:32.680938: Yayy! New best EMA pseudo Dice: 0.8751999735832214 
2025-03-16 13:26:33.400425:  
2025-03-16 13:26:33.405941: Epoch 29 
2025-03-16 13:26:33.409453: Current learning rate: 0.00735 
2025-03-16 13:26:39.854393: train_loss -0.9049 
2025-03-16 13:26:39.860422: val_loss -0.8451 
2025-03-16 13:26:39.862988: Pseudo dice [np.float32(0.8971), np.float32(0.8791)] 
2025-03-16 13:26:39.867012: Epoch time: 6.45 s 
2025-03-16 13:26:39.870555: Yayy! New best EMA pseudo Dice: 0.8765000104904175 
2025-03-16 13:26:40.436152:  
2025-03-16 13:26:40.441702: Epoch 30 
2025-03-16 13:26:40.444291: Current learning rate: 0.00725 
2025-03-16 13:26:46.899339: train_loss -0.9086 
2025-03-16 13:26:46.904871: val_loss -0.8424 
2025-03-16 13:26:46.908382: Pseudo dice [np.float32(0.8959), np.float32(0.8777)] 
2025-03-16 13:26:46.911903: Epoch time: 6.46 s 
2025-03-16 13:26:46.914934: Yayy! New best EMA pseudo Dice: 0.8774999976158142 
2025-03-16 13:26:47.476897:  
2025-03-16 13:26:47.482485: Epoch 31 
2025-03-16 13:26:47.486522: Current learning rate: 0.00716 
2025-03-16 13:26:53.949780: train_loss -0.9091 
2025-03-16 13:26:53.955826: val_loss -0.8429 
2025-03-16 13:26:53.959405: Pseudo dice [np.float32(0.898), np.float32(0.8769)] 
2025-03-16 13:26:53.962958: Epoch time: 6.47 s 
2025-03-16 13:26:53.966510: Yayy! New best EMA pseudo Dice: 0.8784999847412109 
2025-03-16 13:26:54.534554:  
2025-03-16 13:26:54.540122: Epoch 32 
2025-03-16 13:26:54.543252: Current learning rate: 0.00707 
2025-03-16 13:27:01.014061: train_loss -0.9088 
2025-03-16 13:27:01.019626: val_loss -0.8462 
2025-03-16 13:27:01.023175: Pseudo dice [np.float32(0.8989), np.float32(0.8803)] 
2025-03-16 13:27:01.026703: Epoch time: 6.48 s 
2025-03-16 13:27:01.030258: Yayy! New best EMA pseudo Dice: 0.8795999884605408 
2025-03-16 13:27:01.597184:  
2025-03-16 13:27:01.602743: Epoch 33 
2025-03-16 13:27:01.606308: Current learning rate: 0.00697 
2025-03-16 13:27:08.084765: train_loss -0.91 
2025-03-16 13:27:08.090984: val_loss -0.8476 
2025-03-16 13:27:08.094547: Pseudo dice [np.float32(0.9001), np.float32(0.8804)] 
2025-03-16 13:27:08.097578: Epoch time: 6.49 s 
2025-03-16 13:27:08.100643: Yayy! New best EMA pseudo Dice: 0.8806999921798706 
2025-03-16 13:27:08.674435:  
2025-03-16 13:27:08.679448: Epoch 34 
2025-03-16 13:27:08.682957: Current learning rate: 0.00688 
2025-03-16 13:27:15.137914: train_loss -0.9115 
2025-03-16 13:27:15.143433: val_loss -0.8425 
2025-03-16 13:27:15.147947: Pseudo dice [np.float32(0.8966), np.float32(0.8782)] 
2025-03-16 13:27:15.150955: Epoch time: 6.46 s 
2025-03-16 13:27:15.154463: Yayy! New best EMA pseudo Dice: 0.8813999891281128 
2025-03-16 13:27:15.741662:  
2025-03-16 13:27:15.747177: Epoch 35 
2025-03-16 13:27:15.750685: Current learning rate: 0.00679 
2025-03-16 13:27:22.205340: train_loss -0.9115 
2025-03-16 13:27:22.212051: val_loss -0.8459 
2025-03-16 13:27:22.215591: Pseudo dice [np.float32(0.8982), np.float32(0.8795)] 
2025-03-16 13:27:22.219232: Epoch time: 6.46 s 
2025-03-16 13:27:22.222753: Yayy! New best EMA pseudo Dice: 0.882099986076355 
2025-03-16 13:27:22.798513:  
2025-03-16 13:27:22.803528: Epoch 36 
2025-03-16 13:27:22.807545: Current learning rate: 0.00669 
2025-03-16 13:27:29.282057: train_loss -0.9121 
2025-03-16 13:27:29.287625: val_loss -0.8458 
2025-03-16 13:27:29.290679: Pseudo dice [np.float32(0.8977), np.float32(0.8823)] 
2025-03-16 13:27:29.294275: Epoch time: 6.48 s 
2025-03-16 13:27:29.297313: Yayy! New best EMA pseudo Dice: 0.8828999996185303 
2025-03-16 13:27:30.015225:  
2025-03-16 13:27:30.020313: Epoch 37 
2025-03-16 13:27:30.023328: Current learning rate: 0.0066 
2025-03-16 13:27:36.490974: train_loss -0.914 
2025-03-16 13:27:36.497201: val_loss -0.8469 
2025-03-16 13:27:36.500260: Pseudo dice [np.float32(0.9002), np.float32(0.8821)] 
2025-03-16 13:27:36.503787: Epoch time: 6.48 s 
2025-03-16 13:27:36.506810: Yayy! New best EMA pseudo Dice: 0.8837000131607056 
2025-03-16 13:27:37.089446:  
2025-03-16 13:27:37.095488: Epoch 38 
2025-03-16 13:27:37.100048: Current learning rate: 0.0065 
2025-03-16 13:27:43.567584: train_loss -0.9147 
2025-03-16 13:27:43.572639: val_loss -0.8365 
2025-03-16 13:27:43.577270: Pseudo dice [np.float32(0.8927), np.float32(0.8761)] 
2025-03-16 13:27:43.579816: Epoch time: 6.48 s 
2025-03-16 13:27:43.583848: Yayy! New best EMA pseudo Dice: 0.8838000297546387 
2025-03-16 13:27:44.162687:  
2025-03-16 13:27:44.168250: Epoch 39 
2025-03-16 13:27:44.172298: Current learning rate: 0.00641 
2025-03-16 13:27:50.628956: train_loss -0.9159 
2025-03-16 13:27:50.634971: val_loss -0.8425 
2025-03-16 13:27:50.638491: Pseudo dice [np.float32(0.8971), np.float32(0.8816)] 
2025-03-16 13:27:50.641533: Epoch time: 6.47 s 
2025-03-16 13:27:50.645041: Yayy! New best EMA pseudo Dice: 0.8842999935150146 
2025-03-16 13:27:51.229649:  
2025-03-16 13:27:51.235191: Epoch 40 
2025-03-16 13:27:51.237720: Current learning rate: 0.00631 
2025-03-16 13:27:57.689318: train_loss -0.9158 
2025-03-16 13:27:57.694869: val_loss -0.8415 
2025-03-16 13:27:57.698404: Pseudo dice [np.float32(0.8971), np.float32(0.8802)] 
2025-03-16 13:27:57.702036: Epoch time: 6.46 s 
2025-03-16 13:27:57.704288: Yayy! New best EMA pseudo Dice: 0.8848000168800354 
2025-03-16 13:27:58.295013:  
2025-03-16 13:27:58.300024: Epoch 41 
2025-03-16 13:27:58.303038: Current learning rate: 0.00622 
2025-03-16 13:28:04.741156: train_loss -0.9172 
2025-03-16 13:28:04.746752: val_loss -0.8472 
2025-03-16 13:28:04.750618: Pseudo dice [np.float32(0.9018), np.float32(0.8821)] 
2025-03-16 13:28:04.753126: Epoch time: 6.45 s 
2025-03-16 13:28:04.756642: Yayy! New best EMA pseudo Dice: 0.8855000138282776 
2025-03-16 13:28:05.308747:  
2025-03-16 13:28:05.314275: Epoch 42 
2025-03-16 13:28:05.317322: Current learning rate: 0.00612 
2025-03-16 13:28:11.797017: train_loss -0.917 
2025-03-16 13:28:11.803568: val_loss -0.8425 
2025-03-16 13:28:11.806113: Pseudo dice [np.float32(0.8979), np.float32(0.8797)] 
2025-03-16 13:28:11.809627: Epoch time: 6.49 s 
2025-03-16 13:28:11.813143: Yayy! New best EMA pseudo Dice: 0.8858000040054321 
2025-03-16 13:28:12.368312:  
2025-03-16 13:28:12.373855: Epoch 43 
2025-03-16 13:28:12.377392: Current learning rate: 0.00603 
2025-03-16 13:28:18.824176: train_loss -0.9183 
2025-03-16 13:28:18.830286: val_loss -0.8438 
2025-03-16 13:28:18.833834: Pseudo dice [np.float32(0.8978), np.float32(0.8812)] 
2025-03-16 13:28:18.836893: Epoch time: 6.46 s 
2025-03-16 13:28:18.840497: Yayy! New best EMA pseudo Dice: 0.8862000107765198 
2025-03-16 13:28:19.401765:  
2025-03-16 13:28:19.407323: Epoch 44 
2025-03-16 13:28:19.410403: Current learning rate: 0.00593 
2025-03-16 13:28:25.872405: train_loss -0.9175 
2025-03-16 13:28:25.878470: val_loss -0.8423 
2025-03-16 13:28:25.881006: Pseudo dice [np.float32(0.8977), np.float32(0.8796)] 
2025-03-16 13:28:25.885545: Epoch time: 6.47 s 
2025-03-16 13:28:25.888079: Yayy! New best EMA pseudo Dice: 0.8863999843597412 
2025-03-16 13:28:26.592377:  
2025-03-16 13:28:26.598900: Epoch 45 
2025-03-16 13:28:26.602912: Current learning rate: 0.00584 
2025-03-16 13:28:33.085045: train_loss -0.9182 
2025-03-16 13:28:33.090628: val_loss -0.8367 
2025-03-16 13:28:33.094183: Pseudo dice [np.float32(0.8943), np.float32(0.8749)] 
2025-03-16 13:28:33.097264: Epoch time: 6.49 s 
2025-03-16 13:28:33.620903:  
2025-03-16 13:28:33.626481: Epoch 46 
2025-03-16 13:28:33.629515: Current learning rate: 0.00574 
2025-03-16 13:28:40.095515: train_loss -0.9193 
2025-03-16 13:28:40.101566: val_loss -0.8348 
2025-03-16 13:28:40.105070: Pseudo dice [np.float32(0.8935), np.float32(0.8762)] 
2025-03-16 13:28:40.108090: Epoch time: 6.47 s 
2025-03-16 13:28:40.622803:  
2025-03-16 13:28:40.628314: Epoch 47 
2025-03-16 13:28:40.631822: Current learning rate: 0.00565 
2025-03-16 13:28:47.066067: train_loss -0.9196 
2025-03-16 13:28:47.072161: val_loss -0.8428 
2025-03-16 13:28:47.075707: Pseudo dice [np.float32(0.8982), np.float32(0.8812)] 
2025-03-16 13:28:47.080763: Epoch time: 6.44 s 
2025-03-16 13:28:47.082805: Yayy! New best EMA pseudo Dice: 0.8865000009536743 
2025-03-16 13:28:47.641011:  
2025-03-16 13:28:47.646547: Epoch 48 
2025-03-16 13:28:47.650126: Current learning rate: 0.00555 
2025-03-16 13:28:54.122961: train_loss -0.9193 
2025-03-16 13:28:54.129010: val_loss -0.8394 
2025-03-16 13:28:54.132100: Pseudo dice [np.float32(0.8976), np.float32(0.8788)] 
2025-03-16 13:28:54.135609: Epoch time: 6.48 s 
2025-03-16 13:28:54.138305: Yayy! New best EMA pseudo Dice: 0.8866000175476074 
2025-03-16 13:28:54.696660:  
2025-03-16 13:28:54.702175: Epoch 49 
2025-03-16 13:28:54.705687: Current learning rate: 0.00546 
2025-03-16 13:29:01.185903: train_loss -0.9186 
2025-03-16 13:29:01.191994: val_loss -0.841 
2025-03-16 13:29:01.195605: Pseudo dice [np.float32(0.8967), np.float32(0.8803)] 
2025-03-16 13:29:01.199640: Epoch time: 6.49 s 
2025-03-16 13:29:01.233826: Yayy! New best EMA pseudo Dice: 0.8867999911308289 
2025-03-16 13:29:01.797251:  
2025-03-16 13:29:01.802810: Epoch 50 
2025-03-16 13:29:01.805851: Current learning rate: 0.00536 
2025-03-16 13:29:08.270025: train_loss -0.9195 
2025-03-16 13:29:08.276096: val_loss -0.8373 
2025-03-16 13:29:08.280148: Pseudo dice [np.float32(0.896), np.float32(0.8774)] 
2025-03-16 13:29:08.283690: Epoch time: 6.47 s 
2025-03-16 13:29:08.809912:  
2025-03-16 13:29:08.815423: Epoch 51 
2025-03-16 13:29:08.818931: Current learning rate: 0.00526 
2025-03-16 13:29:15.288654: train_loss -0.9225 
2025-03-16 13:29:15.294244: val_loss -0.8415 
2025-03-16 13:29:15.298314: Pseudo dice [np.float32(0.8987), np.float32(0.8815)] 
2025-03-16 13:29:15.300841: Epoch time: 6.48 s 
2025-03-16 13:29:15.304874: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-03-16 13:29:15.869645:  
2025-03-16 13:29:15.874655: Epoch 52 
2025-03-16 13:29:15.879168: Current learning rate: 0.00517 
2025-03-16 13:29:22.351424: train_loss -0.921 
2025-03-16 13:29:22.357522: val_loss -0.8394 
2025-03-16 13:29:22.361148: Pseudo dice [np.float32(0.8972), np.float32(0.879)] 
2025-03-16 13:29:22.363658: Epoch time: 6.48 s 
2025-03-16 13:29:22.366829: Yayy! New best EMA pseudo Dice: 0.8871999979019165 
2025-03-16 13:29:22.926101:  
2025-03-16 13:29:22.931635: Epoch 53 
2025-03-16 13:29:22.935172: Current learning rate: 0.00507 
2025-03-16 13:29:29.373729: train_loss -0.9239 
2025-03-16 13:29:29.378808: val_loss -0.8381 
2025-03-16 13:29:29.382898: Pseudo dice [np.float32(0.8965), np.float32(0.8791)] 
2025-03-16 13:29:29.386461: Epoch time: 6.45 s 
2025-03-16 13:29:29.389496: Yayy! New best EMA pseudo Dice: 0.8873000144958496 
2025-03-16 13:29:30.101164:  
2025-03-16 13:29:30.107234: Epoch 54 
2025-03-16 13:29:30.110310: Current learning rate: 0.00497 
2025-03-16 13:29:36.561558: train_loss -0.9233 
2025-03-16 13:29:36.567630: val_loss -0.8335 
2025-03-16 13:29:36.571153: Pseudo dice [np.float32(0.8929), np.float32(0.8768)] 
2025-03-16 13:29:36.574178: Epoch time: 6.46 s 
2025-03-16 13:29:37.101798:  
2025-03-16 13:29:37.107312: Epoch 55 
2025-03-16 13:29:37.110823: Current learning rate: 0.00487 
2025-03-16 13:29:43.564308: train_loss -0.9233 
2025-03-16 13:29:43.570379: val_loss -0.8353 
2025-03-16 13:29:43.573908: Pseudo dice [np.float32(0.8956), np.float32(0.8774)] 
2025-03-16 13:29:43.576940: Epoch time: 6.46 s 
2025-03-16 13:29:44.103040:  
2025-03-16 13:29:44.108310: Epoch 56 
2025-03-16 13:29:44.112819: Current learning rate: 0.00478 
2025-03-16 13:29:50.566186: train_loss -0.9229 
2025-03-16 13:29:50.573348: val_loss -0.836 
2025-03-16 13:29:50.577380: Pseudo dice [np.float32(0.8953), np.float32(0.8788)] 
2025-03-16 13:29:50.581413: Epoch time: 6.46 s 
2025-03-16 13:29:51.111892:  
2025-03-16 13:29:51.117907: Epoch 57 
2025-03-16 13:29:51.121918: Current learning rate: 0.00468 
2025-03-16 13:29:57.581459: train_loss -0.9257 
2025-03-16 13:29:57.589010: val_loss -0.8365 
2025-03-16 13:29:57.593518: Pseudo dice [np.float32(0.8959), np.float32(0.8795)] 
2025-03-16 13:29:57.597533: Epoch time: 6.47 s 
2025-03-16 13:29:58.131982:  
2025-03-16 13:29:58.138540: Epoch 58 
2025-03-16 13:29:58.141595: Current learning rate: 0.00458 
2025-03-16 13:30:04.590549: train_loss -0.9249 
2025-03-16 13:30:04.596168: val_loss -0.836 
2025-03-16 13:30:04.600791: Pseudo dice [np.float32(0.8964), np.float32(0.8792)] 
2025-03-16 13:30:04.604300: Epoch time: 6.46 s 
2025-03-16 13:30:05.141261:  
2025-03-16 13:30:05.146274: Epoch 59 
2025-03-16 13:30:05.149782: Current learning rate: 0.00448 
2025-03-16 13:30:11.590296: train_loss -0.9235 
2025-03-16 13:30:11.595340: val_loss -0.8414 
2025-03-16 13:30:11.599176: Pseudo dice [np.float32(0.8985), np.float32(0.8817)] 
2025-03-16 13:30:11.603200: Epoch time: 6.45 s 
2025-03-16 13:30:11.606286: Yayy! New best EMA pseudo Dice: 0.8873999714851379 
2025-03-16 13:30:12.177609:  
2025-03-16 13:30:12.182620: Epoch 60 
2025-03-16 13:30:12.185640: Current learning rate: 0.00438 
2025-03-16 13:30:18.668890: train_loss -0.9265 
2025-03-16 13:30:18.675040: val_loss -0.8364 
2025-03-16 13:30:18.680555: Pseudo dice [np.float32(0.8965), np.float32(0.8781)] 
2025-03-16 13:30:18.685596: Epoch time: 6.49 s 
2025-03-16 13:30:19.225257:  
2025-03-16 13:30:19.231349: Epoch 61 
2025-03-16 13:30:19.234475: Current learning rate: 0.00429 
2025-03-16 13:30:25.697856: train_loss -0.9246 
2025-03-16 13:30:25.703587: val_loss -0.8325 
2025-03-16 13:30:25.707361: Pseudo dice [np.float32(0.896), np.float32(0.8768)] 
2025-03-16 13:30:25.710412: Epoch time: 6.47 s 
2025-03-16 13:30:26.395524:  
2025-03-16 13:30:26.403101: Epoch 62 
2025-03-16 13:30:26.406165: Current learning rate: 0.00419 
2025-03-16 13:30:32.844021: train_loss -0.9253 
2025-03-16 13:30:32.850257: val_loss -0.8361 
2025-03-16 13:30:32.853791: Pseudo dice [np.float32(0.8953), np.float32(0.8779)] 
2025-03-16 13:30:32.856941: Epoch time: 6.45 s 
2025-03-16 13:30:33.391891:  
2025-03-16 13:30:33.397413: Epoch 63 
2025-03-16 13:30:33.399924: Current learning rate: 0.00409 
2025-03-16 13:30:39.823584: train_loss -0.9259 
2025-03-16 13:30:39.829153: val_loss -0.8372 
2025-03-16 13:30:39.833183: Pseudo dice [np.float32(0.8967), np.float32(0.8798)] 
2025-03-16 13:30:39.836238: Epoch time: 6.43 s 
2025-03-16 13:30:40.377021:  
2025-03-16 13:30:40.383111: Epoch 64 
2025-03-16 13:30:40.385636: Current learning rate: 0.00399 
2025-03-16 13:30:46.846029: train_loss -0.9245 
2025-03-16 13:30:46.852720: val_loss -0.8305 
2025-03-16 13:30:46.856269: Pseudo dice [np.float32(0.892), np.float32(0.8757)] 
2025-03-16 13:30:46.859305: Epoch time: 6.47 s 
2025-03-16 13:30:47.424261:  
2025-03-16 13:30:47.429350: Epoch 65 
2025-03-16 13:30:47.432860: Current learning rate: 0.00389 
2025-03-16 13:30:53.877383: train_loss -0.9275 
2025-03-16 13:30:53.883502: val_loss -0.8326 
2025-03-16 13:30:53.887050: Pseudo dice [np.float32(0.893), np.float32(0.8775)] 
2025-03-16 13:30:53.890124: Epoch time: 6.45 s 
2025-03-16 13:30:54.428932:  
2025-03-16 13:30:54.434480: Epoch 66 
2025-03-16 13:30:54.438520: Current learning rate: 0.00379 
2025-03-16 13:31:00.892336: train_loss -0.9269 
2025-03-16 13:31:00.897971: val_loss -0.8317 
2025-03-16 13:31:00.901009: Pseudo dice [np.float32(0.8934), np.float32(0.8768)] 
2025-03-16 13:31:00.905108: Epoch time: 6.46 s 
2025-03-16 13:31:01.446238:  
2025-03-16 13:31:01.451881: Epoch 67 
2025-03-16 13:31:01.455458: Current learning rate: 0.00369 
2025-03-16 13:31:07.934204: train_loss -0.9265 
2025-03-16 13:31:07.939835: val_loss -0.8309 
2025-03-16 13:31:07.943430: Pseudo dice [np.float32(0.8929), np.float32(0.8759)] 
2025-03-16 13:31:07.946941: Epoch time: 6.49 s 
2025-03-16 13:31:08.483691:  
2025-03-16 13:31:08.489821: Epoch 68 
2025-03-16 13:31:08.492899: Current learning rate: 0.00359 
2025-03-16 13:31:14.937105: train_loss -0.9287 
2025-03-16 13:31:14.942753: val_loss -0.8325 
2025-03-16 13:31:14.946808: Pseudo dice [np.float32(0.8946), np.float32(0.8767)] 
2025-03-16 13:31:14.949424: Epoch time: 6.45 s 
2025-03-16 13:31:15.643641:  
2025-03-16 13:31:15.648796: Epoch 69 
2025-03-16 13:31:15.652311: Current learning rate: 0.00349 
2025-03-16 13:31:22.100564: train_loss -0.9289 
2025-03-16 13:31:22.107612: val_loss -0.8414 
2025-03-16 13:31:22.110670: Pseudo dice [np.float32(0.8983), np.float32(0.8813)] 
2025-03-16 13:31:22.113740: Epoch time: 6.46 s 
2025-03-16 13:31:22.656535:  
2025-03-16 13:31:22.661553: Epoch 70 
2025-03-16 13:31:22.665063: Current learning rate: 0.00338 
2025-03-16 13:31:29.114619: train_loss -0.9295 
2025-03-16 13:31:29.120200: val_loss -0.8364 
2025-03-16 13:31:29.123811: Pseudo dice [np.float32(0.8978), np.float32(0.8795)] 
2025-03-16 13:31:29.126869: Epoch time: 6.46 s 
2025-03-16 13:31:29.669689:  
2025-03-16 13:31:29.674707: Epoch 71 
2025-03-16 13:31:29.678217: Current learning rate: 0.00328 
2025-03-16 13:31:36.125838: train_loss -0.93 
2025-03-16 13:31:36.130975: val_loss -0.8399 
2025-03-16 13:31:36.135543: Pseudo dice [np.float32(0.8995), np.float32(0.8814)] 
2025-03-16 13:31:36.138596: Epoch time: 6.46 s 
2025-03-16 13:31:36.678678:  
2025-03-16 13:31:36.684725: Epoch 72 
2025-03-16 13:31:36.687139: Current learning rate: 0.00318 
2025-03-16 13:31:43.128864: train_loss -0.9305 
2025-03-16 13:31:43.134999: val_loss -0.8319 
2025-03-16 13:31:43.138061: Pseudo dice [np.float32(0.8942), np.float32(0.8766)] 
2025-03-16 13:31:43.141604: Epoch time: 6.45 s 
2025-03-16 13:31:43.686545:  
2025-03-16 13:31:43.692069: Epoch 73 
2025-03-16 13:31:43.695580: Current learning rate: 0.00308 
2025-03-16 13:31:50.170185: train_loss -0.9302 
2025-03-16 13:31:50.176315: val_loss -0.8321 
2025-03-16 13:31:50.180057: Pseudo dice [np.float32(0.895), np.float32(0.8772)] 
2025-03-16 13:31:50.183633: Epoch time: 6.48 s 
2025-03-16 13:31:50.734963:  
2025-03-16 13:31:50.741091: Epoch 74 
2025-03-16 13:31:50.744142: Current learning rate: 0.00297 
2025-03-16 13:31:57.226254: train_loss -0.9303 
2025-03-16 13:31:57.231928: val_loss -0.8281 
2025-03-16 13:31:57.234971: Pseudo dice [np.float32(0.8937), np.float32(0.8746)] 
2025-03-16 13:31:57.239034: Epoch time: 6.49 s 
2025-03-16 13:31:57.781749:  
2025-03-16 13:31:57.787834: Epoch 75 
2025-03-16 13:31:57.791426: Current learning rate: 0.00287 
2025-03-16 13:32:04.251530: train_loss -0.9301 
2025-03-16 13:32:04.257650: val_loss -0.8348 
2025-03-16 13:32:04.260730: Pseudo dice [np.float32(0.8959), np.float32(0.877)] 
2025-03-16 13:32:04.264274: Epoch time: 6.47 s 
2025-03-16 13:32:04.803245:  
2025-03-16 13:32:04.808340: Epoch 76 
2025-03-16 13:32:04.810895: Current learning rate: 0.00277 
2025-03-16 13:32:11.292616: train_loss -0.9313 
2025-03-16 13:32:11.298659: val_loss -0.8277 
2025-03-16 13:32:11.302171: Pseudo dice [np.float32(0.8923), np.float32(0.8751)] 
2025-03-16 13:32:11.305677: Epoch time: 6.49 s 
2025-03-16 13:32:12.009073:  
2025-03-16 13:32:12.014598: Epoch 77 
2025-03-16 13:32:12.017108: Current learning rate: 0.00266 
2025-03-16 13:32:18.450373: train_loss -0.9312 
2025-03-16 13:32:18.456481: val_loss -0.8359 
2025-03-16 13:32:18.460085: Pseudo dice [np.float32(0.8975), np.float32(0.8806)] 
2025-03-16 13:32:18.463679: Epoch time: 6.44 s 
2025-03-16 13:32:19.020071:  
2025-03-16 13:32:19.025128: Epoch 78 
2025-03-16 13:32:19.028092: Current learning rate: 0.00256 
2025-03-16 13:32:25.480256: train_loss -0.9311 
2025-03-16 13:32:25.485844: val_loss -0.8325 
2025-03-16 13:32:25.488396: Pseudo dice [np.float32(0.8965), np.float32(0.8762)] 
2025-03-16 13:32:25.492509: Epoch time: 6.46 s 
2025-03-16 13:32:26.042546:  
2025-03-16 13:32:26.049062: Epoch 79 
2025-03-16 13:32:26.052574: Current learning rate: 0.00245 
2025-03-16 13:32:32.525213: train_loss -0.9305 
2025-03-16 13:32:32.530801: val_loss -0.834 
2025-03-16 13:32:32.534334: Pseudo dice [np.float32(0.8962), np.float32(0.8787)] 
2025-03-16 13:32:32.537877: Epoch time: 6.48 s 
2025-03-16 13:32:33.090013:  
2025-03-16 13:32:33.096111: Epoch 80 
2025-03-16 13:32:33.098622: Current learning rate: 0.00235 
2025-03-16 13:32:39.526487: train_loss -0.9307 
2025-03-16 13:32:39.532739: val_loss -0.8321 
2025-03-16 13:32:39.536281: Pseudo dice [np.float32(0.8946), np.float32(0.8774)] 
2025-03-16 13:32:39.539307: Epoch time: 6.44 s 
2025-03-16 13:32:40.090347:  
2025-03-16 13:32:40.095968: Epoch 81 
2025-03-16 13:32:40.099003: Current learning rate: 0.00224 
2025-03-16 13:32:46.536609: train_loss -0.9317 
2025-03-16 13:32:46.543158: val_loss -0.8313 
2025-03-16 13:32:46.545700: Pseudo dice [np.float32(0.8949), np.float32(0.8777)] 
2025-03-16 13:32:46.550254: Epoch time: 6.45 s 
2025-03-16 13:32:47.111421:  
2025-03-16 13:32:47.117022: Epoch 82 
2025-03-16 13:32:47.120587: Current learning rate: 0.00214 
2025-03-16 13:32:53.588490: train_loss -0.9325 
2025-03-16 13:32:53.594784: val_loss -0.8317 
2025-03-16 13:32:53.597845: Pseudo dice [np.float32(0.896), np.float32(0.8763)] 
2025-03-16 13:32:53.601391: Epoch time: 6.48 s 
2025-03-16 13:32:54.123369:  
2025-03-16 13:32:54.128388: Epoch 83 
2025-03-16 13:32:54.131409: Current learning rate: 0.00203 
2025-03-16 13:33:00.583819: train_loss -0.9319 
2025-03-16 13:33:00.589907: val_loss -0.8329 
2025-03-16 13:33:00.594058: Pseudo dice [np.float32(0.8967), np.float32(0.8792)] 
2025-03-16 13:33:00.597104: Epoch time: 6.46 s 
2025-03-16 13:33:01.119040:  
2025-03-16 13:33:01.124581: Epoch 84 
2025-03-16 13:33:01.128638: Current learning rate: 0.00192 
2025-03-16 13:33:07.558538: train_loss -0.9324 
2025-03-16 13:33:07.565646: val_loss -0.8288 
2025-03-16 13:33:07.568718: Pseudo dice [np.float32(0.8943), np.float32(0.8755)] 
2025-03-16 13:33:07.572281: Epoch time: 6.44 s 
2025-03-16 13:33:08.244833:  
2025-03-16 13:33:08.250917: Epoch 85 
2025-03-16 13:33:08.253997: Current learning rate: 0.00181 
2025-03-16 13:33:14.712709: train_loss -0.9334 
2025-03-16 13:33:14.718957: val_loss -0.8298 
2025-03-16 13:33:14.721503: Pseudo dice [np.float32(0.8943), np.float32(0.8778)] 
2025-03-16 13:33:14.725635: Epoch time: 6.47 s 
2025-03-16 13:33:15.239086:  
2025-03-16 13:33:15.244609: Epoch 86 
2025-03-16 13:33:15.248118: Current learning rate: 0.0017 
2025-03-16 13:33:21.729254: train_loss -0.933 
2025-03-16 13:33:21.735409: val_loss -0.8321 
2025-03-16 13:33:21.739076: Pseudo dice [np.float32(0.8951), np.float32(0.8777)] 
2025-03-16 13:33:21.742634: Epoch time: 6.49 s 
2025-03-16 13:33:22.264415:  
2025-03-16 13:33:22.270553: Epoch 87 
2025-03-16 13:33:22.273634: Current learning rate: 0.00159 
2025-03-16 13:33:28.736061: train_loss -0.9347 
2025-03-16 13:33:28.740885: val_loss -0.8269 
2025-03-16 13:33:28.744926: Pseudo dice [np.float32(0.8929), np.float32(0.8744)] 
2025-03-16 13:33:28.748027: Epoch time: 6.47 s 
2025-03-16 13:33:29.269182:  
2025-03-16 13:33:29.275300: Epoch 88 
2025-03-16 13:33:29.278364: Current learning rate: 0.00148 
2025-03-16 13:33:35.736595: train_loss -0.9321 
2025-03-16 13:33:35.741668: val_loss -0.8342 
2025-03-16 13:33:35.747217: Pseudo dice [np.float32(0.8966), np.float32(0.8781)] 
2025-03-16 13:33:35.750777: Epoch time: 6.47 s 
2025-03-16 13:33:36.266095:  
2025-03-16 13:33:36.271616: Epoch 89 
2025-03-16 13:33:36.275129: Current learning rate: 0.00137 
2025-03-16 13:33:42.723198: train_loss -0.9343 
2025-03-16 13:33:42.728746: val_loss -0.8222 
2025-03-16 13:33:42.732289: Pseudo dice [np.float32(0.8904), np.float32(0.8728)] 
2025-03-16 13:33:42.736363: Epoch time: 6.46 s 
2025-03-16 13:33:43.252500:  
2025-03-16 13:33:43.258588: Epoch 90 
2025-03-16 13:33:43.261647: Current learning rate: 0.00126 
2025-03-16 13:33:49.733019: train_loss -0.9349 
2025-03-16 13:33:49.738678: val_loss -0.8299 
2025-03-16 13:33:49.741804: Pseudo dice [np.float32(0.8951), np.float32(0.8772)] 
2025-03-16 13:33:49.745346: Epoch time: 6.48 s 
2025-03-16 13:33:50.268075:  
2025-03-16 13:33:50.275152: Epoch 91 
2025-03-16 13:33:50.279255: Current learning rate: 0.00115 
2025-03-16 13:33:56.752649: train_loss -0.9338 
2025-03-16 13:33:56.759233: val_loss -0.8317 
2025-03-16 13:33:56.762876: Pseudo dice [np.float32(0.8956), np.float32(0.8788)] 
2025-03-16 13:33:56.765942: Epoch time: 6.48 s 
2025-03-16 13:33:57.284300:  
2025-03-16 13:33:57.289319: Epoch 92 
2025-03-16 13:33:57.292829: Current learning rate: 0.00103 
2025-03-16 13:34:03.789636: train_loss -0.9353 
2025-03-16 13:34:03.794183: val_loss -0.8255 
2025-03-16 13:34:03.797726: Pseudo dice [np.float32(0.8922), np.float32(0.8739)] 
2025-03-16 13:34:03.801760: Epoch time: 6.51 s 
2025-03-16 13:34:04.464611:  
2025-03-16 13:34:04.470192: Epoch 93 
2025-03-16 13:34:04.472732: Current learning rate: 0.00091 
2025-03-16 13:34:10.923223: train_loss -0.9344 
2025-03-16 13:34:10.929331: val_loss -0.8336 
2025-03-16 13:34:10.932972: Pseudo dice [np.float32(0.897), np.float32(0.8789)] 
2025-03-16 13:34:10.936055: Epoch time: 6.46 s 
2025-03-16 13:34:11.451441:  
2025-03-16 13:34:11.456962: Epoch 94 
2025-03-16 13:34:11.460473: Current learning rate: 0.00079 
2025-03-16 13:34:17.903953: train_loss -0.9348 
2025-03-16 13:34:17.910113: val_loss -0.8278 
2025-03-16 13:34:17.913169: Pseudo dice [np.float32(0.8941), np.float32(0.8767)] 
2025-03-16 13:34:17.916745: Epoch time: 6.45 s 
2025-03-16 13:34:18.429094:  
2025-03-16 13:34:18.435176: Epoch 95 
2025-03-16 13:34:18.438685: Current learning rate: 0.00067 
2025-03-16 13:34:24.902695: train_loss -0.9356 
2025-03-16 13:34:24.909801: val_loss -0.8222 
2025-03-16 13:34:24.912418: Pseudo dice [np.float32(0.8919), np.float32(0.8718)] 
2025-03-16 13:34:24.917033: Epoch time: 6.47 s 
2025-03-16 13:34:25.434709:  
2025-03-16 13:34:25.440281: Epoch 96 
2025-03-16 13:34:25.442816: Current learning rate: 0.00055 
2025-03-16 13:34:31.891696: train_loss -0.9357 
2025-03-16 13:34:31.897804: val_loss -0.8354 
2025-03-16 13:34:31.900388: Pseudo dice [np.float32(0.8972), np.float32(0.8811)] 
2025-03-16 13:34:31.905409: Epoch time: 6.46 s 
2025-03-16 13:34:32.434525:  
2025-03-16 13:34:32.439580: Epoch 97 
2025-03-16 13:34:32.443692: Current learning rate: 0.00043 
2025-03-16 13:34:38.890162: train_loss -0.9354 
2025-03-16 13:34:38.895749: val_loss -0.8308 
2025-03-16 13:34:38.899324: Pseudo dice [np.float32(0.8957), np.float32(0.8769)] 
2025-03-16 13:34:38.902920: Epoch time: 6.46 s 
2025-03-16 13:34:39.427462:  
2025-03-16 13:34:39.432477: Epoch 98 
2025-03-16 13:34:39.435987: Current learning rate: 0.0003 
2025-03-16 13:34:45.881127: train_loss -0.935 
2025-03-16 13:34:45.886745: val_loss -0.8311 
2025-03-16 13:34:45.890795: Pseudo dice [np.float32(0.8958), np.float32(0.878)] 
2025-03-16 13:34:45.894346: Epoch time: 6.45 s 
2025-03-16 13:34:46.421793:  
2025-03-16 13:34:46.428350: Epoch 99 
2025-03-16 13:34:46.431391: Current learning rate: 0.00016 
2025-03-16 13:34:52.862098: train_loss -0.9358 
2025-03-16 13:34:52.867695: val_loss -0.8269 
2025-03-16 13:34:52.872246: Pseudo dice [np.float32(0.8936), np.float32(0.8759)] 
2025-03-16 13:34:52.875365: Epoch time: 6.44 s 
2025-03-16 13:34:53.466757: Training done. 
2025-03-16 13:34:53.501758: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-16 13:34:53.508759: The split file contains 5 splits. 
2025-03-16 13:34:53.513766: Desired fold for training: 0 
2025-03-16 13:34:53.519764: This split has 208 training and 52 validation cases. 
2025-03-16 13:34:53.524767: predicting hippocampus_017 
2025-03-16 13:34:53.530280: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-16 13:34:53.644365: predicting hippocampus_019 
2025-03-16 13:34:53.651368: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-16 13:34:53.693366: predicting hippocampus_033 
2025-03-16 13:34:53.700368: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-16 13:34:53.725369: predicting hippocampus_035 
2025-03-16 13:34:53.731880: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-16 13:34:53.756883: predicting hippocampus_037 
2025-03-16 13:34:53.763882: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-16 13:34:53.789882: predicting hippocampus_049 
2025-03-16 13:34:53.795883: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-16 13:34:53.822885: predicting hippocampus_052 
2025-03-16 13:34:53.829395: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-16 13:34:53.855395: predicting hippocampus_065 
2025-03-16 13:34:53.862395: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-16 13:34:53.891395: predicting hippocampus_083 
2025-03-16 13:34:53.897396: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-16 13:34:53.921399: predicting hippocampus_088 
2025-03-16 13:34:53.927909: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-16 13:34:57.380998: predicting hippocampus_090 
2025-03-16 13:34:57.392004: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-16 13:34:57.443512: predicting hippocampus_092 
2025-03-16 13:34:57.450514: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-16 13:34:57.503513: predicting hippocampus_095 
2025-03-16 13:34:57.510512: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-16 13:34:57.562030: predicting hippocampus_107 
2025-03-16 13:34:57.569030: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-16 13:34:57.630536: predicting hippocampus_108 
2025-03-16 13:34:57.650538: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-16 13:34:57.692536: predicting hippocampus_123 
2025-03-16 13:34:57.697536: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-16 13:34:57.733043: predicting hippocampus_125 
2025-03-16 13:34:57.738043: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-16 13:34:57.801044: predicting hippocampus_157 
2025-03-16 13:34:57.807043: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-16 13:34:57.842550: predicting hippocampus_164 
2025-03-16 13:34:57.849550: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-16 13:34:57.929057: predicting hippocampus_169 
2025-03-16 13:34:57.936057: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-16 13:34:57.962057: predicting hippocampus_175 
2025-03-16 13:34:57.968057: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-16 13:34:57.996057: predicting hippocampus_185 
2025-03-16 13:34:58.004058: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-16 13:34:58.032565: predicting hippocampus_190 
2025-03-16 13:34:58.039565: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-16 13:34:58.066565: predicting hippocampus_194 
2025-03-16 13:34:58.072565: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-16 13:34:58.098565: predicting hippocampus_204 
2025-03-16 13:34:58.102565: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-16 13:34:58.132072: predicting hippocampus_205 
2025-03-16 13:34:58.137072: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-16 13:34:58.162072: predicting hippocampus_210 
2025-03-16 13:34:58.167072: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-16 13:34:58.194073: predicting hippocampus_217 
2025-03-16 13:34:58.199072: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-16 13:34:58.224074: predicting hippocampus_219 
2025-03-16 13:34:58.228577: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-16 13:34:58.255580: predicting hippocampus_229 
2025-03-16 13:34:58.260580: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-16 13:34:58.285580: predicting hippocampus_244 
2025-03-16 13:34:58.290580: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-16 13:34:58.316583: predicting hippocampus_261 
2025-03-16 13:34:58.323583: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-16 13:34:58.371088: predicting hippocampus_264 
2025-03-16 13:34:58.376088: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-16 13:34:58.401088: predicting hippocampus_277 
2025-03-16 13:34:58.406088: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-16 13:34:58.448595: predicting hippocampus_280 
2025-03-16 13:34:58.453595: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-16 13:34:58.481595: predicting hippocampus_286 
2025-03-16 13:34:58.487595: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-16 13:34:58.534104: predicting hippocampus_288 
2025-03-16 13:34:58.542103: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-16 13:34:58.585103: predicting hippocampus_289 
2025-03-16 13:34:58.590103: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-16 13:34:58.616105: predicting hippocampus_296 
2025-03-16 13:34:58.623106: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-16 13:34:58.649611: predicting hippocampus_305 
2025-03-16 13:34:58.654613: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-16 13:34:58.679611: predicting hippocampus_308 
2025-03-16 13:34:58.683611: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-16 13:34:58.708611: predicting hippocampus_317 
2025-03-16 13:34:58.713614: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-16 13:34:58.738119: predicting hippocampus_327 
2025-03-16 13:34:58.743119: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-16 13:34:58.769119: predicting hippocampus_330 
2025-03-16 13:34:58.774119: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-16 13:34:58.800119: predicting hippocampus_332 
2025-03-16 13:34:58.804119: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-16 13:34:58.831626: predicting hippocampus_338 
2025-03-16 13:34:58.836627: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-16 13:34:58.878626: predicting hippocampus_349 
2025-03-16 13:34:58.883626: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-16 13:34:58.908626: predicting hippocampus_350 
2025-03-16 13:34:58.914629: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-16 13:34:58.941135: predicting hippocampus_356 
2025-03-16 13:34:58.946134: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-16 13:34:58.972135: predicting hippocampus_358 
2025-03-16 13:34:58.976134: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-16 13:34:59.004134: predicting hippocampus_374 
2025-03-16 13:34:59.009136: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-16 13:34:59.036642: predicting hippocampus_394 
2025-03-16 13:34:59.040642: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-16 13:35:02.542046: Validation complete 
2025-03-16 13:35:02.547044: Mean Validation Dice:  0.23071048718697734 
