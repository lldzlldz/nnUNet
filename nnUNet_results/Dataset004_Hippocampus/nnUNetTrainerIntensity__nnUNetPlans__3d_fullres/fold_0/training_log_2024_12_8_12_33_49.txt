
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-12-08 12:33:49.873842: do_dummy_2d_data_aug: False 
2024-12-08 12:33:49.873842: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2024-12-08 12:33:49.881994: The split file contains 5 splits. 
2024-12-08 12:33:49.881994: Desired fold for training: 0 
2024-12-08 12:33:49.881994: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2024-12-08 12:33:55.569366: unpacking dataset... 
2024-12-08 12:33:55.883130: unpacking done... 
2024-12-08 12:33:56.940570:  
2024-12-08 12:33:56.943682: Epoch 0 
2024-12-08 12:33:56.948231: Current learning rate: 0.01 
2024-12-08 12:34:03.994554: train_loss -0.4913 
2024-12-08 12:34:03.999135: val_loss -0.8124 
2024-12-08 12:34:04.002208: Pseudo dice [np.float32(0.8674), np.float32(0.8584)] 
2024-12-08 12:34:04.004766: Epoch time: 7.06 s 
2024-12-08 12:34:04.007820: Yayy! New best EMA pseudo Dice: 0.8629000186920166 
2024-12-08 12:34:04.533237:  
2024-12-08 12:34:04.538333: Epoch 1 
2024-12-08 12:34:04.540870: Current learning rate: 0.00991 
2024-12-08 12:34:10.708225: train_loss -0.8464 
2024-12-08 12:34:10.713877: val_loss -0.8259 
2024-12-08 12:34:10.717441: Pseudo dice [np.float32(0.8783), np.float32(0.8652)] 
2024-12-08 12:34:10.720513: Epoch time: 6.18 s 
2024-12-08 12:34:10.723052: Yayy! New best EMA pseudo Dice: 0.8637999892234802 
2024-12-08 12:34:11.302488:  
2024-12-08 12:34:11.307571: Epoch 2 
2024-12-08 12:34:11.311184: Current learning rate: 0.00982 
2024-12-08 12:34:17.470370: train_loss -0.8749 
2024-12-08 12:34:17.475488: val_loss -0.8314 
2024-12-08 12:34:17.479058: Pseudo dice [np.float32(0.8854), np.float32(0.8683)] 
2024-12-08 12:34:17.481620: Epoch time: 6.17 s 
2024-12-08 12:34:17.484682: Yayy! New best EMA pseudo Dice: 0.8651000261306763 
2024-12-08 12:34:18.107147:  
2024-12-08 12:34:18.113770: Epoch 3 
2024-12-08 12:34:18.116308: Current learning rate: 0.00973 
2024-12-08 12:34:24.260880: train_loss -0.8893 
2024-12-08 12:34:24.267454: val_loss -0.835 
2024-12-08 12:34:24.270502: Pseudo dice [np.float32(0.8892), np.float32(0.8725)] 
2024-12-08 12:34:24.273566: Epoch time: 6.15 s 
2024-12-08 12:34:24.276656: Yayy! New best EMA pseudo Dice: 0.8666999936103821 
2024-12-08 12:34:24.875187:  
2024-12-08 12:34:24.881280: Epoch 4 
2024-12-08 12:34:24.884331: Current learning rate: 0.00964 
2024-12-08 12:34:31.031932: train_loss -0.9035 
2024-12-08 12:34:31.037011: val_loss -0.835 
2024-12-08 12:34:31.041066: Pseudo dice [np.float32(0.8911), np.float32(0.8726)] 
2024-12-08 12:34:31.044134: Epoch time: 6.16 s 
2024-12-08 12:34:31.046700: Yayy! New best EMA pseudo Dice: 0.8682000041007996 
2024-12-08 12:34:31.660332:  
2024-12-08 12:34:31.665488: Epoch 5 
2024-12-08 12:34:31.668028: Current learning rate: 0.00955 
2024-12-08 12:34:37.814268: train_loss -0.9121 
2024-12-08 12:34:37.819896: val_loss -0.8352 
2024-12-08 12:34:37.822942: Pseudo dice [np.float32(0.8912), np.float32(0.876)] 
2024-12-08 12:34:37.825985: Epoch time: 6.15 s 
2024-12-08 12:34:37.828020: Yayy! New best EMA pseudo Dice: 0.869700014591217 
2024-12-08 12:34:38.521385:  
2024-12-08 12:34:38.526970: Epoch 6 
2024-12-08 12:34:38.529508: Current learning rate: 0.00946 
2024-12-08 12:34:44.674273: train_loss -0.9186 
2024-12-08 12:34:44.679365: val_loss -0.8299 
2024-12-08 12:34:44.681914: Pseudo dice [np.float32(0.8889), np.float32(0.8733)] 
2024-12-08 12:34:44.684450: Epoch time: 6.15 s 
2024-12-08 12:34:44.689552: Yayy! New best EMA pseudo Dice: 0.8708999752998352 
2024-12-08 12:34:45.277573:  
2024-12-08 12:34:45.282698: Epoch 7 
2024-12-08 12:34:45.286261: Current learning rate: 0.00937 
2024-12-08 12:34:51.444172: train_loss -0.924 
2024-12-08 12:34:51.451288: val_loss -0.8283 
2024-12-08 12:34:51.454363: Pseudo dice [np.float32(0.8899), np.float32(0.8731)] 
2024-12-08 12:34:51.457404: Epoch time: 6.17 s 
2024-12-08 12:34:51.460473: Yayy! New best EMA pseudo Dice: 0.8719000220298767 
2024-12-08 12:34:52.054392:  
2024-12-08 12:34:52.059995: Epoch 8 
2024-12-08 12:34:52.063039: Current learning rate: 0.00928 
2024-12-08 12:34:58.213813: train_loss -0.9302 
2024-12-08 12:34:58.219407: val_loss -0.8286 
2024-12-08 12:34:58.221958: Pseudo dice [np.float32(0.8907), np.float32(0.8742)] 
2024-12-08 12:34:58.224511: Epoch time: 6.16 s 
2024-12-08 12:34:58.228069: Yayy! New best EMA pseudo Dice: 0.8730000257492065 
2024-12-08 12:34:58.829085:  
2024-12-08 12:34:58.834723: Epoch 9 
2024-12-08 12:34:58.837785: Current learning rate: 0.00919 
2024-12-08 12:35:04.989128: train_loss -0.9333 
2024-12-08 12:35:04.996220: val_loss -0.8262 
2024-12-08 12:35:04.998781: Pseudo dice [np.float32(0.8889), np.float32(0.8741)] 
2024-12-08 12:35:05.001321: Epoch time: 6.16 s 
2024-12-08 12:35:05.005891: Yayy! New best EMA pseudo Dice: 0.8737999796867371 
2024-12-08 12:35:05.576056:  
2024-12-08 12:35:05.581685: Epoch 10 
2024-12-08 12:35:05.584741: Current learning rate: 0.0091 
2024-12-08 12:35:11.736991: train_loss -0.9378 
2024-12-08 12:35:11.744154: val_loss -0.823 
2024-12-08 12:35:11.747707: Pseudo dice [np.float32(0.8898), np.float32(0.8731)] 
2024-12-08 12:35:11.749743: Epoch time: 6.16 s 
2024-12-08 12:35:11.753792: Yayy! New best EMA pseudo Dice: 0.8745999932289124 
2024-12-08 12:35:12.332178:  
2024-12-08 12:35:12.337297: Epoch 11 
2024-12-08 12:35:12.339837: Current learning rate: 0.009 
2024-12-08 12:35:18.482237: train_loss -0.9424 
2024-12-08 12:35:18.487827: val_loss -0.8275 
2024-12-08 12:35:18.490887: Pseudo dice [np.float32(0.8923), np.float32(0.8752)] 
2024-12-08 12:35:18.495514: Epoch time: 6.15 s 
2024-12-08 12:35:18.498068: Yayy! New best EMA pseudo Dice: 0.8755000233650208 
2024-12-08 12:35:19.078383:  
2024-12-08 12:35:19.084013: Epoch 12 
2024-12-08 12:35:19.086560: Current learning rate: 0.00891 
2024-12-08 12:35:25.230146: train_loss -0.9436 
2024-12-08 12:35:25.236259: val_loss -0.8258 
2024-12-08 12:35:25.239372: Pseudo dice [np.float32(0.8929), np.float32(0.8757)] 
2024-12-08 12:35:25.242430: Epoch time: 6.15 s 
2024-12-08 12:35:25.245484: Yayy! New best EMA pseudo Dice: 0.8763999938964844 
2024-12-08 12:35:25.832728:  
2024-12-08 12:35:25.838313: Epoch 13 
2024-12-08 12:35:25.842936: Current learning rate: 0.00882 
2024-12-08 12:35:31.982881: train_loss -0.9465 
2024-12-08 12:35:31.988489: val_loss -0.8284 
2024-12-08 12:35:31.991560: Pseudo dice [np.float32(0.895), np.float32(0.8763)] 
2024-12-08 12:35:31.994606: Epoch time: 6.15 s 
2024-12-08 12:35:31.998150: Yayy! New best EMA pseudo Dice: 0.8773000240325928 
2024-12-08 12:35:32.724536:  
2024-12-08 12:35:32.729603: Epoch 14 
2024-12-08 12:35:32.732145: Current learning rate: 0.00873 
2024-12-08 12:35:38.879110: train_loss -0.9487 
2024-12-08 12:35:38.884739: val_loss -0.8177 
2024-12-08 12:35:38.888818: Pseudo dice [np.float32(0.889), np.float32(0.8723)] 
2024-12-08 12:35:38.891898: Epoch time: 6.16 s 
2024-12-08 12:35:38.895466: Yayy! New best EMA pseudo Dice: 0.8776000142097473 
2024-12-08 12:35:39.494516:  
2024-12-08 12:35:39.501119: Epoch 15 
2024-12-08 12:35:39.504671: Current learning rate: 0.00864 
2024-12-08 12:35:45.654483: train_loss -0.9511 
2024-12-08 12:35:45.660089: val_loss -0.8164 
2024-12-08 12:35:45.662126: Pseudo dice [np.float32(0.8882), np.float32(0.8718)] 
2024-12-08 12:35:45.666739: Epoch time: 6.16 s 
2024-12-08 12:35:45.670309: Yayy! New best EMA pseudo Dice: 0.8779000043869019 
2024-12-08 12:35:46.268523:  
2024-12-08 12:35:46.275136: Epoch 16 
2024-12-08 12:35:46.278755: Current learning rate: 0.00855 
2024-12-08 12:35:52.423676: train_loss -0.9524 
2024-12-08 12:35:52.430291: val_loss -0.8217 
2024-12-08 12:35:52.433861: Pseudo dice [np.float32(0.8922), np.float32(0.875)] 
2024-12-08 12:35:52.437405: Epoch time: 6.16 s 
2024-12-08 12:35:52.440481: Yayy! New best EMA pseudo Dice: 0.8784999847412109 
2024-12-08 12:35:53.044222:  
2024-12-08 12:35:53.050314: Epoch 17 
2024-12-08 12:35:53.054379: Current learning rate: 0.00846 
2024-12-08 12:35:59.205432: train_loss -0.9542 
2024-12-08 12:35:59.211013: val_loss -0.8162 
2024-12-08 12:35:59.214070: Pseudo dice [np.float32(0.8877), np.float32(0.8709)] 
2024-12-08 12:35:59.216646: Epoch time: 6.16 s 
2024-12-08 12:35:59.219195: Yayy! New best EMA pseudo Dice: 0.8784999847412109 
2024-12-08 12:35:59.823466:  
2024-12-08 12:35:59.830675: Epoch 18 
2024-12-08 12:35:59.833743: Current learning rate: 0.00836 
2024-12-08 12:36:05.982303: train_loss -0.9566 
2024-12-08 12:36:05.988918: val_loss -0.8211 
2024-12-08 12:36:05.991978: Pseudo dice [np.float32(0.8929), np.float32(0.8758)] 
2024-12-08 12:36:05.994524: Epoch time: 6.16 s 
2024-12-08 12:36:05.998095: Yayy! New best EMA pseudo Dice: 0.8791000247001648 
2024-12-08 12:36:06.608866:  
2024-12-08 12:36:06.614460: Epoch 19 
2024-12-08 12:36:06.617555: Current learning rate: 0.00827 
2024-12-08 12:36:12.769296: train_loss -0.9573 
2024-12-08 12:36:12.774920: val_loss -0.8169 
2024-12-08 12:36:12.777474: Pseudo dice [np.float32(0.8918), np.float32(0.8737)] 
2024-12-08 12:36:12.780520: Epoch time: 6.16 s 
2024-12-08 12:36:12.784583: Yayy! New best EMA pseudo Dice: 0.8794999718666077 
2024-12-08 12:36:13.380674:  
2024-12-08 12:36:13.385770: Epoch 20 
2024-12-08 12:36:13.389336: Current learning rate: 0.00818 
2024-12-08 12:36:19.541058: train_loss -0.9584 
2024-12-08 12:36:19.546653: val_loss -0.8154 
2024-12-08 12:36:19.548678: Pseudo dice [np.float32(0.8899), np.float32(0.8718)] 
2024-12-08 12:36:19.553848: Epoch time: 6.16 s 
2024-12-08 12:36:19.556392: Yayy! New best EMA pseudo Dice: 0.8795999884605408 
2024-12-08 12:36:20.311906:  
2024-12-08 12:36:20.317995: Epoch 21 
2024-12-08 12:36:20.320530: Current learning rate: 0.00809 
2024-12-08 12:36:26.470012: train_loss -0.96 
2024-12-08 12:36:26.476171: val_loss -0.8181 
2024-12-08 12:36:26.478729: Pseudo dice [np.float32(0.8926), np.float32(0.8743)] 
2024-12-08 12:36:26.482277: Epoch time: 6.16 s 
2024-12-08 12:36:26.484818: Yayy! New best EMA pseudo Dice: 0.8799999952316284 
2024-12-08 12:36:27.061851:  
2024-12-08 12:36:27.067518: Epoch 22 
2024-12-08 12:36:27.071092: Current learning rate: 0.008 
2024-12-08 12:36:33.216276: train_loss -0.9615 
2024-12-08 12:36:33.221868: val_loss -0.8144 
2024-12-08 12:36:33.224440: Pseudo dice [np.float32(0.8911), np.float32(0.8726)] 
2024-12-08 12:36:33.228043: Epoch time: 6.15 s 
2024-12-08 12:36:33.232121: Yayy! New best EMA pseudo Dice: 0.8802000284194946 
2024-12-08 12:36:33.807367:  
2024-12-08 12:36:33.812964: Epoch 23 
2024-12-08 12:36:33.815504: Current learning rate: 0.0079 
2024-12-08 12:36:39.965188: train_loss -0.9614 
2024-12-08 12:36:39.971329: val_loss -0.8128 
2024-12-08 12:36:39.974877: Pseudo dice [np.float32(0.8897), np.float32(0.8715)] 
2024-12-08 12:36:39.977951: Epoch time: 6.16 s 
2024-12-08 12:36:39.980496: Yayy! New best EMA pseudo Dice: 0.8802000284194946 
2024-12-08 12:36:40.553122:  
2024-12-08 12:36:40.559211: Epoch 24 
2024-12-08 12:36:40.564321: Current learning rate: 0.00781 
2024-12-08 12:36:46.714266: train_loss -0.9624 
2024-12-08 12:36:46.720358: val_loss -0.8106 
2024-12-08 12:36:46.723902: Pseudo dice [np.float32(0.8885), np.float32(0.871)] 
2024-12-08 12:36:46.726999: Epoch time: 6.16 s 
2024-12-08 12:36:47.270917:  
2024-12-08 12:36:47.276516: Epoch 25 
2024-12-08 12:36:47.279589: Current learning rate: 0.00772 
2024-12-08 12:36:53.443170: train_loss -0.9638 
2024-12-08 12:36:53.450331: val_loss -0.8095 
2024-12-08 12:36:53.452876: Pseudo dice [np.float32(0.8894), np.float32(0.872)] 
2024-12-08 12:36:53.456913: Epoch time: 6.17 s 
2024-12-08 12:36:53.460011: Yayy! New best EMA pseudo Dice: 0.8802000284194946 
2024-12-08 12:36:54.038709:  
2024-12-08 12:36:54.044371: Epoch 26 
2024-12-08 12:36:54.046911: Current learning rate: 0.00763 
2024-12-08 12:37:00.205362: train_loss -0.9634 
2024-12-08 12:37:00.212445: val_loss -0.8172 
2024-12-08 12:37:00.215552: Pseudo dice [np.float32(0.8923), np.float32(0.8739)] 
2024-12-08 12:37:00.220640: Epoch time: 6.17 s 
2024-12-08 12:37:00.223714: Yayy! New best EMA pseudo Dice: 0.8805000185966492 
2024-12-08 12:37:00.811223:  
2024-12-08 12:37:00.817356: Epoch 27 
2024-12-08 12:37:00.820873: Current learning rate: 0.00753 
2024-12-08 12:37:06.989731: train_loss -0.9648 
2024-12-08 12:37:06.995325: val_loss -0.8112 
2024-12-08 12:37:06.998416: Pseudo dice [np.float32(0.89), np.float32(0.8735)] 
2024-12-08 12:37:07.001985: Epoch time: 6.18 s 
2024-12-08 12:37:07.005054: Yayy! New best EMA pseudo Dice: 0.8805999755859375 
2024-12-08 12:37:07.590564:  
2024-12-08 12:37:07.596664: Epoch 28 
2024-12-08 12:37:07.599207: Current learning rate: 0.00744 
2024-12-08 12:37:13.763519: train_loss -0.9657 
2024-12-08 12:37:13.769108: val_loss -0.8082 
2024-12-08 12:37:13.771644: Pseudo dice [np.float32(0.8896), np.float32(0.8708)] 
2024-12-08 12:37:13.775728: Epoch time: 6.17 s 
2024-12-08 12:37:14.322834:  
2024-12-08 12:37:14.327972: Epoch 29 
2024-12-08 12:37:14.330513: Current learning rate: 0.00735 
2024-12-08 12:37:20.494793: train_loss -0.9667 
2024-12-08 12:37:20.500432: val_loss -0.8113 
2024-12-08 12:37:20.504026: Pseudo dice [np.float32(0.8907), np.float32(0.8729)] 
2024-12-08 12:37:20.507596: Epoch time: 6.17 s 
2024-12-08 12:37:20.510700: Yayy! New best EMA pseudo Dice: 0.8806999921798706 
2024-12-08 12:37:21.247251:  
2024-12-08 12:37:21.252842: Epoch 30 
2024-12-08 12:37:21.257398: Current learning rate: 0.00725 
2024-12-08 12:37:27.415786: train_loss -0.9678 
2024-12-08 12:37:27.421432: val_loss -0.8068 
2024-12-08 12:37:27.424524: Pseudo dice [np.float32(0.8893), np.float32(0.8713)] 
2024-12-08 12:37:27.428611: Epoch time: 6.17 s 
2024-12-08 12:37:27.980513:  
2024-12-08 12:37:27.985637: Epoch 31 
2024-12-08 12:37:27.988180: Current learning rate: 0.00716 
2024-12-08 12:37:34.133997: train_loss -0.9673 
2024-12-08 12:37:34.138587: val_loss -0.8126 
2024-12-08 12:37:34.143696: Pseudo dice [np.float32(0.8918), np.float32(0.874)] 
2024-12-08 12:37:34.147259: Epoch time: 6.15 s 
2024-12-08 12:37:34.150804: Yayy! New best EMA pseudo Dice: 0.8809000253677368 
2024-12-08 12:37:34.746336:  
2024-12-08 12:37:34.751909: Epoch 32 
2024-12-08 12:37:34.755468: Current learning rate: 0.00707 
2024-12-08 12:37:40.891876: train_loss -0.9693 
2024-12-08 12:37:40.897511: val_loss -0.8073 
2024-12-08 12:37:40.900052: Pseudo dice [np.float32(0.8891), np.float32(0.8717)] 
2024-12-08 12:37:40.904610: Epoch time: 6.15 s 
2024-12-08 12:37:41.462918:  
2024-12-08 12:37:41.469013: Epoch 33 
2024-12-08 12:37:41.473106: Current learning rate: 0.00697 
2024-12-08 12:37:47.628709: train_loss -0.9698 
2024-12-08 12:37:47.634318: val_loss -0.8069 
2024-12-08 12:37:47.636876: Pseudo dice [np.float32(0.8884), np.float32(0.871)] 
2024-12-08 12:37:47.640939: Epoch time: 6.17 s 
2024-12-08 12:37:48.198386:  
2024-12-08 12:37:48.203492: Epoch 34 
2024-12-08 12:37:48.207055: Current learning rate: 0.00688 
2024-12-08 12:37:54.366753: train_loss -0.9709 
2024-12-08 12:37:54.373858: val_loss -0.8136 
2024-12-08 12:37:54.376930: Pseudo dice [np.float32(0.8923), np.float32(0.8762)] 
2024-12-08 12:37:54.379998: Epoch time: 6.17 s 
2024-12-08 12:37:54.384064: Yayy! New best EMA pseudo Dice: 0.8810999989509583 
2024-12-08 12:37:54.986798:  
2024-12-08 12:37:54.991892: Epoch 35 
2024-12-08 12:37:54.995965: Current learning rate: 0.00679 
2024-12-08 12:38:01.151769: train_loss -0.9707 
2024-12-08 12:38:01.158370: val_loss -0.8053 
2024-12-08 12:38:01.160908: Pseudo dice [np.float32(0.8895), np.float32(0.8715)] 
2024-12-08 12:38:01.164455: Epoch time: 6.17 s 
2024-12-08 12:38:01.728985:  
2024-12-08 12:38:01.733544: Epoch 36 
2024-12-08 12:38:01.736600: Current learning rate: 0.00669 
2024-12-08 12:38:07.900382: train_loss -0.9711 
2024-12-08 12:38:07.904991: val_loss -0.8073 
2024-12-08 12:38:07.908045: Pseudo dice [np.float32(0.8892), np.float32(0.8724)] 
2024-12-08 12:38:07.911587: Epoch time: 6.17 s 
2024-12-08 12:38:08.606466:  
2024-12-08 12:38:08.612582: Epoch 37 
2024-12-08 12:38:08.615641: Current learning rate: 0.0066 
2024-12-08 12:38:14.769087: train_loss -0.9725 
2024-12-08 12:38:14.774694: val_loss -0.794 
2024-12-08 12:38:14.777794: Pseudo dice [np.float32(0.8848), np.float32(0.8667)] 
2024-12-08 12:38:14.781365: Epoch time: 6.16 s 
2024-12-08 12:38:15.341807:  
2024-12-08 12:38:15.348407: Epoch 38 
2024-12-08 12:38:15.351462: Current learning rate: 0.0065 
2024-12-08 12:38:21.491906: train_loss -0.9727 
2024-12-08 12:38:21.497516: val_loss -0.8131 
2024-12-08 12:38:21.500574: Pseudo dice [np.float32(0.8939), np.float32(0.877)] 
2024-12-08 12:38:21.503660: Epoch time: 6.15 s 
2024-12-08 12:38:22.068321:  
2024-12-08 12:38:22.074471: Epoch 39 
2024-12-08 12:38:22.077020: Current learning rate: 0.00641 
2024-12-08 12:38:28.224731: train_loss -0.9735 
2024-12-08 12:38:28.230856: val_loss -0.8064 
2024-12-08 12:38:28.233925: Pseudo dice [np.float32(0.8891), np.float32(0.872)] 
2024-12-08 12:38:28.236979: Epoch time: 6.16 s 
2024-12-08 12:38:28.819497:  
2024-12-08 12:38:28.825581: Epoch 40 
2024-12-08 12:38:28.828124: Current learning rate: 0.00631 
2024-12-08 12:38:34.974183: train_loss -0.9738 
2024-12-08 12:38:34.980256: val_loss -0.8017 
2024-12-08 12:38:34.983334: Pseudo dice [np.float32(0.8887), np.float32(0.8692)] 
2024-12-08 12:38:34.985873: Epoch time: 6.15 s 
2024-12-08 12:38:35.556929:  
2024-12-08 12:38:35.562009: Epoch 41 
2024-12-08 12:38:35.564543: Current learning rate: 0.00622 
2024-12-08 12:38:41.708475: train_loss -0.9737 
2024-12-08 12:38:41.714630: val_loss -0.8044 
2024-12-08 12:38:41.717691: Pseudo dice [np.float32(0.8882), np.float32(0.872)] 
2024-12-08 12:38:41.720227: Epoch time: 6.15 s 
2024-12-08 12:38:42.261003:  
2024-12-08 12:38:42.266075: Epoch 42 
2024-12-08 12:38:42.268612: Current learning rate: 0.00612 
2024-12-08 12:38:48.417476: train_loss -0.9737 
2024-12-08 12:38:48.422056: val_loss -0.8027 
2024-12-08 12:38:48.424598: Pseudo dice [np.float32(0.8868), np.float32(0.8695)] 
2024-12-08 12:38:48.429168: Epoch time: 6.16 s 
2024-12-08 12:38:48.971768:  
2024-12-08 12:38:48.976865: Epoch 43 
2024-12-08 12:38:48.980429: Current learning rate: 0.00603 
2024-12-08 12:38:55.133533: train_loss -0.9753 
2024-12-08 12:38:55.140203: val_loss -0.8054 
2024-12-08 12:38:55.143774: Pseudo dice [np.float32(0.8886), np.float32(0.8737)] 
2024-12-08 12:38:55.146819: Epoch time: 6.16 s 
2024-12-08 12:38:55.690444:  
2024-12-08 12:38:55.696024: Epoch 44 
2024-12-08 12:38:55.699119: Current learning rate: 0.00593 
2024-12-08 12:39:01.856677: train_loss -0.9758 
2024-12-08 12:39:01.862801: val_loss -0.8056 
2024-12-08 12:39:01.865339: Pseudo dice [np.float32(0.8892), np.float32(0.8719)] 
2024-12-08 12:39:01.869430: Epoch time: 6.17 s 
2024-12-08 12:39:02.559939:  
2024-12-08 12:39:02.565025: Epoch 45 
2024-12-08 12:39:02.568579: Current learning rate: 0.00584 
2024-12-08 12:39:08.720254: train_loss -0.9769 
2024-12-08 12:39:08.726335: val_loss -0.8065 
2024-12-08 12:39:08.729397: Pseudo dice [np.float32(0.8902), np.float32(0.8719)] 
2024-12-08 12:39:08.732463: Epoch time: 6.16 s 
2024-12-08 12:39:09.268380:  
2024-12-08 12:39:09.273480: Epoch 46 
2024-12-08 12:39:09.277541: Current learning rate: 0.00574 
2024-12-08 12:39:15.427046: train_loss -0.9761 
2024-12-08 12:39:15.432703: val_loss -0.8048 
2024-12-08 12:39:15.435242: Pseudo dice [np.float32(0.8901), np.float32(0.872)] 
2024-12-08 12:39:15.438801: Epoch time: 6.16 s 
2024-12-08 12:39:15.977738:  
2024-12-08 12:39:15.982851: Epoch 47 
2024-12-08 12:39:15.986419: Current learning rate: 0.00565 
2024-12-08 12:39:22.141186: train_loss -0.9765 
2024-12-08 12:39:22.147275: val_loss -0.8053 
2024-12-08 12:39:22.150332: Pseudo dice [np.float32(0.8904), np.float32(0.8725)] 
2024-12-08 12:39:22.153382: Epoch time: 6.16 s 
2024-12-08 12:39:22.690058:  
2024-12-08 12:39:22.696205: Epoch 48 
2024-12-08 12:39:22.699748: Current learning rate: 0.00555 
2024-12-08 12:39:28.840869: train_loss -0.977 
2024-12-08 12:39:28.847437: val_loss -0.8013 
2024-12-08 12:39:28.850543: Pseudo dice [np.float32(0.8886), np.float32(0.8724)] 
2024-12-08 12:39:28.853087: Epoch time: 6.15 s 
2024-12-08 12:39:29.400208:  
2024-12-08 12:39:29.406806: Epoch 49 
2024-12-08 12:39:29.409342: Current learning rate: 0.00546 
2024-12-08 12:39:35.551339: train_loss -0.9776 
2024-12-08 12:39:35.556921: val_loss -0.7988 
2024-12-08 12:39:35.560019: Pseudo dice [np.float32(0.8865), np.float32(0.8693)] 
2024-12-08 12:39:35.565113: Epoch time: 6.15 s 
2024-12-08 12:39:36.143345:  
2024-12-08 12:39:36.148447: Epoch 50 
2024-12-08 12:39:36.152513: Current learning rate: 0.00536 
2024-12-08 12:39:42.300061: train_loss -0.9777 
2024-12-08 12:39:42.305652: val_loss -0.8023 
2024-12-08 12:39:42.308734: Pseudo dice [np.float32(0.889), np.float32(0.8724)] 
2024-12-08 12:39:42.311799: Epoch time: 6.16 s 
2024-12-08 12:39:42.863654:  
2024-12-08 12:39:42.868728: Epoch 51 
2024-12-08 12:39:42.871271: Current learning rate: 0.00526 
2024-12-08 12:39:49.027894: train_loss -0.9785 
2024-12-08 12:39:49.033989: val_loss -0.7981 
2024-12-08 12:39:49.036025: Pseudo dice [np.float32(0.8878), np.float32(0.8714)] 
2024-12-08 12:39:49.040620: Epoch time: 6.16 s 
2024-12-08 12:39:49.591359:  
2024-12-08 12:39:49.596438: Epoch 52 
2024-12-08 12:39:49.598984: Current learning rate: 0.00517 
2024-12-08 12:39:55.744736: train_loss -0.9787 
2024-12-08 12:39:55.753440: val_loss -0.8091 
2024-12-08 12:39:55.757526: Pseudo dice [np.float32(0.8924), np.float32(0.8752)] 
2024-12-08 12:39:55.760067: Epoch time: 6.15 s 
2024-12-08 12:39:56.454618:  
2024-12-08 12:39:56.460710: Epoch 53 
2024-12-08 12:39:56.463253: Current learning rate: 0.00507 
2024-12-08 12:40:02.609196: train_loss -0.9791 
2024-12-08 12:40:02.614796: val_loss -0.8032 
2024-12-08 12:40:02.617859: Pseudo dice [np.float32(0.8887), np.float32(0.8723)] 
2024-12-08 12:40:02.620926: Epoch time: 6.15 s 
2024-12-08 12:40:03.165573:  
2024-12-08 12:40:03.171704: Epoch 54 
2024-12-08 12:40:03.174746: Current learning rate: 0.00497 
2024-12-08 12:40:09.320070: train_loss -0.9802 
2024-12-08 12:40:09.325137: val_loss -0.8014 
2024-12-08 12:40:09.328182: Pseudo dice [np.float32(0.8895), np.float32(0.8721)] 
2024-12-08 12:40:09.331250: Epoch time: 6.15 s 
2024-12-08 12:40:09.884048:  
2024-12-08 12:40:09.889117: Epoch 55 
2024-12-08 12:40:09.891645: Current learning rate: 0.00487 
2024-12-08 12:40:16.036265: train_loss -0.9808 
2024-12-08 12:40:16.041864: val_loss -0.7983 
2024-12-08 12:40:16.044923: Pseudo dice [np.float32(0.8883), np.float32(0.8707)] 
2024-12-08 12:40:16.048036: Epoch time: 6.15 s 
2024-12-08 12:40:16.600055:  
2024-12-08 12:40:16.605128: Epoch 56 
2024-12-08 12:40:16.607696: Current learning rate: 0.00478 
2024-12-08 12:40:22.751005: train_loss -0.9803 
2024-12-08 12:40:22.756586: val_loss -0.798 
2024-12-08 12:40:22.759687: Pseudo dice [np.float32(0.8874), np.float32(0.8706)] 
2024-12-08 12:40:22.762791: Epoch time: 6.15 s 
2024-12-08 12:40:23.314442:  
2024-12-08 12:40:23.318535: Epoch 57 
2024-12-08 12:40:23.322595: Current learning rate: 0.00468 
2024-12-08 12:40:29.467254: train_loss -0.9806 
2024-12-08 12:40:29.472839: val_loss -0.8014 
2024-12-08 12:40:29.474874: Pseudo dice [np.float32(0.8901), np.float32(0.8721)] 
2024-12-08 12:40:29.478953: Epoch time: 6.15 s 
2024-12-08 12:40:30.029233:  
2024-12-08 12:40:30.034322: Epoch 58 
2024-12-08 12:40:30.036859: Current learning rate: 0.00458 
2024-12-08 12:40:36.181390: train_loss -0.9814 
2024-12-08 12:40:36.187999: val_loss -0.7972 
2024-12-08 12:40:36.191073: Pseudo dice [np.float32(0.8884), np.float32(0.8707)] 
2024-12-08 12:40:36.193140: Epoch time: 6.15 s 
2024-12-08 12:40:36.752686:  
2024-12-08 12:40:36.758297: Epoch 59 
2024-12-08 12:40:36.760837: Current learning rate: 0.00448 
2024-12-08 12:40:42.900088: train_loss -0.9814 
2024-12-08 12:40:42.905682: val_loss -0.7997 
2024-12-08 12:40:42.908224: Pseudo dice [np.float32(0.8896), np.float32(0.8722)] 
2024-12-08 12:40:42.910766: Epoch time: 6.15 s 
2024-12-08 12:40:43.470371:  
2024-12-08 12:40:43.476485: Epoch 60 
2024-12-08 12:40:43.479053: Current learning rate: 0.00438 
2024-12-08 12:40:49.631390: train_loss -0.9813 
2024-12-08 12:40:49.636516: val_loss -0.8041 
2024-12-08 12:40:49.640074: Pseudo dice [np.float32(0.892), np.float32(0.8739)] 
2024-12-08 12:40:49.642109: Epoch time: 6.16 s 
2024-12-08 12:40:50.205571:  
2024-12-08 12:40:50.210634: Epoch 61 
2024-12-08 12:40:50.213170: Current learning rate: 0.00429 
2024-12-08 12:40:56.359458: train_loss -0.9811 
2024-12-08 12:40:56.366036: val_loss -0.7966 
2024-12-08 12:40:56.369675: Pseudo dice [np.float32(0.8884), np.float32(0.8713)] 
2024-12-08 12:40:56.372224: Epoch time: 6.15 s 
2024-12-08 12:40:57.081871:  
2024-12-08 12:40:57.087984: Epoch 62 
2024-12-08 12:40:57.090524: Current learning rate: 0.00419 
2024-12-08 12:41:03.234136: train_loss -0.9811 
2024-12-08 12:41:03.239771: val_loss -0.8032 
2024-12-08 12:41:03.242843: Pseudo dice [np.float32(0.89), np.float32(0.8737)] 
2024-12-08 12:41:03.244876: Epoch time: 6.15 s 
2024-12-08 12:41:03.801968:  
2024-12-08 12:41:03.807585: Epoch 63 
2024-12-08 12:41:03.810643: Current learning rate: 0.00409 
2024-12-08 12:41:09.945263: train_loss -0.9823 
2024-12-08 12:41:09.949319: val_loss -0.8019 
2024-12-08 12:41:09.953902: Pseudo dice [np.float32(0.8904), np.float32(0.8738)] 
2024-12-08 12:41:09.956994: Epoch time: 6.14 s 
2024-12-08 12:41:10.513833:  
2024-12-08 12:41:10.519439: Epoch 64 
2024-12-08 12:41:10.522504: Current learning rate: 0.00399 
2024-12-08 12:41:16.657086: train_loss -0.9822 
2024-12-08 12:41:16.662183: val_loss -0.8011 
2024-12-08 12:41:16.664725: Pseudo dice [np.float32(0.8899), np.float32(0.8738)] 
2024-12-08 12:41:16.668777: Epoch time: 6.14 s 
2024-12-08 12:41:17.228951:  
2024-12-08 12:41:17.234070: Epoch 65 
2024-12-08 12:41:17.237135: Current learning rate: 0.00389 
2024-12-08 12:41:23.365568: train_loss -0.9826 
2024-12-08 12:41:23.370661: val_loss -0.7937 
2024-12-08 12:41:23.373202: Pseudo dice [np.float32(0.8862), np.float32(0.8696)] 
2024-12-08 12:41:23.377249: Epoch time: 6.14 s 
2024-12-08 12:41:23.935770:  
2024-12-08 12:41:23.941361: Epoch 66 
2024-12-08 12:41:23.943897: Current learning rate: 0.00379 
2024-12-08 12:41:30.070817: train_loss -0.9824 
2024-12-08 12:41:30.075911: val_loss -0.7984 
2024-12-08 12:41:30.079490: Pseudo dice [np.float32(0.8902), np.float32(0.8719)] 
2024-12-08 12:41:30.082040: Epoch time: 6.14 s 
2024-12-08 12:41:30.639741:  
2024-12-08 12:41:30.644829: Epoch 67 
2024-12-08 12:41:30.648389: Current learning rate: 0.00369 
2024-12-08 12:41:36.780046: train_loss -0.9831 
2024-12-08 12:41:36.785156: val_loss -0.7956 
2024-12-08 12:41:36.788220: Pseudo dice [np.float32(0.8883), np.float32(0.87)] 
2024-12-08 12:41:36.791769: Epoch time: 6.14 s 
2024-12-08 12:41:37.355863:  
2024-12-08 12:41:37.360945: Epoch 68 
2024-12-08 12:41:37.366073: Current learning rate: 0.00359 
2024-12-08 12:41:43.502016: train_loss -0.9834 
2024-12-08 12:41:43.507605: val_loss -0.8006 
2024-12-08 12:41:43.510143: Pseudo dice [np.float32(0.889), np.float32(0.8727)] 
2024-12-08 12:41:43.513685: Epoch time: 6.15 s 
2024-12-08 12:41:44.076016:  
2024-12-08 12:41:44.081621: Epoch 69 
2024-12-08 12:41:44.084162: Current learning rate: 0.00349 
2024-12-08 12:41:50.215290: train_loss -0.9834 
2024-12-08 12:41:50.220879: val_loss -0.7993 
2024-12-08 12:41:50.223931: Pseudo dice [np.float32(0.8893), np.float32(0.8728)] 
2024-12-08 12:41:50.226475: Epoch time: 6.14 s 
2024-12-08 12:41:50.936263:  
2024-12-08 12:41:50.941345: Epoch 70 
2024-12-08 12:41:50.946474: Current learning rate: 0.00338 
2024-12-08 12:41:57.088316: train_loss -0.9844 
2024-12-08 12:41:57.093383: val_loss -0.7952 
2024-12-08 12:41:57.096932: Pseudo dice [np.float32(0.8878), np.float32(0.8717)] 
2024-12-08 12:41:57.100068: Epoch time: 6.15 s 
2024-12-08 12:41:57.669795:  
2024-12-08 12:41:57.675404: Epoch 71 
2024-12-08 12:41:57.677945: Current learning rate: 0.00328 
2024-12-08 12:42:03.821124: train_loss -0.984 
2024-12-08 12:42:03.826743: val_loss -0.7934 
2024-12-08 12:42:03.829812: Pseudo dice [np.float32(0.8872), np.float32(0.8704)] 
2024-12-08 12:42:03.832865: Epoch time: 6.15 s 
2024-12-08 12:42:04.398780:  
2024-12-08 12:42:04.405395: Epoch 72 
2024-12-08 12:42:04.409045: Current learning rate: 0.00318 
2024-12-08 12:42:10.550776: train_loss -0.9837 
2024-12-08 12:42:10.555879: val_loss -0.7899 
2024-12-08 12:42:10.559940: Pseudo dice [np.float32(0.8851), np.float32(0.8676)] 
2024-12-08 12:42:10.562478: Epoch time: 6.15 s 
2024-12-08 12:42:11.128590:  
2024-12-08 12:42:11.133666: Epoch 73 
2024-12-08 12:42:11.136204: Current learning rate: 0.00308 
2024-12-08 12:42:17.266196: train_loss -0.9839 
2024-12-08 12:42:17.271330: val_loss -0.7948 
2024-12-08 12:42:17.273862: Pseudo dice [np.float32(0.8871), np.float32(0.8702)] 
2024-12-08 12:42:17.277412: Epoch time: 6.14 s 
2024-12-08 12:42:17.847496:  
2024-12-08 12:42:17.852614: Epoch 74 
2024-12-08 12:42:17.855164: Current learning rate: 0.00297 
2024-12-08 12:42:23.988020: train_loss -0.9847 
2024-12-08 12:42:23.993611: val_loss -0.7987 
2024-12-08 12:42:23.996732: Pseudo dice [np.float32(0.8892), np.float32(0.8723)] 
2024-12-08 12:42:23.999799: Epoch time: 6.14 s 
2024-12-08 12:42:24.568042:  
2024-12-08 12:42:24.574633: Epoch 75 
2024-12-08 12:42:24.577200: Current learning rate: 0.00287 
2024-12-08 12:42:30.712759: train_loss -0.9843 
2024-12-08 12:42:30.717367: val_loss -0.7994 
2024-12-08 12:42:30.720935: Pseudo dice [np.float32(0.8894), np.float32(0.8734)] 
2024-12-08 12:42:30.724484: Epoch time: 6.14 s 
2024-12-08 12:42:31.297515:  
2024-12-08 12:42:31.304130: Epoch 76 
2024-12-08 12:42:31.307207: Current learning rate: 0.00277 
2024-12-08 12:42:37.435109: train_loss -0.9853 
2024-12-08 12:42:37.440225: val_loss -0.7961 
2024-12-08 12:42:37.445302: Pseudo dice [np.float32(0.8884), np.float32(0.8714)] 
2024-12-08 12:42:37.448362: Epoch time: 6.14 s 
2024-12-08 12:42:38.155513:  
2024-12-08 12:42:38.160586: Epoch 77 
2024-12-08 12:42:38.163129: Current learning rate: 0.00266 
2024-12-08 12:42:44.278396: train_loss -0.9843 
2024-12-08 12:42:44.284537: val_loss -0.7953 
2024-12-08 12:42:44.286569: Pseudo dice [np.float32(0.8887), np.float32(0.8715)] 
2024-12-08 12:42:44.291140: Epoch time: 6.12 s 
2024-12-08 12:42:44.867702:  
2024-12-08 12:42:44.872774: Epoch 78 
2024-12-08 12:42:44.875316: Current learning rate: 0.00256 
2024-12-08 12:42:51.008393: train_loss -0.9846 
2024-12-08 12:42:51.015488: val_loss -0.7993 
2024-12-08 12:42:51.019617: Pseudo dice [np.float32(0.8895), np.float32(0.8718)] 
2024-12-08 12:42:51.023171: Epoch time: 6.14 s 
2024-12-08 12:42:51.601452:  
2024-12-08 12:42:51.608057: Epoch 79 
2024-12-08 12:42:51.612153: Current learning rate: 0.00245 
2024-12-08 12:42:57.745389: train_loss -0.9856 
2024-12-08 12:42:57.752490: val_loss -0.7991 
2024-12-08 12:42:57.755549: Pseudo dice [np.float32(0.891), np.float32(0.8728)] 
2024-12-08 12:42:57.759105: Epoch time: 6.14 s 
2024-12-08 12:42:58.335964:  
2024-12-08 12:42:58.341549: Epoch 80 
2024-12-08 12:42:58.344114: Current learning rate: 0.00235 
2024-12-08 12:43:04.489519: train_loss -0.9854 
2024-12-08 12:43:04.495640: val_loss -0.7938 
2024-12-08 12:43:04.498694: Pseudo dice [np.float32(0.8882), np.float32(0.87)] 
2024-12-08 12:43:04.502247: Epoch time: 6.15 s 
2024-12-08 12:43:05.080005:  
2024-12-08 12:43:05.085077: Epoch 81 
2024-12-08 12:43:05.087605: Current learning rate: 0.00224 
2024-12-08 12:43:11.231844: train_loss -0.9851 
2024-12-08 12:43:11.237454: val_loss -0.7983 
2024-12-08 12:43:11.241033: Pseudo dice [np.float32(0.8893), np.float32(0.8727)] 
2024-12-08 12:43:11.243581: Epoch time: 6.15 s 
2024-12-08 12:43:11.821782:  
2024-12-08 12:43:11.827860: Epoch 82 
2024-12-08 12:43:11.830904: Current learning rate: 0.00214 
2024-12-08 12:43:17.971744: train_loss -0.9854 
2024-12-08 12:43:17.976822: val_loss -0.7968 
2024-12-08 12:43:17.979364: Pseudo dice [np.float32(0.8899), np.float32(0.8725)] 
2024-12-08 12:43:17.982424: Epoch time: 6.15 s 
2024-12-08 12:43:18.539339:  
2024-12-08 12:43:18.545434: Epoch 83 
2024-12-08 12:43:18.548485: Current learning rate: 0.00203 
2024-12-08 12:43:24.693323: train_loss -0.9861 
2024-12-08 12:43:24.698912: val_loss -0.7994 
2024-12-08 12:43:24.701452: Pseudo dice [np.float32(0.8907), np.float32(0.8734)] 
2024-12-08 12:43:24.703988: Epoch time: 6.15 s 
2024-12-08 12:43:25.252673:  
2024-12-08 12:43:25.257764: Epoch 84 
2024-12-08 12:43:25.260814: Current learning rate: 0.00192 
2024-12-08 12:43:31.408402: train_loss -0.986 
2024-12-08 12:43:31.414002: val_loss -0.7922 
2024-12-08 12:43:31.416538: Pseudo dice [np.float32(0.8874), np.float32(0.8707)] 
2024-12-08 12:43:31.420613: Epoch time: 6.16 s 
2024-12-08 12:43:32.124357:  
2024-12-08 12:43:32.129440: Epoch 85 
2024-12-08 12:43:32.132512: Current learning rate: 0.00181 
2024-12-08 12:43:38.279212: train_loss -0.986 
2024-12-08 12:43:38.284300: val_loss -0.792 
2024-12-08 12:43:38.286415: Pseudo dice [np.float32(0.8864), np.float32(0.8693)] 
2024-12-08 12:43:38.291498: Epoch time: 6.16 s 
2024-12-08 12:43:38.838923:  
2024-12-08 12:43:38.844559: Epoch 86 
2024-12-08 12:43:38.847102: Current learning rate: 0.0017 
2024-12-08 12:43:44.989319: train_loss -0.986 
2024-12-08 12:43:44.993893: val_loss -0.7954 
2024-12-08 12:43:44.997955: Pseudo dice [np.float32(0.8887), np.float32(0.8708)] 
2024-12-08 12:43:45.001522: Epoch time: 6.15 s 
2024-12-08 12:43:45.541624:  
2024-12-08 12:43:45.546700: Epoch 87 
2024-12-08 12:43:45.550793: Current learning rate: 0.00159 
2024-12-08 12:43:51.686816: train_loss -0.9871 
2024-12-08 12:43:51.694513: val_loss -0.792 
2024-12-08 12:43:51.699113: Pseudo dice [np.float32(0.8875), np.float32(0.8698)] 
2024-12-08 12:43:51.702181: Epoch time: 6.15 s 
2024-12-08 12:43:52.251839:  
2024-12-08 12:43:52.256941: Epoch 88 
2024-12-08 12:43:52.259486: Current learning rate: 0.00148 
2024-12-08 12:43:58.400134: train_loss -0.9862 
2024-12-08 12:43:58.405712: val_loss -0.7969 
2024-12-08 12:43:58.409788: Pseudo dice [np.float32(0.8901), np.float32(0.8728)] 
2024-12-08 12:43:58.412835: Epoch time: 6.15 s 
2024-12-08 12:43:58.954398:  
2024-12-08 12:43:58.959503: Epoch 89 
2024-12-08 12:43:58.962046: Current learning rate: 0.00137 
2024-12-08 12:44:05.113357: train_loss -0.9869 
2024-12-08 12:44:05.118944: val_loss -0.7948 
2024-12-08 12:44:05.122044: Pseudo dice [np.float32(0.8896), np.float32(0.8704)] 
2024-12-08 12:44:05.124600: Epoch time: 6.16 s 
2024-12-08 12:44:05.662457:  
2024-12-08 12:44:05.668579: Epoch 90 
2024-12-08 12:44:05.671129: Current learning rate: 0.00126 
2024-12-08 12:44:11.836370: train_loss -0.9863 
2024-12-08 12:44:11.841954: val_loss -0.7922 
2024-12-08 12:44:11.845018: Pseudo dice [np.float32(0.8874), np.float32(0.8702)] 
2024-12-08 12:44:11.848084: Epoch time: 6.17 s 
2024-12-08 12:44:12.391794:  
2024-12-08 12:44:12.396948: Epoch 91 
2024-12-08 12:44:12.401014: Current learning rate: 0.00115 
2024-12-08 12:44:18.548081: train_loss -0.9862 
2024-12-08 12:44:18.555159: val_loss -0.7916 
2024-12-08 12:44:18.558761: Pseudo dice [np.float32(0.8874), np.float32(0.8694)] 
2024-12-08 12:44:18.560794: Epoch time: 6.16 s 
2024-12-08 12:44:19.101115:  
2024-12-08 12:44:19.105674: Epoch 92 
2024-12-08 12:44:19.108765: Current learning rate: 0.00103 
2024-12-08 12:44:25.275242: train_loss -0.9864 
2024-12-08 12:44:25.280353: val_loss -0.7908 
2024-12-08 12:44:25.283952: Pseudo dice [np.float32(0.8858), np.float32(0.8691)] 
2024-12-08 12:44:25.286515: Epoch time: 6.17 s 
2024-12-08 12:44:25.823104:  
2024-12-08 12:44:25.829701: Epoch 93 
2024-12-08 12:44:25.832252: Current learning rate: 0.00091 
2024-12-08 12:44:31.986571: train_loss -0.9869 
2024-12-08 12:44:31.992182: val_loss -0.797 
2024-12-08 12:44:31.994745: Pseudo dice [np.float32(0.8892), np.float32(0.8714)] 
2024-12-08 12:44:31.997292: Epoch time: 6.16 s 
2024-12-08 12:44:32.691547:  
2024-12-08 12:44:32.697712: Epoch 94 
2024-12-08 12:44:32.700260: Current learning rate: 0.00079 
2024-12-08 12:44:38.849165: train_loss -0.9867 
2024-12-08 12:44:38.853748: val_loss -0.7926 
2024-12-08 12:44:38.858835: Pseudo dice [np.float32(0.8879), np.float32(0.871)] 
2024-12-08 12:44:38.861905: Epoch time: 6.16 s 
2024-12-08 12:44:39.401433:  
2024-12-08 12:44:39.407075: Epoch 95 
2024-12-08 12:44:39.410135: Current learning rate: 0.00067 
2024-12-08 12:44:45.554143: train_loss -0.987 
2024-12-08 12:44:45.559221: val_loss -0.7916 
2024-12-08 12:44:45.562804: Pseudo dice [np.float32(0.8879), np.float32(0.8704)] 
2024-12-08 12:44:45.565857: Epoch time: 6.15 s 
2024-12-08 12:44:46.109760:  
2024-12-08 12:44:46.115453: Epoch 96 
2024-12-08 12:44:46.118003: Current learning rate: 0.00055 
2024-12-08 12:44:52.274448: train_loss -0.9866 
2024-12-08 12:44:52.281586: val_loss -0.7964 
2024-12-08 12:44:52.286666: Pseudo dice [np.float32(0.889), np.float32(0.8727)] 
2024-12-08 12:44:52.289263: Epoch time: 6.16 s 
2024-12-08 12:44:52.842058:  
2024-12-08 12:44:52.847120: Epoch 97 
2024-12-08 12:44:52.849656: Current learning rate: 0.00043 
2024-12-08 12:44:59.000501: train_loss -0.9873 
2024-12-08 12:44:59.005577: val_loss -0.7926 
2024-12-08 12:44:59.009136: Pseudo dice [np.float32(0.8874), np.float32(0.8688)] 
2024-12-08 12:44:59.012189: Epoch time: 6.16 s 
2024-12-08 12:44:59.563835:  
2024-12-08 12:44:59.568920: Epoch 98 
2024-12-08 12:44:59.571461: Current learning rate: 0.0003 
2024-12-08 12:45:05.733100: train_loss -0.9869 
2024-12-08 12:45:05.739205: val_loss -0.7922 
2024-12-08 12:45:05.741239: Pseudo dice [np.float32(0.887), np.float32(0.8704)] 
2024-12-08 12:45:05.745298: Epoch time: 6.17 s 
2024-12-08 12:45:06.298178:  
2024-12-08 12:45:06.304281: Epoch 99 
2024-12-08 12:45:06.306814: Current learning rate: 0.00016 
2024-12-08 12:45:12.467044: train_loss -0.987 
2024-12-08 12:45:12.472114: val_loss -0.7957 
2024-12-08 12:45:12.476188: Pseudo dice [np.float32(0.8876), np.float32(0.8716)] 
2024-12-08 12:45:12.479237: Epoch time: 6.17 s 
2024-12-08 12:45:13.079010: Training done. 
2024-12-08 12:45:13.119878: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2024-12-08 12:45:13.119878: The split file contains 5 splits. 
2024-12-08 12:45:13.131769: Desired fold for training: 0 
2024-12-08 12:45:13.136142: This split has 208 training and 52 validation cases. 
2024-12-08 12:45:13.136142: predicting hippocampus_017 
2024-12-08 12:45:13.144224: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2024-12-08 12:45:13.234090: predicting hippocampus_019 
2024-12-08 12:45:13.242128: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2024-12-08 12:45:13.250181: predicting hippocampus_033 
2024-12-08 12:45:13.258502: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2024-12-08 12:45:13.266672: predicting hippocampus_035 
2024-12-08 12:45:13.266672: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2024-12-08 12:45:13.283341: predicting hippocampus_037 
2024-12-08 12:45:13.283341: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2024-12-08 12:45:13.291353: predicting hippocampus_049 
2024-12-08 12:45:13.299387: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2024-12-08 12:45:13.307625: predicting hippocampus_052 
2024-12-08 12:45:13.315921: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2024-12-08 12:45:13.323853: predicting hippocampus_065 
2024-12-08 12:45:13.332048: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2024-12-08 12:45:13.340421: predicting hippocampus_083 
2024-12-08 12:45:13.348760: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2024-12-08 12:45:13.357008: predicting hippocampus_088 
2024-12-08 12:45:13.365083: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2024-12-08 12:45:17.094440: predicting hippocampus_090 
2024-12-08 12:45:17.128221: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2024-12-08 12:45:17.142237: predicting hippocampus_092 
2024-12-08 12:45:17.148243: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2024-12-08 12:45:17.162260: predicting hippocampus_095 
2024-12-08 12:45:17.168267: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2024-12-08 12:45:17.184284: predicting hippocampus_107 
2024-12-08 12:45:17.188289: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2024-12-08 12:45:17.206310: predicting hippocampus_108 
2024-12-08 12:45:17.212317: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2024-12-08 12:45:17.226333: predicting hippocampus_123 
2024-12-08 12:45:17.232342: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2024-12-08 12:45:17.248361: predicting hippocampus_125 
2024-12-08 12:45:17.252365: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2024-12-08 12:45:17.276393: predicting hippocampus_157 
2024-12-08 12:45:17.280399: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2024-12-08 12:45:17.298420: predicting hippocampus_164 
2024-12-08 12:45:17.302426: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2024-12-08 12:45:17.340470: predicting hippocampus_169 
2024-12-08 12:45:17.346477: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2024-12-08 12:45:17.360493: predicting hippocampus_175 
2024-12-08 12:45:17.366501: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2024-12-08 12:45:17.382521: predicting hippocampus_185 
2024-12-08 12:45:17.386525: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2024-12-08 12:45:17.402542: predicting hippocampus_190 
2024-12-08 12:45:17.406548: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2024-12-08 12:45:17.420567: predicting hippocampus_194 
2024-12-08 12:45:17.426573: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2024-12-08 12:45:17.442592: predicting hippocampus_204 
2024-12-08 12:45:17.448601: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2024-12-08 12:45:17.462617: predicting hippocampus_205 
2024-12-08 12:45:17.468625: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2024-12-08 12:45:17.482644: predicting hippocampus_210 
2024-12-08 12:45:17.488650: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2024-12-08 12:45:17.502666: predicting hippocampus_217 
2024-12-08 12:45:17.506673: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2024-12-08 12:45:17.524696: predicting hippocampus_219 
2024-12-08 12:45:17.528700: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2024-12-08 12:45:17.542716: predicting hippocampus_229 
2024-12-08 12:45:17.546721: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2024-12-08 12:45:17.562740: predicting hippocampus_244 
2024-12-08 12:45:17.568747: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2024-12-08 12:45:17.582765: predicting hippocampus_261 
2024-12-08 12:45:17.586771: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2024-12-08 12:45:17.608796: predicting hippocampus_264 
2024-12-08 12:45:17.612800: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2024-12-08 12:45:17.626816: predicting hippocampus_277 
2024-12-08 12:45:17.630820: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2024-12-08 12:45:17.654847: predicting hippocampus_280 
2024-12-08 12:45:17.658852: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2024-12-08 12:45:17.674870: predicting hippocampus_286 
2024-12-08 12:45:17.678875: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2024-12-08 12:45:17.698896: predicting hippocampus_288 
2024-12-08 12:45:17.702900: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2024-12-08 12:45:17.722924: predicting hippocampus_289 
2024-12-08 12:45:17.728931: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2024-12-08 12:45:17.742948: predicting hippocampus_296 
2024-12-08 12:45:17.746952: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2024-12-08 12:45:17.760968: predicting hippocampus_305 
2024-12-08 12:45:17.766978: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2024-12-08 12:45:17.780997: predicting hippocampus_308 
2024-12-08 12:45:17.785001: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2024-12-08 12:45:17.801020: predicting hippocampus_317 
2024-12-08 12:45:17.805026: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2024-12-08 12:45:17.819042: predicting hippocampus_327 
2024-12-08 12:45:17.823046: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2024-12-08 12:45:17.837064: predicting hippocampus_330 
2024-12-08 12:45:17.843073: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2024-12-08 12:45:17.855088: predicting hippocampus_332 
2024-12-08 12:45:17.861094: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2024-12-08 12:45:17.875114: predicting hippocampus_338 
2024-12-08 12:45:17.879118: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2024-12-08 12:45:17.899140: predicting hippocampus_349 
2024-12-08 12:45:17.903147: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2024-12-08 12:45:17.917167: predicting hippocampus_350 
2024-12-08 12:45:17.923174: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2024-12-08 12:45:17.937190: predicting hippocampus_356 
2024-12-08 12:45:17.943197: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2024-12-08 12:45:17.959217: predicting hippocampus_358 
2024-12-08 12:45:17.965225: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2024-12-08 12:45:17.979240: predicting hippocampus_374 
2024-12-08 12:45:17.983245: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2024-12-08 12:45:17.999263: predicting hippocampus_394 
2024-12-08 12:45:18.005274: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2024-12-08 12:45:21.471240: Validation complete 
2024-12-08 12:45:21.479600: Mean Validation Dice:  0.8785993064506619 
