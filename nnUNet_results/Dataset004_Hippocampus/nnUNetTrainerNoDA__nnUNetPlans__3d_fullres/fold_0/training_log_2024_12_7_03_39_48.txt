
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-12-07 03:39:48.204166: do_dummy_2d_data_aug: False 
2024-12-07 03:39:48.208166: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2024-12-07 03:39:48.214033: The split file contains 5 splits. 
2024-12-07 03:39:48.216165: Desired fold for training: 0 
2024-12-07 03:39:48.219033: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2024-12-07 03:39:53.808372: unpacking dataset... 
2024-12-07 03:39:54.612668: unpacking done... 
2024-12-07 03:39:56.352247:  
2024-12-07 03:39:56.357258: Epoch 0 
2024-12-07 03:39:56.359767: Current learning rate: 0.01 
2024-12-07 03:40:03.417302: train_loss -0.6283 
2024-12-07 03:40:03.423364: val_loss -0.8219 
2024-12-07 03:40:03.425882: Pseudo dice [np.float32(0.8724), np.float32(0.8653)] 
2024-12-07 03:40:03.428912: Epoch time: 7.07 s 
2024-12-07 03:40:03.431446: Yayy! New best EMA pseudo Dice: 0.8687999844551086 
2024-12-07 03:40:03.962294:  
2024-12-07 03:40:03.968377: Epoch 1 
2024-12-07 03:40:03.971931: Current learning rate: 0.00991 
2024-12-07 03:40:10.139920: train_loss -0.8525 
2024-12-07 03:40:10.145324: val_loss -0.8366 
2024-12-07 03:40:10.148837: Pseudo dice [np.float32(0.8855), np.float32(0.873)] 
2024-12-07 03:40:10.151347: Epoch time: 6.18 s 
2024-12-07 03:40:10.154862: Yayy! New best EMA pseudo Dice: 0.8698999881744385 
2024-12-07 03:40:10.741179:  
2024-12-07 03:40:10.745997: Epoch 2 
2024-12-07 03:40:10.749510: Current learning rate: 0.00982 
2024-12-07 03:40:16.945961: train_loss -0.8775 
2024-12-07 03:40:16.951572: val_loss -0.8376 
2024-12-07 03:40:16.954650: Pseudo dice [np.float32(0.8904), np.float32(0.8754)] 
2024-12-07 03:40:16.957185: Epoch time: 6.21 s 
2024-12-07 03:40:16.959749: Yayy! New best EMA pseudo Dice: 0.8712000250816345 
2024-12-07 03:40:17.576241:  
2024-12-07 03:40:17.580752: Epoch 3 
2024-12-07 03:40:17.583765: Current learning rate: 0.00973 
2024-12-07 03:40:23.745707: train_loss -0.8931 
2024-12-07 03:40:23.751867: val_loss -0.8368 
2024-12-07 03:40:23.754380: Pseudo dice [np.float32(0.8901), np.float32(0.8764)] 
2024-12-07 03:40:23.756892: Epoch time: 6.17 s 
2024-12-07 03:40:23.760409: Yayy! New best EMA pseudo Dice: 0.8723999857902527 
2024-12-07 03:40:24.353058:  
2024-12-07 03:40:24.357627: Epoch 4 
2024-12-07 03:40:24.360725: Current learning rate: 0.00964 
2024-12-07 03:40:30.519910: train_loss -0.9058 
2024-12-07 03:40:30.525015: val_loss -0.8417 
2024-12-07 03:40:30.528065: Pseudo dice [np.float32(0.8966), np.float32(0.879)] 
2024-12-07 03:40:30.531093: Epoch time: 6.17 s 
2024-12-07 03:40:30.534157: Yayy! New best EMA pseudo Dice: 0.8738999962806702 
2024-12-07 03:40:31.267596:  
2024-12-07 03:40:31.273186: Epoch 5 
2024-12-07 03:40:31.276238: Current learning rate: 0.00955 
2024-12-07 03:40:37.437196: train_loss -0.9144 
2024-12-07 03:40:37.443289: val_loss -0.8371 
2024-12-07 03:40:37.445816: Pseudo dice [np.float32(0.8933), np.float32(0.8765)] 
2024-12-07 03:40:37.449353: Epoch time: 6.17 s 
2024-12-07 03:40:37.451904: Yayy! New best EMA pseudo Dice: 0.875 
2024-12-07 03:40:38.038708:  
2024-12-07 03:40:38.043721: Epoch 6 
2024-12-07 03:40:38.046228: Current learning rate: 0.00946 
2024-12-07 03:40:44.213134: train_loss -0.919 
2024-12-07 03:40:44.217856: val_loss -0.8365 
2024-12-07 03:40:44.220877: Pseudo dice [np.float32(0.8933), np.float32(0.8765)] 
2024-12-07 03:40:44.223424: Epoch time: 6.17 s 
2024-12-07 03:40:44.226466: Yayy! New best EMA pseudo Dice: 0.8759999871253967 
2024-12-07 03:40:44.837691:  
2024-12-07 03:40:44.843282: Epoch 7 
2024-12-07 03:40:44.845846: Current learning rate: 0.00937 
2024-12-07 03:40:51.016962: train_loss -0.9254 
2024-12-07 03:40:51.023035: val_loss -0.8292 
2024-12-07 03:40:51.025587: Pseudo dice [np.float32(0.8901), np.float32(0.8736)] 
2024-12-07 03:40:51.028609: Epoch time: 6.18 s 
2024-12-07 03:40:51.031649: Yayy! New best EMA pseudo Dice: 0.8766000270843506 
2024-12-07 03:40:51.634303:  
2024-12-07 03:40:51.639869: Epoch 8 
2024-12-07 03:40:51.642918: Current learning rate: 0.00928 
2024-12-07 03:40:57.820781: train_loss -0.9302 
2024-12-07 03:40:57.826362: val_loss -0.8341 
2024-12-07 03:40:57.829911: Pseudo dice [np.float32(0.8944), np.float32(0.8775)] 
2024-12-07 03:40:57.832980: Epoch time: 6.19 s 
2024-12-07 03:40:57.835534: Yayy! New best EMA pseudo Dice: 0.8774999976158142 
2024-12-07 03:40:58.465127:  
2024-12-07 03:40:58.470689: Epoch 9 
2024-12-07 03:40:58.473769: Current learning rate: 0.00919 
2024-12-07 03:41:04.640189: train_loss -0.9346 
2024-12-07 03:41:04.645208: val_loss -0.8239 
2024-12-07 03:41:04.648725: Pseudo dice [np.float32(0.8892), np.float32(0.8729)] 
2024-12-07 03:41:04.651374: Epoch time: 6.18 s 
2024-12-07 03:41:04.653917: Yayy! New best EMA pseudo Dice: 0.8779000043869019 
2024-12-07 03:41:05.241672:  
2024-12-07 03:41:05.246702: Epoch 10 
2024-12-07 03:41:05.249021: Current learning rate: 0.0091 
2024-12-07 03:41:11.423369: train_loss -0.9381 
2024-12-07 03:41:11.428403: val_loss -0.8299 
2024-12-07 03:41:11.431543: Pseudo dice [np.float32(0.8931), np.float32(0.875)] 
2024-12-07 03:41:11.434568: Epoch time: 6.18 s 
2024-12-07 03:41:11.437095: Yayy! New best EMA pseudo Dice: 0.8784999847412109 
2024-12-07 03:41:12.026923:  
2024-12-07 03:41:12.031942: Epoch 11 
2024-12-07 03:41:12.034164: Current learning rate: 0.009 
2024-12-07 03:41:18.196185: train_loss -0.9408 
2024-12-07 03:41:18.201260: val_loss -0.8215 
2024-12-07 03:41:18.204819: Pseudo dice [np.float32(0.8891), np.float32(0.8725)] 
2024-12-07 03:41:18.207908: Epoch time: 6.17 s 
2024-12-07 03:41:18.210457: Yayy! New best EMA pseudo Dice: 0.8787000179290771 
2024-12-07 03:41:18.806472:  
2024-12-07 03:41:18.811510: Epoch 12 
2024-12-07 03:41:18.814142: Current learning rate: 0.00891 
2024-12-07 03:41:24.967806: train_loss -0.9449 
2024-12-07 03:41:24.972890: val_loss -0.8263 
2024-12-07 03:41:24.976951: Pseudo dice [np.float32(0.8921), np.float32(0.8769)] 
2024-12-07 03:41:24.980012: Epoch time: 6.16 s 
2024-12-07 03:41:24.983051: Yayy! New best EMA pseudo Dice: 0.8792999982833862 
2024-12-07 03:41:25.730988:  
2024-12-07 03:41:25.736041: Epoch 13 
2024-12-07 03:41:25.739074: Current learning rate: 0.00882 
2024-12-07 03:41:31.903239: train_loss -0.9482 
2024-12-07 03:41:31.908296: val_loss -0.8212 
2024-12-07 03:41:31.911335: Pseudo dice [np.float32(0.89), np.float32(0.874)] 
2024-12-07 03:41:31.913860: Epoch time: 6.17 s 
2024-12-07 03:41:31.917398: Yayy! New best EMA pseudo Dice: 0.8795999884605408 
2024-12-07 03:41:32.582919:  
2024-12-07 03:41:32.587931: Epoch 14 
2024-12-07 03:41:32.591445: Current learning rate: 0.00873 
2024-12-07 03:41:38.891555: train_loss -0.9506 
2024-12-07 03:41:38.897624: val_loss -0.826 
2024-12-07 03:41:38.900632: Pseudo dice [np.float32(0.8938), np.float32(0.8768)] 
2024-12-07 03:41:38.904148: Epoch time: 6.31 s 
2024-12-07 03:41:38.906659: Yayy! New best EMA pseudo Dice: 0.8801000118255615 
2024-12-07 03:41:39.672200:  
2024-12-07 03:41:39.677224: Epoch 15 
2024-12-07 03:41:39.679736: Current learning rate: 0.00864 
2024-12-07 03:41:45.989075: train_loss -0.9517 
2024-12-07 03:41:45.994622: val_loss -0.82 
2024-12-07 03:41:45.997124: Pseudo dice [np.float32(0.8906), np.float32(0.8725)] 
2024-12-07 03:41:46.001140: Epoch time: 6.32 s 
2024-12-07 03:41:46.003647: Yayy! New best EMA pseudo Dice: 0.880299985408783 
2024-12-07 03:41:46.818079:  
2024-12-07 03:41:46.823672: Epoch 16 
2024-12-07 03:41:46.826221: Current learning rate: 0.00855 
2024-12-07 03:41:53.000580: train_loss -0.9544 
2024-12-07 03:41:53.006172: val_loss -0.8171 
2024-12-07 03:41:53.009249: Pseudo dice [np.float32(0.8897), np.float32(0.8732)] 
2024-12-07 03:41:53.011813: Epoch time: 6.18 s 
2024-12-07 03:41:53.014855: Yayy! New best EMA pseudo Dice: 0.8804000020027161 
2024-12-07 03:41:53.655648:  
2024-12-07 03:41:53.660662: Epoch 17 
2024-12-07 03:41:53.663674: Current learning rate: 0.00846 
2024-12-07 03:41:59.814098: train_loss -0.9556 
2024-12-07 03:41:59.819683: val_loss -0.8146 
2024-12-07 03:41:59.822748: Pseudo dice [np.float32(0.8886), np.float32(0.8722)] 
2024-12-07 03:41:59.826291: Epoch time: 6.16 s 
2024-12-07 03:41:59.829875: Yayy! New best EMA pseudo Dice: 0.8804000020027161 
2024-12-07 03:42:00.442023:  
2024-12-07 03:42:00.448055: Epoch 18 
2024-12-07 03:42:00.450562: Current learning rate: 0.00836 
2024-12-07 03:42:06.617452: train_loss -0.9575 
2024-12-07 03:42:06.623083: val_loss -0.8181 
2024-12-07 03:42:06.626592: Pseudo dice [np.float32(0.8908), np.float32(0.8738)] 
2024-12-07 03:42:06.629606: Epoch time: 6.18 s 
2024-12-07 03:42:06.632654: Yayy! New best EMA pseudo Dice: 0.8805999755859375 
2024-12-07 03:42:07.255461:  
2024-12-07 03:42:07.259991: Epoch 19 
2024-12-07 03:42:07.263022: Current learning rate: 0.00827 
2024-12-07 03:42:13.432692: train_loss -0.958 
2024-12-07 03:42:13.438259: val_loss -0.8153 
2024-12-07 03:42:13.441299: Pseudo dice [np.float32(0.8887), np.float32(0.8728)] 
2024-12-07 03:42:13.443827: Epoch time: 6.18 s 
2024-12-07 03:42:13.447363: Yayy! New best EMA pseudo Dice: 0.8805999755859375 
2024-12-07 03:42:14.204413:  
2024-12-07 03:42:14.209468: Epoch 20 
2024-12-07 03:42:14.212534: Current learning rate: 0.00818 
2024-12-07 03:42:20.395192: train_loss -0.9593 
2024-12-07 03:42:20.400251: val_loss -0.8184 
2024-12-07 03:42:20.403343: Pseudo dice [np.float32(0.8917), np.float32(0.876)] 
2024-12-07 03:42:20.406395: Epoch time: 6.19 s 
2024-12-07 03:42:20.409454: Yayy! New best EMA pseudo Dice: 0.8809000253677368 
2024-12-07 03:42:21.086078:  
2024-12-07 03:42:21.091619: Epoch 21 
2024-12-07 03:42:21.095136: Current learning rate: 0.00809 
2024-12-07 03:42:27.261252: train_loss -0.961 
2024-12-07 03:42:27.266811: val_loss -0.8182 
2024-12-07 03:42:27.269861: Pseudo dice [np.float32(0.8916), np.float32(0.8755)] 
2024-12-07 03:42:27.272942: Epoch time: 6.18 s 
2024-12-07 03:42:27.274981: Yayy! New best EMA pseudo Dice: 0.8812000155448914 
2024-12-07 03:42:27.861539:  
2024-12-07 03:42:27.866922: Epoch 22 
2024-12-07 03:42:27.869435: Current learning rate: 0.008 
2024-12-07 03:42:34.066045: train_loss -0.962 
2024-12-07 03:42:34.071656: val_loss -0.8187 
2024-12-07 03:42:34.074197: Pseudo dice [np.float32(0.8919), np.float32(0.8768)] 
2024-12-07 03:42:34.077744: Epoch time: 6.21 s 
2024-12-07 03:42:34.080789: Yayy! New best EMA pseudo Dice: 0.8815000057220459 
2024-12-07 03:42:34.741992:  
2024-12-07 03:42:34.747512: Epoch 23 
2024-12-07 03:42:34.750019: Current learning rate: 0.0079 
2024-12-07 03:42:40.946129: train_loss -0.9629 
2024-12-07 03:42:40.951694: val_loss -0.8168 
2024-12-07 03:42:40.953738: Pseudo dice [np.float32(0.891), np.float32(0.8742)] 
2024-12-07 03:42:40.957801: Epoch time: 6.21 s 
2024-12-07 03:42:40.961393: Yayy! New best EMA pseudo Dice: 0.881600022315979 
2024-12-07 03:42:41.563294:  
2024-12-07 03:42:41.568308: Epoch 24 
2024-12-07 03:42:41.571318: Current learning rate: 0.00781 
2024-12-07 03:42:47.727579: train_loss -0.965 
2024-12-07 03:42:47.733150: val_loss -0.8168 
2024-12-07 03:42:47.736175: Pseudo dice [np.float32(0.8923), np.float32(0.8746)] 
2024-12-07 03:42:47.739213: Epoch time: 6.17 s 
2024-12-07 03:42:47.742729: Yayy! New best EMA pseudo Dice: 0.8817999958992004 
2024-12-07 03:42:48.335359:  
2024-12-07 03:42:48.342498: Epoch 25 
2024-12-07 03:42:48.346559: Current learning rate: 0.00772 
2024-12-07 03:42:54.508382: train_loss -0.9651 
2024-12-07 03:42:54.513965: val_loss -0.816 
2024-12-07 03:42:54.517497: Pseudo dice [np.float32(0.8918), np.float32(0.875)] 
2024-12-07 03:42:54.520266: Epoch time: 6.17 s 
2024-12-07 03:42:54.522785: Yayy! New best EMA pseudo Dice: 0.8820000290870667 
2024-12-07 03:42:55.113120:  
2024-12-07 03:42:55.118131: Epoch 26 
2024-12-07 03:42:55.120638: Current learning rate: 0.00763 
2024-12-07 03:43:01.284611: train_loss -0.9659 
2024-12-07 03:43:01.290167: val_loss -0.8129 
2024-12-07 03:43:01.293216: Pseudo dice [np.float32(0.8909), np.float32(0.873)] 
2024-12-07 03:43:01.296292: Epoch time: 6.17 s 
2024-12-07 03:43:01.869044:  
2024-12-07 03:43:01.874627: Epoch 27 
2024-12-07 03:43:01.877133: Current learning rate: 0.00753 
2024-12-07 03:43:08.031552: train_loss -0.9667 
2024-12-07 03:43:08.037153: val_loss -0.8161 
2024-12-07 03:43:08.040683: Pseudo dice [np.float32(0.8917), np.float32(0.8747)] 
2024-12-07 03:43:08.043735: Epoch time: 6.16 s 
2024-12-07 03:43:08.046787: Yayy! New best EMA pseudo Dice: 0.882099986076355 
2024-12-07 03:43:08.797257:  
2024-12-07 03:43:08.801784: Epoch 28 
2024-12-07 03:43:08.805295: Current learning rate: 0.00744 
2024-12-07 03:43:15.020538: train_loss -0.9678 
2024-12-07 03:43:15.025647: val_loss -0.8102 
2024-12-07 03:43:15.028713: Pseudo dice [np.float32(0.8892), np.float32(0.8727)] 
2024-12-07 03:43:15.031243: Epoch time: 6.22 s 
2024-12-07 03:43:15.749285:  
2024-12-07 03:43:15.755300: Epoch 29 
2024-12-07 03:43:15.758313: Current learning rate: 0.00735 
2024-12-07 03:43:21.991026: train_loss -0.9695 
2024-12-07 03:43:21.995242: val_loss -0.8089 
2024-12-07 03:43:21.998336: Pseudo dice [np.float32(0.8881), np.float32(0.8717)] 
2024-12-07 03:43:22.001406: Epoch time: 6.24 s 
2024-12-07 03:43:22.592313:  
2024-12-07 03:43:22.595341: Epoch 30 
2024-12-07 03:43:22.599434: Current learning rate: 0.00725 
2024-12-07 03:43:28.773453: train_loss -0.9702 
2024-12-07 03:43:28.779996: val_loss -0.8087 
2024-12-07 03:43:28.783005: Pseudo dice [np.float32(0.8877), np.float32(0.8722)] 
2024-12-07 03:43:28.786517: Epoch time: 6.18 s 
2024-12-07 03:43:29.360904:  
2024-12-07 03:43:29.365918: Epoch 31 
2024-12-07 03:43:29.369427: Current learning rate: 0.00716 
2024-12-07 03:43:35.540151: train_loss -0.9697 
2024-12-07 03:43:35.545369: val_loss -0.8065 
2024-12-07 03:43:35.547403: Pseudo dice [np.float32(0.8869), np.float32(0.8704)] 
2024-12-07 03:43:35.551988: Epoch time: 6.18 s 
2024-12-07 03:43:36.116092:  
2024-12-07 03:43:36.119604: Epoch 32 
2024-12-07 03:43:36.123618: Current learning rate: 0.00707 
2024-12-07 03:43:42.290268: train_loss -0.971 
2024-12-07 03:43:42.296862: val_loss -0.8057 
2024-12-07 03:43:42.300457: Pseudo dice [np.float32(0.8862), np.float32(0.8715)] 
2024-12-07 03:43:42.303513: Epoch time: 6.18 s 
2024-12-07 03:43:42.880270:  
2024-12-07 03:43:42.883004: Epoch 33 
2024-12-07 03:43:42.887061: Current learning rate: 0.00697 
2024-12-07 03:43:49.052288: train_loss -0.9716 
2024-12-07 03:43:49.058813: val_loss -0.8095 
2024-12-07 03:43:49.062328: Pseudo dice [np.float32(0.8893), np.float32(0.872)] 
2024-12-07 03:43:49.065344: Epoch time: 6.17 s 
2024-12-07 03:43:49.636263:  
2024-12-07 03:43:49.641299: Epoch 34 
2024-12-07 03:43:49.643904: Current learning rate: 0.00688 
2024-12-07 03:43:55.805535: train_loss -0.9725 
2024-12-07 03:43:55.811102: val_loss -0.8132 
2024-12-07 03:43:55.814136: Pseudo dice [np.float32(0.8912), np.float32(0.8762)] 
2024-12-07 03:43:55.817674: Epoch time: 6.17 s 
2024-12-07 03:43:56.404857:  
2024-12-07 03:43:56.409886: Epoch 35 
2024-12-07 03:43:56.413425: Current learning rate: 0.00679 
2024-12-07 03:44:02.564702: train_loss -0.973 
2024-12-07 03:44:02.569295: val_loss -0.8033 
2024-12-07 03:44:02.572824: Pseudo dice [np.float32(0.8872), np.float32(0.8699)] 
2024-12-07 03:44:02.575858: Epoch time: 6.16 s 
2024-12-07 03:44:03.317011:  
2024-12-07 03:44:03.322024: Epoch 36 
2024-12-07 03:44:03.326035: Current learning rate: 0.00669 
2024-12-07 03:44:09.506761: train_loss -0.9737 
2024-12-07 03:44:09.512338: val_loss -0.8078 
2024-12-07 03:44:09.515384: Pseudo dice [np.float32(0.8886), np.float32(0.8723)] 
2024-12-07 03:44:09.518458: Epoch time: 6.19 s 
2024-12-07 03:44:10.081730:  
2024-12-07 03:44:10.086262: Epoch 37 
2024-12-07 03:44:10.089369: Current learning rate: 0.0066 
2024-12-07 03:44:16.257881: train_loss -0.9739 
2024-12-07 03:44:16.263465: val_loss -0.8093 
2024-12-07 03:44:16.266516: Pseudo dice [np.float32(0.8887), np.float32(0.8719)] 
2024-12-07 03:44:16.269039: Epoch time: 6.18 s 
2024-12-07 03:44:16.855577:  
2024-12-07 03:44:16.859591: Epoch 38 
2024-12-07 03:44:16.863603: Current learning rate: 0.0065 
2024-12-07 03:44:23.020525: train_loss -0.9741 
2024-12-07 03:44:23.026613: val_loss -0.8006 
2024-12-07 03:44:23.029641: Pseudo dice [np.float32(0.886), np.float32(0.8684)] 
2024-12-07 03:44:23.033155: Epoch time: 6.17 s 
2024-12-07 03:44:23.617758:  
2024-12-07 03:44:23.620767: Epoch 39 
2024-12-07 03:44:23.623794: Current learning rate: 0.00641 
2024-12-07 03:44:29.779873: train_loss -0.975 
2024-12-07 03:44:29.785464: val_loss -0.8087 
2024-12-07 03:44:29.788493: Pseudo dice [np.float32(0.8901), np.float32(0.8734)] 
2024-12-07 03:44:29.791534: Epoch time: 6.16 s 
2024-12-07 03:44:30.370044:  
2024-12-07 03:44:30.375058: Epoch 40 
2024-12-07 03:44:30.377566: Current learning rate: 0.00631 
2024-12-07 03:44:36.531045: train_loss -0.9753 
2024-12-07 03:44:36.536641: val_loss -0.8048 
2024-12-07 03:44:36.540212: Pseudo dice [np.float32(0.8876), np.float32(0.871)] 
2024-12-07 03:44:36.542772: Epoch time: 6.16 s 
2024-12-07 03:44:37.131713:  
2024-12-07 03:44:37.136727: Epoch 41 
2024-12-07 03:44:37.139739: Current learning rate: 0.00622 
2024-12-07 03:44:43.302872: train_loss -0.9762 
2024-12-07 03:44:43.308891: val_loss -0.8105 
2024-12-07 03:44:43.312397: Pseudo dice [np.float32(0.892), np.float32(0.8738)] 
2024-12-07 03:44:43.315408: Epoch time: 6.17 s 
2024-12-07 03:44:43.875201:  
2024-12-07 03:44:43.879729: Epoch 42 
2024-12-07 03:44:43.882803: Current learning rate: 0.00612 
2024-12-07 03:44:50.054472: train_loss -0.9764 
2024-12-07 03:44:50.059854: val_loss -0.8035 
2024-12-07 03:44:50.061882: Pseudo dice [np.float32(0.8883), np.float32(0.8715)] 
2024-12-07 03:44:50.065916: Epoch time: 6.18 s 
2024-12-07 03:44:50.636294:  
2024-12-07 03:44:50.641815: Epoch 43 
2024-12-07 03:44:50.644013: Current learning rate: 0.00603 
2024-12-07 03:44:56.949866: train_loss -0.9772 
2024-12-07 03:44:56.956064: val_loss -0.8046 
2024-12-07 03:44:56.958092: Pseudo dice [np.float32(0.8888), np.float32(0.8714)] 
2024-12-07 03:44:56.962141: Epoch time: 6.31 s 
2024-12-07 03:44:57.529942:  
2024-12-07 03:44:57.533991: Epoch 44 
2024-12-07 03:44:57.538554: Current learning rate: 0.00593 
2024-12-07 03:45:03.693928: train_loss -0.9774 
2024-12-07 03:45:03.698850: val_loss -0.8033 
2024-12-07 03:45:03.701928: Pseudo dice [np.float32(0.8876), np.float32(0.8706)] 
2024-12-07 03:45:03.705944: Epoch time: 6.16 s 
2024-12-07 03:45:04.300124:  
2024-12-07 03:45:04.305012: Epoch 45 
2024-12-07 03:45:04.308108: Current learning rate: 0.00584 
2024-12-07 03:45:10.464029: train_loss -0.9776 
2024-12-07 03:45:10.469583: val_loss -0.8014 
2024-12-07 03:45:10.473096: Pseudo dice [np.float32(0.887), np.float32(0.8691)] 
2024-12-07 03:45:10.476179: Epoch time: 6.16 s 
2024-12-07 03:45:11.031221:  
2024-12-07 03:45:11.035244: Epoch 46 
2024-12-07 03:45:11.038451: Current learning rate: 0.00574 
2024-12-07 03:45:17.206956: train_loss -0.9784 
2024-12-07 03:45:17.212122: val_loss -0.8019 
2024-12-07 03:45:17.214655: Pseudo dice [np.float32(0.8885), np.float32(0.87)] 
2024-12-07 03:45:17.218237: Epoch time: 6.18 s 
2024-12-07 03:45:17.760108:  
2024-12-07 03:45:17.764642: Epoch 47 
2024-12-07 03:45:17.768164: Current learning rate: 0.00565 
2024-12-07 03:45:23.919570: train_loss -0.9779 
2024-12-07 03:45:23.925670: val_loss -0.8098 
2024-12-07 03:45:23.928184: Pseudo dice [np.float32(0.8918), np.float32(0.8741)] 
2024-12-07 03:45:23.931886: Epoch time: 6.16 s 
2024-12-07 03:45:24.472933:  
2024-12-07 03:45:24.477004: Epoch 48 
2024-12-07 03:45:24.481095: Current learning rate: 0.00555 
2024-12-07 03:45:30.649677: train_loss -0.9789 
2024-12-07 03:45:30.655215: val_loss -0.7994 
2024-12-07 03:45:30.658776: Pseudo dice [np.float32(0.8865), np.float32(0.869)] 
2024-12-07 03:45:30.661291: Epoch time: 6.18 s 
2024-12-07 03:45:31.226454:  
2024-12-07 03:45:31.230989: Epoch 49 
2024-12-07 03:45:31.234064: Current learning rate: 0.00546 
2024-12-07 03:45:37.390781: train_loss -0.9791 
2024-12-07 03:45:37.396339: val_loss -0.8034 
2024-12-07 03:45:37.399384: Pseudo dice [np.float32(0.8878), np.float32(0.872)] 
2024-12-07 03:45:37.401920: Epoch time: 6.16 s 
2024-12-07 03:45:38.010933:  
2024-12-07 03:45:38.016472: Epoch 50 
2024-12-07 03:45:38.019506: Current learning rate: 0.00536 
2024-12-07 03:45:44.189356: train_loss -0.9799 
2024-12-07 03:45:44.195470: val_loss -0.8003 
2024-12-07 03:45:44.198533: Pseudo dice [np.float32(0.8866), np.float32(0.8687)] 
2024-12-07 03:45:44.201562: Epoch time: 6.18 s 
2024-12-07 03:45:44.915447:  
2024-12-07 03:45:44.920963: Epoch 51 
2024-12-07 03:45:44.923470: Current learning rate: 0.00526 
2024-12-07 03:45:51.080639: train_loss -0.9799 
2024-12-07 03:45:51.086832: val_loss -0.8043 
2024-12-07 03:45:51.090900: Pseudo dice [np.float32(0.8896), np.float32(0.8715)] 
2024-12-07 03:45:51.093977: Epoch time: 6.17 s 
2024-12-07 03:45:51.650525:  
2024-12-07 03:45:51.655540: Epoch 52 
2024-12-07 03:45:51.658047: Current learning rate: 0.00517 
2024-12-07 03:45:57.811954: train_loss -0.9804 
2024-12-07 03:45:57.817545: val_loss -0.8021 
2024-12-07 03:45:57.820599: Pseudo dice [np.float32(0.8867), np.float32(0.87)] 
2024-12-07 03:45:57.823696: Epoch time: 6.16 s 
2024-12-07 03:45:58.385273:  
2024-12-07 03:45:58.390302: Epoch 53 
2024-12-07 03:45:58.393327: Current learning rate: 0.00507 
2024-12-07 03:46:04.561638: train_loss -0.9807 
2024-12-07 03:46:04.566659: val_loss -0.7996 
2024-12-07 03:46:04.570181: Pseudo dice [np.float32(0.8863), np.float32(0.87)] 
2024-12-07 03:46:04.573231: Epoch time: 6.18 s 
2024-12-07 03:46:05.132595:  
2024-12-07 03:46:05.137106: Epoch 54 
2024-12-07 03:46:05.140114: Current learning rate: 0.00497 
2024-12-07 03:46:11.306351: train_loss -0.9809 
2024-12-07 03:46:11.312439: val_loss -0.8035 
2024-12-07 03:46:11.315471: Pseudo dice [np.float32(0.8897), np.float32(0.8715)] 
2024-12-07 03:46:11.318487: Epoch time: 6.17 s 
2024-12-07 03:46:11.887234:  
2024-12-07 03:46:11.892281: Epoch 55 
2024-12-07 03:46:11.895326: Current learning rate: 0.00487 
2024-12-07 03:46:18.056177: train_loss -0.9817 
2024-12-07 03:46:18.062270: val_loss -0.807 
2024-12-07 03:46:18.064843: Pseudo dice [np.float32(0.8901), np.float32(0.8742)] 
2024-12-07 03:46:18.068380: Epoch time: 6.17 s 
2024-12-07 03:46:18.626774:  
2024-12-07 03:46:18.630822: Epoch 56 
2024-12-07 03:46:18.634881: Current learning rate: 0.00478 
2024-12-07 03:46:24.786737: train_loss -0.9815 
2024-12-07 03:46:24.792294: val_loss -0.8025 
2024-12-07 03:46:24.795921: Pseudo dice [np.float32(0.8867), np.float32(0.8711)] 
2024-12-07 03:46:24.798941: Epoch time: 6.16 s 
2024-12-07 03:46:25.368986:  
2024-12-07 03:46:25.373013: Epoch 57 
2024-12-07 03:46:25.376616: Current learning rate: 0.00468 
2024-12-07 03:46:31.533823: train_loss -0.9822 
2024-12-07 03:46:31.539376: val_loss -0.8033 
2024-12-07 03:46:31.541808: Pseudo dice [np.float32(0.8895), np.float32(0.8721)] 
2024-12-07 03:46:31.544314: Epoch time: 6.17 s 
2024-12-07 03:46:32.118828:  
2024-12-07 03:46:32.123336: Epoch 58 
2024-12-07 03:46:32.126347: Current learning rate: 0.00458 
2024-12-07 03:46:38.278236: train_loss -0.9824 
2024-12-07 03:46:38.283824: val_loss -0.8007 
2024-12-07 03:46:38.286351: Pseudo dice [np.float32(0.8881), np.float32(0.8697)] 
2024-12-07 03:46:38.289387: Epoch time: 6.16 s 
2024-12-07 03:46:39.021621:  
2024-12-07 03:46:39.026407: Epoch 59 
2024-12-07 03:46:39.029921: Current learning rate: 0.00448 
2024-12-07 03:46:45.185612: train_loss -0.9822 
2024-12-07 03:46:45.191724: val_loss -0.8002 
2024-12-07 03:46:45.194755: Pseudo dice [np.float32(0.8876), np.float32(0.8698)] 
2024-12-07 03:46:45.197262: Epoch time: 6.16 s 
2024-12-07 03:46:45.763727:  
2024-12-07 03:46:45.767763: Epoch 60 
2024-12-07 03:46:45.771331: Current learning rate: 0.00438 
2024-12-07 03:46:51.937965: train_loss -0.9828 
2024-12-07 03:46:51.945051: val_loss -0.7974 
2024-12-07 03:46:51.948603: Pseudo dice [np.float32(0.8861), np.float32(0.8684)] 
2024-12-07 03:46:51.951170: Epoch time: 6.18 s 
2024-12-07 03:46:52.518704:  
2024-12-07 03:46:52.523717: Epoch 61 
2024-12-07 03:46:52.526731: Current learning rate: 0.00429 
2024-12-07 03:46:58.678638: train_loss -0.9831 
2024-12-07 03:46:58.684217: val_loss -0.8029 
2024-12-07 03:46:58.686765: Pseudo dice [np.float32(0.8895), np.float32(0.8726)] 
2024-12-07 03:46:58.690304: Epoch time: 6.16 s 
2024-12-07 03:46:59.267407:  
2024-12-07 03:46:59.271442: Epoch 62 
2024-12-07 03:46:59.275490: Current learning rate: 0.00419 
2024-12-07 03:47:05.445198: train_loss -0.9835 
2024-12-07 03:47:05.450254: val_loss -0.7963 
2024-12-07 03:47:05.453828: Pseudo dice [np.float32(0.8857), np.float32(0.8676)] 
2024-12-07 03:47:05.456887: Epoch time: 6.18 s 
2024-12-07 03:47:06.024009:  
2024-12-07 03:47:06.028804: Epoch 63 
2024-12-07 03:47:06.031816: Current learning rate: 0.00409 
2024-12-07 03:47:12.206811: train_loss -0.9836 
2024-12-07 03:47:12.212382: val_loss -0.796 
2024-12-07 03:47:12.215417: Pseudo dice [np.float32(0.8862), np.float32(0.8692)] 
2024-12-07 03:47:12.218924: Epoch time: 6.18 s 
2024-12-07 03:47:12.792642:  
2024-12-07 03:47:12.797660: Epoch 64 
2024-12-07 03:47:12.801176: Current learning rate: 0.00399 
2024-12-07 03:47:18.974003: train_loss -0.9839 
2024-12-07 03:47:18.979628: val_loss -0.7974 
2024-12-07 03:47:18.982680: Pseudo dice [np.float32(0.8846), np.float32(0.8685)] 
2024-12-07 03:47:18.984708: Epoch time: 6.18 s 
2024-12-07 03:47:19.561661:  
2024-12-07 03:47:19.567236: Epoch 65 
2024-12-07 03:47:19.569792: Current learning rate: 0.00389 
2024-12-07 03:47:25.740658: train_loss -0.9846 
2024-12-07 03:47:25.746304: val_loss -0.798 
2024-12-07 03:47:25.749395: Pseudo dice [np.float32(0.8868), np.float32(0.8695)] 
2024-12-07 03:47:25.752475: Epoch time: 6.18 s 
2024-12-07 03:47:26.331526:  
2024-12-07 03:47:26.335538: Epoch 66 
2024-12-07 03:47:26.339552: Current learning rate: 0.00379 
2024-12-07 03:47:32.507609: train_loss -0.9835 
2024-12-07 03:47:32.513172: val_loss -0.8074 
2024-12-07 03:47:32.515684: Pseudo dice [np.float32(0.8912), np.float32(0.8741)] 
2024-12-07 03:47:32.519201: Epoch time: 6.18 s 
2024-12-07 03:47:33.245804:  
2024-12-07 03:47:33.251372: Epoch 67 
2024-12-07 03:47:33.253922: Current learning rate: 0.00369 
2024-12-07 03:47:39.419919: train_loss -0.9844 
2024-12-07 03:47:39.424979: val_loss -0.7934 
2024-12-07 03:47:39.428519: Pseudo dice [np.float32(0.8839), np.float32(0.8674)] 
2024-12-07 03:47:39.431221: Epoch time: 6.17 s 
2024-12-07 03:47:40.116021:  
2024-12-07 03:47:40.120033: Epoch 68 
2024-12-07 03:47:40.123081: Current learning rate: 0.00359 
2024-12-07 03:47:46.286725: train_loss -0.9848 
2024-12-07 03:47:46.291806: val_loss -0.7976 
2024-12-07 03:47:46.294887: Pseudo dice [np.float32(0.8867), np.float32(0.8691)] 
2024-12-07 03:47:46.298397: Epoch time: 6.17 s 
2024-12-07 03:47:46.879492:  
2024-12-07 03:47:46.885004: Epoch 69 
2024-12-07 03:47:46.887509: Current learning rate: 0.00349 
2024-12-07 03:47:53.056471: train_loss -0.9855 
2024-12-07 03:47:53.063069: val_loss -0.8001 
2024-12-07 03:47:53.066714: Pseudo dice [np.float32(0.8874), np.float32(0.8717)] 
2024-12-07 03:47:53.069264: Epoch time: 6.18 s 
2024-12-07 03:47:53.652084:  
2024-12-07 03:47:53.657128: Epoch 70 
2024-12-07 03:47:53.660207: Current learning rate: 0.00338 
2024-12-07 03:47:59.827988: train_loss -0.9851 
2024-12-07 03:47:59.833308: val_loss -0.7983 
2024-12-07 03:47:59.835843: Pseudo dice [np.float32(0.8876), np.float32(0.8705)] 
2024-12-07 03:47:59.840908: Epoch time: 6.18 s 
2024-12-07 03:48:00.432148:  
2024-12-07 03:48:00.437160: Epoch 71 
2024-12-07 03:48:00.440168: Current learning rate: 0.00328 
2024-12-07 03:48:06.598197: train_loss -0.9855 
2024-12-07 03:48:06.602272: val_loss -0.7896 
2024-12-07 03:48:06.606805: Pseudo dice [np.float32(0.8839), np.float32(0.8657)] 
2024-12-07 03:48:06.609311: Epoch time: 6.17 s 
2024-12-07 03:48:07.195897:  
2024-12-07 03:48:07.200926: Epoch 72 
2024-12-07 03:48:07.204582: Current learning rate: 0.00318 
2024-12-07 03:48:13.376186: train_loss -0.9855 
2024-12-07 03:48:13.381226: val_loss -0.7968 
2024-12-07 03:48:13.384773: Pseudo dice [np.float32(0.8871), np.float32(0.8699)] 
2024-12-07 03:48:13.387803: Epoch time: 6.18 s 
2024-12-07 03:48:13.964624:  
2024-12-07 03:48:13.968655: Epoch 73 
2024-12-07 03:48:13.972268: Current learning rate: 0.00308 
2024-12-07 03:48:20.136113: train_loss -0.9854 
2024-12-07 03:48:20.141183: val_loss -0.8009 
2024-12-07 03:48:20.144789: Pseudo dice [np.float32(0.8887), np.float32(0.8719)] 
2024-12-07 03:48:20.147856: Epoch time: 6.17 s 
2024-12-07 03:48:20.743688:  
2024-12-07 03:48:20.748884: Epoch 74 
2024-12-07 03:48:20.751391: Current learning rate: 0.00297 
2024-12-07 03:48:26.895656: train_loss -0.9855 
2024-12-07 03:48:26.901207: val_loss -0.7979 
2024-12-07 03:48:26.903729: Pseudo dice [np.float32(0.888), np.float32(0.8709)] 
2024-12-07 03:48:26.907277: Epoch time: 6.15 s 
2024-12-07 03:48:27.642328:  
2024-12-07 03:48:27.646382: Epoch 75 
2024-12-07 03:48:27.648933: Current learning rate: 0.00287 
2024-12-07 03:48:33.808862: train_loss -0.986 
2024-12-07 03:48:33.813934: val_loss -0.7958 
2024-12-07 03:48:33.817523: Pseudo dice [np.float32(0.8853), np.float32(0.869)] 
2024-12-07 03:48:33.820575: Epoch time: 6.17 s 
2024-12-07 03:48:34.392703:  
2024-12-07 03:48:34.398271: Epoch 76 
2024-12-07 03:48:34.400840: Current learning rate: 0.00277 
2024-12-07 03:48:40.559888: train_loss -0.9856 
2024-12-07 03:48:40.565488: val_loss -0.8003 
2024-12-07 03:48:40.568558: Pseudo dice [np.float32(0.8883), np.float32(0.8711)] 
2024-12-07 03:48:40.571605: Epoch time: 6.17 s 
2024-12-07 03:48:41.162903:  
2024-12-07 03:48:41.166919: Epoch 77 
2024-12-07 03:48:41.169427: Current learning rate: 0.00266 
2024-12-07 03:48:47.336596: train_loss -0.9862 
2024-12-07 03:48:47.342149: val_loss -0.8007 
2024-12-07 03:48:47.344671: Pseudo dice [np.float32(0.8893), np.float32(0.872)] 
2024-12-07 03:48:47.348228: Epoch time: 6.17 s 
2024-12-07 03:48:47.934566:  
2024-12-07 03:48:47.938138: Epoch 78 
2024-12-07 03:48:47.942198: Current learning rate: 0.00256 
2024-12-07 03:48:54.109768: train_loss -0.9866 
2024-12-07 03:48:54.115081: val_loss -0.8002 
2024-12-07 03:48:54.117593: Pseudo dice [np.float32(0.8895), np.float32(0.8722)] 
2024-12-07 03:48:54.121192: Epoch time: 6.18 s 
2024-12-07 03:48:54.717123:  
2024-12-07 03:48:54.721633: Epoch 79 
2024-12-07 03:48:54.724643: Current learning rate: 0.00245 
2024-12-07 03:49:00.899204: train_loss -0.987 
2024-12-07 03:49:00.904777: val_loss -0.8038 
2024-12-07 03:49:00.908307: Pseudo dice [np.float32(0.8897), np.float32(0.8738)] 
2024-12-07 03:49:00.911381: Epoch time: 6.18 s 
2024-12-07 03:49:01.503927:  
2024-12-07 03:49:01.508938: Epoch 80 
2024-12-07 03:49:01.511443: Current learning rate: 0.00235 
2024-12-07 03:49:07.667632: train_loss -0.987 
2024-12-07 03:49:07.673160: val_loss -0.8008 
2024-12-07 03:49:07.675678: Pseudo dice [np.float32(0.8898), np.float32(0.8729)] 
2024-12-07 03:49:07.689421: Epoch time: 6.16 s 
2024-12-07 03:49:08.276173:  
2024-12-07 03:49:08.281214: Epoch 81 
2024-12-07 03:49:08.283719: Current learning rate: 0.00224 
2024-12-07 03:49:14.458411: train_loss -0.987 
2024-12-07 03:49:14.464011: val_loss -0.7913 
2024-12-07 03:49:14.467102: Pseudo dice [np.float32(0.8854), np.float32(0.8674)] 
2024-12-07 03:49:14.470134: Epoch time: 6.18 s 
2024-12-07 03:49:15.223889:  
2024-12-07 03:49:15.228922: Epoch 82 
2024-12-07 03:49:15.232552: Current learning rate: 0.00214 
2024-12-07 03:49:21.388607: train_loss -0.987 
2024-12-07 03:49:21.393646: val_loss -0.7966 
2024-12-07 03:49:21.396677: Pseudo dice [np.float32(0.888), np.float32(0.8693)] 
2024-12-07 03:49:21.400211: Epoch time: 6.16 s 
2024-12-07 03:49:21.958433:  
2024-12-07 03:49:21.961941: Epoch 83 
2024-12-07 03:49:21.965447: Current learning rate: 0.00203 
2024-12-07 03:49:28.120751: train_loss -0.9875 
2024-12-07 03:49:28.126321: val_loss -0.7944 
2024-12-07 03:49:28.130423: Pseudo dice [np.float32(0.8876), np.float32(0.8689)] 
2024-12-07 03:49:28.132948: Epoch time: 6.16 s 
2024-12-07 03:49:28.700706:  
2024-12-07 03:49:28.705751: Epoch 84 
2024-12-07 03:49:28.708816: Current learning rate: 0.00192 
2024-12-07 03:49:34.884490: train_loss -0.9878 
2024-12-07 03:49:34.888550: val_loss -0.7918 
2024-12-07 03:49:34.891617: Pseudo dice [np.float32(0.8849), np.float32(0.8679)] 
2024-12-07 03:49:34.894684: Epoch time: 6.18 s 
2024-12-07 03:49:35.469391:  
2024-12-07 03:49:35.474419: Epoch 85 
2024-12-07 03:49:35.477449: Current learning rate: 0.00181 
2024-12-07 03:49:41.639390: train_loss -0.9878 
2024-12-07 03:49:41.643451: val_loss -0.7911 
2024-12-07 03:49:41.647485: Pseudo dice [np.float32(0.8852), np.float32(0.8681)] 
2024-12-07 03:49:41.650523: Epoch time: 6.17 s 
2024-12-07 03:49:42.201666:  
2024-12-07 03:49:42.206678: Epoch 86 
2024-12-07 03:49:42.209687: Current learning rate: 0.0017 
2024-12-07 03:49:48.377568: train_loss -0.9874 
2024-12-07 03:49:48.382133: val_loss -0.7896 
2024-12-07 03:49:48.385713: Pseudo dice [np.float32(0.8848), np.float32(0.8674)] 
2024-12-07 03:49:48.388247: Epoch time: 6.18 s 
2024-12-07 03:49:48.938216:  
2024-12-07 03:49:48.943266: Epoch 87 
2024-12-07 03:49:48.946347: Current learning rate: 0.00159 
2024-12-07 03:49:55.110558: train_loss -0.9877 
2024-12-07 03:49:55.117188: val_loss -0.8 
2024-12-07 03:49:55.120749: Pseudo dice [np.float32(0.8889), np.float32(0.8722)] 
2024-12-07 03:49:55.123813: Epoch time: 6.17 s 
2024-12-07 03:49:55.678942:  
2024-12-07 03:49:55.683973: Epoch 88 
2024-12-07 03:49:55.686937: Current learning rate: 0.00148 
2024-12-07 03:50:01.851254: train_loss -0.9875 
2024-12-07 03:50:01.856979: val_loss -0.7891 
2024-12-07 03:50:01.860012: Pseudo dice [np.float32(0.8847), np.float32(0.8678)] 
2024-12-07 03:50:01.864023: Epoch time: 6.17 s 
2024-12-07 03:50:02.423294:  
2024-12-07 03:50:02.428128: Epoch 89 
2024-12-07 03:50:02.431640: Current learning rate: 0.00137 
2024-12-07 03:50:08.599323: train_loss -0.9875 
2024-12-07 03:50:08.604337: val_loss -0.791 
2024-12-07 03:50:08.607397: Pseudo dice [np.float32(0.8856), np.float32(0.867)] 
2024-12-07 03:50:08.611461: Epoch time: 6.18 s 
2024-12-07 03:50:09.311506:  
2024-12-07 03:50:09.315564: Epoch 90 
2024-12-07 03:50:09.318113: Current learning rate: 0.00126 
2024-12-07 03:50:15.487457: train_loss -0.9877 
2024-12-07 03:50:15.492557: val_loss -0.8023 
2024-12-07 03:50:15.495616: Pseudo dice [np.float32(0.8898), np.float32(0.8743)] 
2024-12-07 03:50:15.498123: Epoch time: 6.18 s 
2024-12-07 03:50:16.131868:  
2024-12-07 03:50:16.137385: Epoch 91 
2024-12-07 03:50:16.139892: Current learning rate: 0.00115 
2024-12-07 03:50:22.314759: train_loss -0.9883 
2024-12-07 03:50:22.319910: val_loss -0.7911 
2024-12-07 03:50:22.322977: Pseudo dice [np.float32(0.885), np.float32(0.8685)] 
2024-12-07 03:50:22.326568: Epoch time: 6.18 s 
2024-12-07 03:50:22.879230:  
2024-12-07 03:50:22.884324: Epoch 92 
2024-12-07 03:50:22.886873: Current learning rate: 0.00103 
2024-12-07 03:50:29.052684: train_loss -0.9879 
2024-12-07 03:50:29.058774: val_loss -0.7985 
2024-12-07 03:50:29.061812: Pseudo dice [np.float32(0.887), np.float32(0.87)] 
2024-12-07 03:50:29.064871: Epoch time: 6.17 s 
2024-12-07 03:50:29.615461:  
2024-12-07 03:50:29.620474: Epoch 93 
2024-12-07 03:50:29.622981: Current learning rate: 0.00091 
2024-12-07 03:50:35.793736: train_loss -0.988 
2024-12-07 03:50:35.798824: val_loss -0.7926 
2024-12-07 03:50:35.801932: Pseudo dice [np.float32(0.8854), np.float32(0.8681)] 
2024-12-07 03:50:35.805009: Epoch time: 6.18 s 
2024-12-07 03:50:36.355962:  
2024-12-07 03:50:36.361030: Epoch 94 
2024-12-07 03:50:36.363659: Current learning rate: 0.00079 
2024-12-07 03:50:42.540746: train_loss -0.9879 
2024-12-07 03:50:42.545324: val_loss -0.7961 
2024-12-07 03:50:42.548875: Pseudo dice [np.float32(0.8869), np.float32(0.8703)] 
2024-12-07 03:50:42.551722: Epoch time: 6.18 s 
2024-12-07 03:50:43.113109:  
2024-12-07 03:50:43.118160: Epoch 95 
2024-12-07 03:50:43.121196: Current learning rate: 0.00067 
2024-12-07 03:50:49.289459: train_loss -0.9886 
2024-12-07 03:50:49.294521: val_loss -0.7961 
2024-12-07 03:50:49.297543: Pseudo dice [np.float32(0.8862), np.float32(0.8694)] 
2024-12-07 03:50:49.301565: Epoch time: 6.18 s 
2024-12-07 03:50:49.860931:  
2024-12-07 03:50:49.864959: Epoch 96 
2024-12-07 03:50:49.869035: Current learning rate: 0.00055 
2024-12-07 03:50:56.022314: train_loss -0.9885 
2024-12-07 03:50:56.028425: val_loss -0.7957 
2024-12-07 03:50:56.032447: Pseudo dice [np.float32(0.887), np.float32(0.8683)] 
2024-12-07 03:50:56.034967: Epoch time: 6.16 s 
2024-12-07 03:50:56.593583:  
2024-12-07 03:50:56.598149: Epoch 97 
2024-12-07 03:50:56.601697: Current learning rate: 0.00043 
2024-12-07 03:51:02.752798: train_loss -0.9888 
2024-12-07 03:51:02.758808: val_loss -0.7888 
2024-12-07 03:51:02.761818: Pseudo dice [np.float32(0.8855), np.float32(0.8669)] 
2024-12-07 03:51:02.764323: Epoch time: 6.16 s 
2024-12-07 03:51:03.472404:  
2024-12-07 03:51:03.477425: Epoch 98 
2024-12-07 03:51:03.480948: Current learning rate: 0.0003 
2024-12-07 03:51:09.642141: train_loss -0.9885 
2024-12-07 03:51:09.647209: val_loss -0.7929 
2024-12-07 03:51:09.650729: Pseudo dice [np.float32(0.8859), np.float32(0.8683)] 
2024-12-07 03:51:09.653751: Epoch time: 6.17 s 
2024-12-07 03:51:10.219060:  
2024-12-07 03:51:10.223085: Epoch 99 
2024-12-07 03:51:10.226592: Current learning rate: 0.00016 
2024-12-07 03:51:16.395226: train_loss -0.9884 
2024-12-07 03:51:16.400767: val_loss -0.7896 
2024-12-07 03:51:16.403790: Pseudo dice [np.float32(0.8851), np.float32(0.8664)] 
2024-12-07 03:51:16.406307: Epoch time: 6.18 s 
2024-12-07 03:51:17.030316: Training done. 
2024-12-07 03:51:17.065316: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2024-12-07 03:51:17.072318: The split file contains 5 splits. 
2024-12-07 03:51:17.078319: Desired fold for training: 0 
2024-12-07 03:51:17.083316: This split has 208 training and 52 validation cases. 
2024-12-07 03:51:17.087317: predicting hippocampus_017 
2024-12-07 03:51:17.094319: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2024-12-07 03:51:17.196316: predicting hippocampus_019 
2024-12-07 03:51:17.203317: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2024-12-07 03:51:17.220316: predicting hippocampus_033 
2024-12-07 03:51:17.226316: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2024-12-07 03:51:17.237316: predicting hippocampus_035 
2024-12-07 03:51:17.243318: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2024-12-07 03:51:17.254316: predicting hippocampus_037 
2024-12-07 03:51:17.261318: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2024-12-07 03:51:17.275316: predicting hippocampus_049 
2024-12-07 03:51:17.281316: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2024-12-07 03:51:17.295317: predicting hippocampus_052 
2024-12-07 03:51:17.301316: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2024-12-07 03:51:17.316316: predicting hippocampus_065 
2024-12-07 03:51:17.322317: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2024-12-07 03:51:17.337317: predicting hippocampus_083 
2024-12-07 03:51:17.344317: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2024-12-07 03:51:17.355316: predicting hippocampus_088 
2024-12-07 03:51:17.361320: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2024-12-07 03:51:20.903991: predicting hippocampus_090 
2024-12-07 03:51:20.936991: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2024-12-07 03:51:20.974992: predicting hippocampus_092 
2024-12-07 03:51:20.989992: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2024-12-07 03:51:21.014992: predicting hippocampus_095 
2024-12-07 03:51:21.021991: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2024-12-07 03:51:21.051989: predicting hippocampus_107 
2024-12-07 03:51:21.078990: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2024-12-07 03:51:21.097989: predicting hippocampus_108 
2024-12-07 03:51:21.103991: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2024-12-07 03:51:21.120989: predicting hippocampus_123 
2024-12-07 03:51:21.124990: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2024-12-07 03:51:21.140989: predicting hippocampus_125 
2024-12-07 03:51:21.145990: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2024-12-07 03:51:21.168989: predicting hippocampus_157 
2024-12-07 03:51:21.173991: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2024-12-07 03:51:21.188989: predicting hippocampus_164 
2024-12-07 03:51:21.195990: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2024-12-07 03:51:21.231989: predicting hippocampus_169 
2024-12-07 03:51:21.236991: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2024-12-07 03:51:21.251989: predicting hippocampus_175 
2024-12-07 03:51:21.255991: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2024-12-07 03:51:21.271990: predicting hippocampus_185 
2024-12-07 03:51:21.278991: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2024-12-07 03:51:21.294989: predicting hippocampus_190 
2024-12-07 03:51:21.299990: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2024-12-07 03:51:21.314989: predicting hippocampus_194 
2024-12-07 03:51:21.320990: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2024-12-07 03:51:21.335989: predicting hippocampus_204 
2024-12-07 03:51:21.342990: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2024-12-07 03:51:21.356989: predicting hippocampus_205 
2024-12-07 03:51:21.361991: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2024-12-07 03:51:21.375989: predicting hippocampus_210 
2024-12-07 03:51:21.380991: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2024-12-07 03:51:21.395989: predicting hippocampus_217 
2024-12-07 03:51:21.401991: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2024-12-07 03:51:21.415989: predicting hippocampus_219 
2024-12-07 03:51:21.422991: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2024-12-07 03:51:21.435989: predicting hippocampus_229 
2024-12-07 03:51:21.440989: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2024-12-07 03:51:21.455989: predicting hippocampus_244 
2024-12-07 03:51:21.460990: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2024-12-07 03:51:21.476990: predicting hippocampus_261 
2024-12-07 03:51:21.481990: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2024-12-07 03:51:21.502989: predicting hippocampus_264 
2024-12-07 03:51:21.508990: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2024-12-07 03:51:21.524989: predicting hippocampus_277 
2024-12-07 03:51:21.529991: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2024-12-07 03:51:21.549989: predicting hippocampus_280 
2024-12-07 03:51:21.556991: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2024-12-07 03:51:21.571989: predicting hippocampus_286 
2024-12-07 03:51:21.576990: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2024-12-07 03:51:21.597989: predicting hippocampus_288 
2024-12-07 03:51:21.604989: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2024-12-07 03:51:21.625989: predicting hippocampus_289 
2024-12-07 03:51:21.630991: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2024-12-07 03:51:21.644990: predicting hippocampus_296 
2024-12-07 03:51:21.649990: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2024-12-07 03:51:21.665989: predicting hippocampus_305 
2024-12-07 03:51:21.671989: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2024-12-07 03:51:21.686989: predicting hippocampus_308 
2024-12-07 03:51:21.692991: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2024-12-07 03:51:21.706989: predicting hippocampus_317 
2024-12-07 03:51:21.711992: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2024-12-07 03:51:21.727989: predicting hippocampus_327 
2024-12-07 03:51:21.732990: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2024-12-07 03:51:21.749989: predicting hippocampus_330 
2024-12-07 03:51:21.754990: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2024-12-07 03:51:21.768989: predicting hippocampus_332 
2024-12-07 03:51:21.773990: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2024-12-07 03:51:21.788989: predicting hippocampus_338 
2024-12-07 03:51:21.793990: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2024-12-07 03:51:21.814990: predicting hippocampus_349 
2024-12-07 03:51:21.819990: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2024-12-07 03:51:21.834989: predicting hippocampus_350 
2024-12-07 03:51:21.841991: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2024-12-07 03:51:21.856989: predicting hippocampus_356 
2024-12-07 03:51:21.863992: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2024-12-07 03:51:21.877989: predicting hippocampus_358 
2024-12-07 03:51:21.882992: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2024-12-07 03:51:21.898989: predicting hippocampus_374 
2024-12-07 03:51:21.904989: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2024-12-07 03:51:21.918989: predicting hippocampus_394 
2024-12-07 03:51:21.923990: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2024-12-07 03:51:25.400047: Validation complete 
2024-12-07 03:51:25.405047: Mean Validation Dice:  0.8764820375746231 
