
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-16 03:40:17.101273: do_dummy_2d_data_aug: False 
2025-03-16 03:40:17.107272: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-16 03:40:17.113277: The split file contains 5 splits. 
2025-03-16 03:40:17.117276: Desired fold for training: 0 
2025-03-16 03:40:17.119276: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-16 03:40:23.385478: unpacking dataset... 
2025-03-16 03:40:23.711596: unpacking done... 
2025-03-16 03:40:24.885691:  
2025-03-16 03:40:24.890702: Epoch 0 
2025-03-16 03:40:24.893712: Current learning rate: 0.01 
2025-03-16 03:40:32.302360: train_loss -0.563 
2025-03-16 03:40:32.307156: val_loss -0.8155 
2025-03-16 03:40:32.310463: Pseudo dice [np.float32(0.8722), np.float32(0.8574)] 
2025-03-16 03:40:32.313488: Epoch time: 7.42 s 
2025-03-16 03:40:32.317528: Yayy! New best EMA pseudo Dice: 0.864799976348877 
2025-03-16 03:40:32.816796:  
2025-03-16 03:40:32.822367: Epoch 1 
2025-03-16 03:40:32.824906: Current learning rate: 0.00991 
2025-03-16 03:40:39.394283: train_loss -0.8144 
2025-03-16 03:40:39.400318: val_loss -0.8334 
2025-03-16 03:40:39.404160: Pseudo dice [np.float32(0.8827), np.float32(0.869)] 
2025-03-16 03:40:39.407230: Epoch time: 6.58 s 
2025-03-16 03:40:39.410305: Yayy! New best EMA pseudo Dice: 0.8658999800682068 
2025-03-16 03:40:39.961937:  
2025-03-16 03:40:39.968951: Epoch 2 
2025-03-16 03:40:39.971964: Current learning rate: 0.00982 
2025-03-16 03:40:46.513225: train_loss -0.836 
2025-03-16 03:40:46.519396: val_loss -0.841 
2025-03-16 03:40:46.523459: Pseudo dice [np.float32(0.8907), np.float32(0.8729)] 
2025-03-16 03:40:46.526541: Epoch time: 6.55 s 
2025-03-16 03:40:46.529092: Yayy! New best EMA pseudo Dice: 0.8675000071525574 
2025-03-16 03:40:47.111109:  
2025-03-16 03:40:47.116117: Epoch 3 
2025-03-16 03:40:47.120133: Current learning rate: 0.00973 
2025-03-16 03:40:53.634166: train_loss -0.8456 
2025-03-16 03:40:53.641236: val_loss -0.8365 
2025-03-16 03:40:53.643746: Pseudo dice [np.float32(0.8887), np.float32(0.8702)] 
2025-03-16 03:40:53.647813: Epoch time: 6.52 s 
2025-03-16 03:40:53.650856: Yayy! New best EMA pseudo Dice: 0.8687000274658203 
2025-03-16 03:40:54.211483:  
2025-03-16 03:40:54.216996: Epoch 4 
2025-03-16 03:40:54.220508: Current learning rate: 0.00964 
2025-03-16 03:41:00.733543: train_loss -0.8512 
2025-03-16 03:41:00.740154: val_loss -0.8456 
2025-03-16 03:41:00.744174: Pseudo dice [np.float32(0.8964), np.float32(0.876)] 
2025-03-16 03:41:00.747724: Epoch time: 6.52 s 
2025-03-16 03:41:00.750272: Yayy! New best EMA pseudo Dice: 0.8704000115394592 
2025-03-16 03:41:01.438874:  
2025-03-16 03:41:01.444403: Epoch 5 
2025-03-16 03:41:01.447914: Current learning rate: 0.00955 
2025-03-16 03:41:07.977819: train_loss -0.855 
2025-03-16 03:41:07.983386: val_loss -0.85 
2025-03-16 03:41:07.986898: Pseudo dice [np.float32(0.8986), np.float32(0.8801)] 
2025-03-16 03:41:07.990504: Epoch time: 6.54 s 
2025-03-16 03:41:07.993608: Yayy! New best EMA pseudo Dice: 0.8723000288009644 
2025-03-16 03:41:08.540249:  
2025-03-16 03:41:08.546259: Epoch 6 
2025-03-16 03:41:08.549267: Current learning rate: 0.00946 
2025-03-16 03:41:15.062168: train_loss -0.8607 
2025-03-16 03:41:15.068236: val_loss -0.8494 
2025-03-16 03:41:15.071761: Pseudo dice [np.float32(0.8975), np.float32(0.8779)] 
2025-03-16 03:41:15.074823: Epoch time: 6.52 s 
2025-03-16 03:41:15.077911: Yayy! New best EMA pseudo Dice: 0.8738999962806702 
2025-03-16 03:41:15.630971:  
2025-03-16 03:41:15.637120: Epoch 7 
2025-03-16 03:41:15.639641: Current learning rate: 0.00937 
2025-03-16 03:41:22.152191: train_loss -0.8616 
2025-03-16 03:41:22.157246: val_loss -0.8526 
2025-03-16 03:41:22.161805: Pseudo dice [np.float32(0.898), np.float32(0.882)] 
2025-03-16 03:41:22.165316: Epoch time: 6.52 s 
2025-03-16 03:41:22.167825: Yayy! New best EMA pseudo Dice: 0.8755000233650208 
2025-03-16 03:41:22.732449:  
2025-03-16 03:41:22.738461: Epoch 8 
2025-03-16 03:41:22.741476: Current learning rate: 0.00928 
2025-03-16 03:41:29.254345: train_loss -0.8649 
2025-03-16 03:41:29.260409: val_loss -0.8496 
2025-03-16 03:41:29.265988: Pseudo dice [np.float32(0.8966), np.float32(0.8792)] 
2025-03-16 03:41:29.269040: Epoch time: 6.52 s 
2025-03-16 03:41:29.272084: Yayy! New best EMA pseudo Dice: 0.8766999840736389 
2025-03-16 03:41:29.864084:  
2025-03-16 03:41:29.869094: Epoch 9 
2025-03-16 03:41:29.872610: Current learning rate: 0.00919 
2025-03-16 03:41:36.396986: train_loss -0.8677 
2025-03-16 03:41:36.402642: val_loss -0.8481 
2025-03-16 03:41:36.406162: Pseudo dice [np.float32(0.8951), np.float32(0.8781)] 
2025-03-16 03:41:36.408767: Epoch time: 6.53 s 
2025-03-16 03:41:36.412319: Yayy! New best EMA pseudo Dice: 0.8776999711990356 
2025-03-16 03:41:36.961850:  
2025-03-16 03:41:36.966870: Epoch 10 
2025-03-16 03:41:36.970386: Current learning rate: 0.0091 
2025-03-16 03:41:43.497996: train_loss -0.8707 
2025-03-16 03:41:43.505573: val_loss -0.8527 
2025-03-16 03:41:43.509615: Pseudo dice [np.float32(0.899), np.float32(0.8812)] 
2025-03-16 03:41:43.513898: Epoch time: 6.54 s 
2025-03-16 03:41:43.517463: Yayy! New best EMA pseudo Dice: 0.8788999915122986 
2025-03-16 03:41:44.069161:  
2025-03-16 03:41:44.075673: Epoch 11 
2025-03-16 03:41:44.078178: Current learning rate: 0.009 
2025-03-16 03:41:50.581806: train_loss -0.8717 
2025-03-16 03:41:50.587860: val_loss -0.8503 
2025-03-16 03:41:50.591472: Pseudo dice [np.float32(0.8965), np.float32(0.8815)] 
2025-03-16 03:41:50.594527: Epoch time: 6.51 s 
2025-03-16 03:41:50.598064: Yayy! New best EMA pseudo Dice: 0.8799999952316284 
2025-03-16 03:41:51.140557:  
2025-03-16 03:41:51.146146: Epoch 12 
2025-03-16 03:41:51.149743: Current learning rate: 0.00891 
2025-03-16 03:41:57.830702: train_loss -0.8751 
2025-03-16 03:41:57.836270: val_loss -0.8498 
2025-03-16 03:41:57.839303: Pseudo dice [np.float32(0.8983), np.float32(0.8794)] 
2025-03-16 03:41:57.842832: Epoch time: 6.69 s 
2025-03-16 03:41:57.844915: Yayy! New best EMA pseudo Dice: 0.8808000087738037 
2025-03-16 03:41:58.401393:  
2025-03-16 03:41:58.406947: Epoch 13 
2025-03-16 03:41:58.409540: Current learning rate: 0.00882 
2025-03-16 03:42:04.907964: train_loss -0.8768 
2025-03-16 03:42:04.914100: val_loss -0.8509 
2025-03-16 03:42:04.916620: Pseudo dice [np.float32(0.8994), np.float32(0.8808)] 
2025-03-16 03:42:04.920741: Epoch time: 6.51 s 
2025-03-16 03:42:04.923799: Yayy! New best EMA pseudo Dice: 0.8817999958992004 
2025-03-16 03:42:05.481175:  
2025-03-16 03:42:05.486253: Epoch 14 
2025-03-16 03:42:05.489793: Current learning rate: 0.00873 
2025-03-16 03:42:11.998872: train_loss -0.8796 
2025-03-16 03:42:12.005421: val_loss -0.8512 
2025-03-16 03:42:12.008959: Pseudo dice [np.float32(0.899), np.float32(0.8806)] 
2025-03-16 03:42:12.012000: Epoch time: 6.52 s 
2025-03-16 03:42:12.015587: Yayy! New best EMA pseudo Dice: 0.8826000094413757 
2025-03-16 03:42:12.587687:  
2025-03-16 03:42:12.593198: Epoch 15 
2025-03-16 03:42:12.596711: Current learning rate: 0.00864 
2025-03-16 03:42:19.132506: train_loss -0.8805 
2025-03-16 03:42:19.138300: val_loss -0.8454 
2025-03-16 03:42:19.141866: Pseudo dice [np.float32(0.895), np.float32(0.878)] 
2025-03-16 03:42:19.144941: Epoch time: 6.55 s 
2025-03-16 03:42:19.147452: Yayy! New best EMA pseudo Dice: 0.8830000162124634 
2025-03-16 03:42:19.722254:  
2025-03-16 03:42:19.728298: Epoch 16 
2025-03-16 03:42:19.731348: Current learning rate: 0.00855 
2025-03-16 03:42:26.244520: train_loss -0.8831 
2025-03-16 03:42:26.250606: val_loss -0.8445 
2025-03-16 03:42:26.254130: Pseudo dice [np.float32(0.894), np.float32(0.8776)] 
2025-03-16 03:42:26.256657: Epoch time: 6.52 s 
2025-03-16 03:42:26.260718: Yayy! New best EMA pseudo Dice: 0.8831999897956848 
2025-03-16 03:42:26.836205:  
2025-03-16 03:42:26.840243: Epoch 17 
2025-03-16 03:42:26.844827: Current learning rate: 0.00846 
2025-03-16 03:42:33.375308: train_loss -0.8824 
2025-03-16 03:42:33.380921: val_loss -0.8512 
2025-03-16 03:42:33.384436: Pseudo dice [np.float32(0.8981), np.float32(0.8817)] 
2025-03-16 03:42:33.387973: Epoch time: 6.54 s 
2025-03-16 03:42:33.390997: Yayy! New best EMA pseudo Dice: 0.883899986743927 
2025-03-16 03:42:33.964712:  
2025-03-16 03:42:33.970225: Epoch 18 
2025-03-16 03:42:33.973737: Current learning rate: 0.00836 
2025-03-16 03:42:40.496647: train_loss -0.8858 
2025-03-16 03:42:40.502706: val_loss -0.8446 
2025-03-16 03:42:40.506697: Pseudo dice [np.float32(0.8941), np.float32(0.8746)] 
2025-03-16 03:42:40.510245: Epoch time: 6.53 s 
2025-03-16 03:42:40.512771: Yayy! New best EMA pseudo Dice: 0.883899986743927 
2025-03-16 03:42:41.081740:  
2025-03-16 03:42:41.085770: Epoch 19 
2025-03-16 03:42:41.088966: Current learning rate: 0.00827 
2025-03-16 03:42:47.592281: train_loss -0.8867 
2025-03-16 03:42:47.598378: val_loss -0.8507 
2025-03-16 03:42:47.600885: Pseudo dice [np.float32(0.8984), np.float32(0.8805)] 
2025-03-16 03:42:47.604898: Epoch time: 6.51 s 
2025-03-16 03:42:47.607952: Yayy! New best EMA pseudo Dice: 0.8845000267028809 
2025-03-16 03:42:48.310008:  
2025-03-16 03:42:48.316029: Epoch 20 
2025-03-16 03:42:48.319835: Current learning rate: 0.00818 
2025-03-16 03:42:54.846620: train_loss -0.8889 
2025-03-16 03:42:54.852748: val_loss -0.8484 
2025-03-16 03:42:54.856325: Pseudo dice [np.float32(0.898), np.float32(0.8791)] 
2025-03-16 03:42:54.859360: Epoch time: 6.54 s 
2025-03-16 03:42:54.862870: Yayy! New best EMA pseudo Dice: 0.8848999738693237 
2025-03-16 03:42:55.452389:  
2025-03-16 03:42:55.457900: Epoch 21 
2025-03-16 03:42:55.461413: Current learning rate: 0.00809 
2025-03-16 03:43:01.958334: train_loss -0.8908 
2025-03-16 03:43:01.963890: val_loss -0.8464 
2025-03-16 03:43:01.967962: Pseudo dice [np.float32(0.8964), np.float32(0.8791)] 
2025-03-16 03:43:01.971014: Epoch time: 6.51 s 
2025-03-16 03:43:01.974117: Yayy! New best EMA pseudo Dice: 0.885200023651123 
2025-03-16 03:43:02.521413:  
2025-03-16 03:43:02.526467: Epoch 22 
2025-03-16 03:43:02.529506: Current learning rate: 0.008 
2025-03-16 03:43:09.027244: train_loss -0.8916 
2025-03-16 03:43:09.033329: val_loss -0.8461 
2025-03-16 03:43:09.035869: Pseudo dice [np.float32(0.896), np.float32(0.8767)] 
2025-03-16 03:43:09.039912: Epoch time: 6.51 s 
2025-03-16 03:43:09.042971: Yayy! New best EMA pseudo Dice: 0.8852999806404114 
2025-03-16 03:43:09.587651:  
2025-03-16 03:43:09.594174: Epoch 23 
2025-03-16 03:43:09.596683: Current learning rate: 0.0079 
2025-03-16 03:43:16.103611: train_loss -0.8916 
2025-03-16 03:43:16.109742: val_loss -0.8499 
2025-03-16 03:43:16.113276: Pseudo dice [np.float32(0.8984), np.float32(0.8807)] 
2025-03-16 03:43:16.115318: Epoch time: 6.52 s 
2025-03-16 03:43:16.119369: Yayy! New best EMA pseudo Dice: 0.885699987411499 
2025-03-16 03:43:16.677220:  
2025-03-16 03:43:16.682246: Epoch 24 
2025-03-16 03:43:16.684937: Current learning rate: 0.00781 
2025-03-16 03:43:23.206085: train_loss -0.8921 
2025-03-16 03:43:23.211756: val_loss -0.8381 
2025-03-16 03:43:23.215302: Pseudo dice [np.float32(0.891), np.float32(0.8735)] 
2025-03-16 03:43:23.218874: Epoch time: 6.53 s 
2025-03-16 03:43:23.734046:  
2025-03-16 03:43:23.740145: Epoch 25 
2025-03-16 03:43:23.743190: Current learning rate: 0.00772 
2025-03-16 03:43:30.256314: train_loss -0.8931 
2025-03-16 03:43:30.262440: val_loss -0.8481 
2025-03-16 03:43:30.266032: Pseudo dice [np.float32(0.8983), np.float32(0.8811)] 
2025-03-16 03:43:30.269156: Epoch time: 6.52 s 
2025-03-16 03:43:30.272214: Yayy! New best EMA pseudo Dice: 0.8858000040054321 
2025-03-16 03:43:30.828677:  
2025-03-16 03:43:30.834737: Epoch 26 
2025-03-16 03:43:30.837789: Current learning rate: 0.00763 
2025-03-16 03:43:37.357864: train_loss -0.8945 
2025-03-16 03:43:37.363960: val_loss -0.8498 
2025-03-16 03:43:37.367067: Pseudo dice [np.float32(0.8984), np.float32(0.8811)] 
2025-03-16 03:43:37.371126: Epoch time: 6.53 s 
2025-03-16 03:43:37.374189: Yayy! New best EMA pseudo Dice: 0.8862000107765198 
2025-03-16 03:43:37.925653:  
2025-03-16 03:43:37.931233: Epoch 27 
2025-03-16 03:43:37.933771: Current learning rate: 0.00753 
2025-03-16 03:43:44.435201: train_loss -0.8953 
2025-03-16 03:43:44.441295: val_loss -0.8438 
2025-03-16 03:43:44.444847: Pseudo dice [np.float32(0.8958), np.float32(0.8762)] 
2025-03-16 03:43:44.447927: Epoch time: 6.51 s 
2025-03-16 03:43:45.120367:  
2025-03-16 03:43:45.125396: Epoch 28 
2025-03-16 03:43:45.128961: Current learning rate: 0.00744 
2025-03-16 03:43:51.639274: train_loss -0.8959 
2025-03-16 03:43:51.645419: val_loss -0.845 
2025-03-16 03:43:51.647946: Pseudo dice [np.float32(0.8951), np.float32(0.8776)] 
2025-03-16 03:43:51.652103: Epoch time: 6.52 s 
2025-03-16 03:43:52.164646:  
2025-03-16 03:43:52.170711: Epoch 29 
2025-03-16 03:43:52.174793: Current learning rate: 0.00735 
2025-03-16 03:43:58.681278: train_loss -0.897 
2025-03-16 03:43:58.686941: val_loss -0.8518 
2025-03-16 03:43:58.689972: Pseudo dice [np.float32(0.8997), np.float32(0.8829)] 
2025-03-16 03:43:58.693622: Epoch time: 6.52 s 
2025-03-16 03:43:58.696707: Yayy! New best EMA pseudo Dice: 0.8866999745368958 
2025-03-16 03:43:59.261025:  
2025-03-16 03:43:59.266572: Epoch 30 
2025-03-16 03:43:59.270114: Current learning rate: 0.00725 
2025-03-16 03:44:05.796596: train_loss -0.8985 
2025-03-16 03:44:05.802325: val_loss -0.8455 
2025-03-16 03:44:05.805860: Pseudo dice [np.float32(0.8948), np.float32(0.8791)] 
2025-03-16 03:44:05.809451: Epoch time: 6.54 s 
2025-03-16 03:44:05.812491: Yayy! New best EMA pseudo Dice: 0.8866999745368958 
2025-03-16 03:44:06.380736:  
2025-03-16 03:44:06.386744: Epoch 31 
2025-03-16 03:44:06.390758: Current learning rate: 0.00716 
2025-03-16 03:44:12.906205: train_loss -0.9004 
2025-03-16 03:44:12.912358: val_loss -0.8448 
2025-03-16 03:44:12.915889: Pseudo dice [np.float32(0.8951), np.float32(0.8787)] 
2025-03-16 03:44:12.919429: Epoch time: 6.53 s 
2025-03-16 03:44:12.922462: Yayy! New best EMA pseudo Dice: 0.8867999911308289 
2025-03-16 03:44:13.502350:  
2025-03-16 03:44:13.507359: Epoch 32 
2025-03-16 03:44:13.510370: Current learning rate: 0.00707 
2025-03-16 03:44:20.011543: train_loss -0.8991 
2025-03-16 03:44:20.018089: val_loss -0.8435 
2025-03-16 03:44:20.023112: Pseudo dice [np.float32(0.8962), np.float32(0.8772)] 
2025-03-16 03:44:20.026624: Epoch time: 6.51 s 
2025-03-16 03:44:20.553398:  
2025-03-16 03:44:20.557442: Epoch 33 
2025-03-16 03:44:20.561508: Current learning rate: 0.00697 
2025-03-16 03:44:27.072949: train_loss -0.902 
2025-03-16 03:44:27.078857: val_loss -0.844 
2025-03-16 03:44:27.082871: Pseudo dice [np.float32(0.8944), np.float32(0.8786)] 
2025-03-16 03:44:27.087425: Epoch time: 6.52 s 
2025-03-16 03:44:27.609250:  
2025-03-16 03:44:27.614293: Epoch 34 
2025-03-16 03:44:27.617855: Current learning rate: 0.00688 
2025-03-16 03:44:34.132309: train_loss -0.9031 
2025-03-16 03:44:34.137916: val_loss -0.846 
2025-03-16 03:44:34.141971: Pseudo dice [np.float32(0.897), np.float32(0.8803)] 
2025-03-16 03:44:34.145519: Epoch time: 6.52 s 
2025-03-16 03:44:34.148530: Yayy! New best EMA pseudo Dice: 0.886900007724762 
2025-03-16 03:44:34.710724:  
2025-03-16 03:44:34.716270: Epoch 35 
2025-03-16 03:44:34.719371: Current learning rate: 0.00679 
2025-03-16 03:44:41.226027: train_loss -0.9013 
2025-03-16 03:44:41.231586: val_loss -0.8466 
2025-03-16 03:44:41.235126: Pseudo dice [np.float32(0.898), np.float32(0.8791)] 
2025-03-16 03:44:41.238655: Epoch time: 6.52 s 
2025-03-16 03:44:41.241688: Yayy! New best EMA pseudo Dice: 0.8870999813079834 
2025-03-16 03:44:41.948058:  
2025-03-16 03:44:41.953279: Epoch 36 
2025-03-16 03:44:41.956798: Current learning rate: 0.00669 
2025-03-16 03:44:48.476495: train_loss -0.9029 
2025-03-16 03:44:48.482609: val_loss -0.849 
2025-03-16 03:44:48.486704: Pseudo dice [np.float32(0.8994), np.float32(0.8815)] 
2025-03-16 03:44:48.489732: Epoch time: 6.53 s 
2025-03-16 03:44:48.492762: Yayy! New best EMA pseudo Dice: 0.8873999714851379 
2025-03-16 03:44:49.059689:  
2025-03-16 03:44:49.064788: Epoch 37 
2025-03-16 03:44:49.068456: Current learning rate: 0.0066 
2025-03-16 03:44:55.573340: train_loss -0.9038 
2025-03-16 03:44:55.578966: val_loss -0.8478 
2025-03-16 03:44:55.583018: Pseudo dice [np.float32(0.8977), np.float32(0.8812)] 
2025-03-16 03:44:55.586053: Epoch time: 6.51 s 
2025-03-16 03:44:55.589130: Yayy! New best EMA pseudo Dice: 0.8876000046730042 
2025-03-16 03:44:56.163199:  
2025-03-16 03:44:56.168240: Epoch 38 
2025-03-16 03:44:56.171837: Current learning rate: 0.0065 
2025-03-16 03:45:02.685850: train_loss -0.9048 
2025-03-16 03:45:02.691934: val_loss -0.8388 
2025-03-16 03:45:02.695494: Pseudo dice [np.float32(0.8925), np.float32(0.8758)] 
2025-03-16 03:45:02.699056: Epoch time: 6.52 s 
2025-03-16 03:45:03.233186:  
2025-03-16 03:45:03.240756: Epoch 39 
2025-03-16 03:45:03.244270: Current learning rate: 0.00641 
2025-03-16 03:45:09.772174: train_loss -0.9056 
2025-03-16 03:45:09.777236: val_loss -0.8391 
2025-03-16 03:45:09.780791: Pseudo dice [np.float32(0.8925), np.float32(0.8771)] 
2025-03-16 03:45:09.785396: Epoch time: 6.54 s 
2025-03-16 03:45:10.327882:  
2025-03-16 03:45:10.333443: Epoch 40 
2025-03-16 03:45:10.337022: Current learning rate: 0.00631 
2025-03-16 03:45:16.825347: train_loss -0.9054 
2025-03-16 03:45:16.830953: val_loss -0.846 
2025-03-16 03:45:16.835054: Pseudo dice [np.float32(0.897), np.float32(0.8799)] 
2025-03-16 03:45:16.838005: Epoch time: 6.5 s 
2025-03-16 03:45:17.388116:  
2025-03-16 03:45:17.394166: Epoch 41 
2025-03-16 03:45:17.397218: Current learning rate: 0.00622 
2025-03-16 03:45:23.906265: train_loss -0.9056 
2025-03-16 03:45:23.911885: val_loss -0.8504 
2025-03-16 03:45:23.915576: Pseudo dice [np.float32(0.8993), np.float32(0.8821)] 
2025-03-16 03:45:23.918108: Epoch time: 6.52 s 
2025-03-16 03:45:24.443593:  
2025-03-16 03:45:24.449132: Epoch 42 
2025-03-16 03:45:24.452645: Current learning rate: 0.00612 
2025-03-16 03:45:30.962838: train_loss -0.9045 
2025-03-16 03:45:30.968905: val_loss -0.8427 
2025-03-16 03:45:30.972457: Pseudo dice [np.float32(0.8959), np.float32(0.8766)] 
2025-03-16 03:45:30.975512: Epoch time: 6.52 s 
2025-03-16 03:45:31.488842:  
2025-03-16 03:45:31.494928: Epoch 43 
2025-03-16 03:45:31.497962: Current learning rate: 0.00603 
2025-03-16 03:45:38.178915: train_loss -0.9066 
2025-03-16 03:45:38.184936: val_loss -0.8486 
2025-03-16 03:45:38.188447: Pseudo dice [np.float32(0.8998), np.float32(0.8816)] 
2025-03-16 03:45:38.191462: Epoch time: 6.69 s 
2025-03-16 03:45:38.194973: Yayy! New best EMA pseudo Dice: 0.8877000212669373 
2025-03-16 03:45:38.743722:  
2025-03-16 03:45:38.749813: Epoch 44 
2025-03-16 03:45:38.752892: Current learning rate: 0.00593 
2025-03-16 03:45:45.223972: train_loss -0.9074 
2025-03-16 03:45:45.230894: val_loss -0.8465 
2025-03-16 03:45:45.234403: Pseudo dice [np.float32(0.8972), np.float32(0.8799)] 
2025-03-16 03:45:45.237412: Epoch time: 6.48 s 
2025-03-16 03:45:45.240922: Yayy! New best EMA pseudo Dice: 0.8877999782562256 
2025-03-16 03:45:45.794523:  
2025-03-16 03:45:45.799553: Epoch 45 
2025-03-16 03:45:45.802164: Current learning rate: 0.00584 
2025-03-16 03:45:52.304832: train_loss -0.9081 
2025-03-16 03:45:52.310410: val_loss -0.84 
2025-03-16 03:45:52.314046: Pseudo dice [np.float32(0.8937), np.float32(0.8774)] 
2025-03-16 03:45:52.317117: Epoch time: 6.51 s 
2025-03-16 03:45:52.824729:  
2025-03-16 03:45:52.830776: Epoch 46 
2025-03-16 03:45:52.833865: Current learning rate: 0.00574 
2025-03-16 03:45:59.339284: train_loss -0.9079 
2025-03-16 03:45:59.345522: val_loss -0.843 
2025-03-16 03:45:59.349054: Pseudo dice [np.float32(0.8969), np.float32(0.8786)] 
2025-03-16 03:45:59.352635: Epoch time: 6.51 s 
2025-03-16 03:45:59.865699:  
2025-03-16 03:45:59.871223: Epoch 47 
2025-03-16 03:45:59.874287: Current learning rate: 0.00565 
2025-03-16 03:46:06.378736: train_loss -0.9085 
2025-03-16 03:46:06.384852: val_loss -0.8456 
2025-03-16 03:46:06.388381: Pseudo dice [np.float32(0.8973), np.float32(0.8805)] 
2025-03-16 03:46:06.391424: Epoch time: 6.51 s 
2025-03-16 03:46:06.898978:  
2025-03-16 03:46:06.905542: Epoch 48 
2025-03-16 03:46:06.908086: Current learning rate: 0.00555 
2025-03-16 03:46:13.415138: train_loss -0.9097 
2025-03-16 03:46:13.420646: val_loss -0.8456 
2025-03-16 03:46:13.424203: Pseudo dice [np.float32(0.8987), np.float32(0.8798)] 
2025-03-16 03:46:13.427280: Epoch time: 6.52 s 
2025-03-16 03:46:13.430818: Yayy! New best EMA pseudo Dice: 0.8878999948501587 
2025-03-16 03:46:13.983145:  
2025-03-16 03:46:13.988719: Epoch 49 
2025-03-16 03:46:13.992312: Current learning rate: 0.00546 
2025-03-16 03:46:20.510458: train_loss -0.9103 
2025-03-16 03:46:20.516491: val_loss -0.8414 
2025-03-16 03:46:20.519504: Pseudo dice [np.float32(0.896), np.float32(0.8768)] 
2025-03-16 03:46:20.523030: Epoch time: 6.53 s 
2025-03-16 03:46:21.070565:  
2025-03-16 03:46:21.076112: Epoch 50 
2025-03-16 03:46:21.079666: Current learning rate: 0.00536 
2025-03-16 03:46:27.582263: train_loss -0.908 
2025-03-16 03:46:27.588330: val_loss -0.842 
2025-03-16 03:46:27.591361: Pseudo dice [np.float32(0.8967), np.float32(0.8774)] 
2025-03-16 03:46:27.594890: Epoch time: 6.51 s 
2025-03-16 03:46:28.134973:  
2025-03-16 03:46:28.140526: Epoch 51 
2025-03-16 03:46:28.143071: Current learning rate: 0.00526 
2025-03-16 03:46:34.655203: train_loss -0.9086 
2025-03-16 03:46:34.659837: val_loss -0.8418 
2025-03-16 03:46:34.663893: Pseudo dice [np.float32(0.8949), np.float32(0.8791)] 
2025-03-16 03:46:34.666983: Epoch time: 6.52 s 
2025-03-16 03:46:35.327709:  
2025-03-16 03:46:35.332341: Epoch 52 
2025-03-16 03:46:35.336879: Current learning rate: 0.00517 
2025-03-16 03:46:41.842992: train_loss -0.9106 
2025-03-16 03:46:41.850180: val_loss -0.8442 
2025-03-16 03:46:41.854278: Pseudo dice [np.float32(0.8977), np.float32(0.8794)] 
2025-03-16 03:46:41.857863: Epoch time: 6.52 s 
2025-03-16 03:46:42.376790:  
2025-03-16 03:46:42.380822: Epoch 53 
2025-03-16 03:46:42.383400: Current learning rate: 0.00507 
2025-03-16 03:46:48.877946: train_loss -0.9106 
2025-03-16 03:46:48.884563: val_loss -0.8442 
2025-03-16 03:46:48.887599: Pseudo dice [np.float32(0.8984), np.float32(0.8791)] 
2025-03-16 03:46:48.891135: Epoch time: 6.5 s 
2025-03-16 03:46:49.412416:  
2025-03-16 03:46:49.418475: Epoch 54 
2025-03-16 03:46:49.421534: Current learning rate: 0.00497 
2025-03-16 03:46:55.950903: train_loss -0.9125 
2025-03-16 03:46:55.957599: val_loss -0.8424 
2025-03-16 03:46:55.961129: Pseudo dice [np.float32(0.8953), np.float32(0.879)] 
2025-03-16 03:46:55.964658: Epoch time: 6.54 s 
2025-03-16 03:46:56.490276:  
2025-03-16 03:46:56.494801: Epoch 55 
2025-03-16 03:46:56.497841: Current learning rate: 0.00487 
2025-03-16 03:47:03.034599: train_loss -0.9132 
2025-03-16 03:47:03.041300: val_loss -0.8375 
2025-03-16 03:47:03.045360: Pseudo dice [np.float32(0.8938), np.float32(0.8742)] 
2025-03-16 03:47:03.048381: Epoch time: 6.54 s 
2025-03-16 03:47:03.565340:  
2025-03-16 03:47:03.569888: Epoch 56 
2025-03-16 03:47:03.572953: Current learning rate: 0.00478 
2025-03-16 03:47:10.060440: train_loss -0.9145 
2025-03-16 03:47:10.066034: val_loss -0.8394 
2025-03-16 03:47:10.069642: Pseudo dice [np.float32(0.8945), np.float32(0.8759)] 
2025-03-16 03:47:10.073197: Epoch time: 6.5 s 
2025-03-16 03:47:10.593346:  
2025-03-16 03:47:10.596897: Epoch 57 
2025-03-16 03:47:10.599447: Current learning rate: 0.00468 
2025-03-16 03:47:17.131906: train_loss -0.9136 
2025-03-16 03:47:17.137465: val_loss -0.8443 
2025-03-16 03:47:17.141502: Pseudo dice [np.float32(0.8971), np.float32(0.8798)] 
2025-03-16 03:47:17.144578: Epoch time: 6.54 s 
2025-03-16 03:47:17.664629:  
2025-03-16 03:47:17.668707: Epoch 58 
2025-03-16 03:47:17.671257: Current learning rate: 0.00458 
2025-03-16 03:47:24.159151: train_loss -0.9142 
2025-03-16 03:47:24.164728: val_loss -0.8455 
2025-03-16 03:47:24.168294: Pseudo dice [np.float32(0.8988), np.float32(0.8793)] 
2025-03-16 03:47:24.171340: Epoch time: 6.49 s 
2025-03-16 03:47:24.700329:  
2025-03-16 03:47:24.705927: Epoch 59 
2025-03-16 03:47:24.709458: Current learning rate: 0.00448 
2025-03-16 03:47:31.234741: train_loss -0.9137 
2025-03-16 03:47:31.240808: val_loss -0.8436 
2025-03-16 03:47:31.243830: Pseudo dice [np.float32(0.8985), np.float32(0.88)] 
2025-03-16 03:47:31.246723: Epoch time: 6.53 s 
2025-03-16 03:47:31.920324:  
2025-03-16 03:47:31.925333: Epoch 60 
2025-03-16 03:47:31.929349: Current learning rate: 0.00438 
2025-03-16 03:47:38.398510: train_loss -0.9144 
2025-03-16 03:47:38.404067: val_loss -0.8443 
2025-03-16 03:47:38.407094: Pseudo dice [np.float32(0.8986), np.float32(0.8806)] 
2025-03-16 03:47:38.410613: Epoch time: 6.48 s 
2025-03-16 03:47:38.942803:  
2025-03-16 03:47:38.948819: Epoch 61 
2025-03-16 03:47:38.952829: Current learning rate: 0.00429 
2025-03-16 03:47:45.485029: train_loss -0.9146 
2025-03-16 03:47:45.491569: val_loss -0.8393 
2025-03-16 03:47:45.495095: Pseudo dice [np.float32(0.8957), np.float32(0.878)] 
2025-03-16 03:47:45.497609: Epoch time: 6.54 s 
2025-03-16 03:47:46.027645:  
2025-03-16 03:47:46.033688: Epoch 62 
2025-03-16 03:47:46.036198: Current learning rate: 0.00419 
2025-03-16 03:47:52.565776: train_loss -0.916 
2025-03-16 03:47:52.571896: val_loss -0.8402 
2025-03-16 03:47:52.574947: Pseudo dice [np.float32(0.8947), np.float32(0.8774)] 
2025-03-16 03:47:52.578501: Epoch time: 6.54 s 
2025-03-16 03:47:53.111205:  
2025-03-16 03:47:53.116718: Epoch 63 
2025-03-16 03:47:53.119225: Current learning rate: 0.00409 
2025-03-16 03:47:59.623717: train_loss -0.9172 
2025-03-16 03:47:59.629814: val_loss -0.8404 
2025-03-16 03:47:59.633374: Pseudo dice [np.float32(0.8959), np.float32(0.8772)] 
2025-03-16 03:47:59.636509: Epoch time: 6.51 s 
2025-03-16 03:48:00.166311:  
2025-03-16 03:48:00.172442: Epoch 64 
2025-03-16 03:48:00.175487: Current learning rate: 0.00399 
2025-03-16 03:48:06.698782: train_loss -0.9147 
2025-03-16 03:48:06.703818: val_loss -0.843 
2025-03-16 03:48:06.708924: Pseudo dice [np.float32(0.8983), np.float32(0.8797)] 
2025-03-16 03:48:06.711454: Epoch time: 6.53 s 
2025-03-16 03:48:07.240124:  
2025-03-16 03:48:07.245649: Epoch 65 
2025-03-16 03:48:07.248679: Current learning rate: 0.00389 
2025-03-16 03:48:13.783485: train_loss -0.9153 
2025-03-16 03:48:13.789081: val_loss -0.8382 
2025-03-16 03:48:13.792642: Pseudo dice [np.float32(0.8938), np.float32(0.8769)] 
2025-03-16 03:48:13.796306: Epoch time: 6.54 s 
2025-03-16 03:48:14.323461:  
2025-03-16 03:48:14.329039: Epoch 66 
2025-03-16 03:48:14.332872: Current learning rate: 0.00379 
2025-03-16 03:48:20.836600: train_loss -0.9167 
2025-03-16 03:48:20.843145: val_loss -0.8404 
2025-03-16 03:48:20.846711: Pseudo dice [np.float32(0.8958), np.float32(0.8774)] 
2025-03-16 03:48:20.849762: Epoch time: 6.51 s 
2025-03-16 03:48:21.525571:  
2025-03-16 03:48:21.530581: Epoch 67 
2025-03-16 03:48:21.534098: Current learning rate: 0.00369 
2025-03-16 03:48:28.017868: train_loss -0.9165 
2025-03-16 03:48:28.023966: val_loss -0.8353 
2025-03-16 03:48:28.027487: Pseudo dice [np.float32(0.893), np.float32(0.8762)] 
2025-03-16 03:48:28.030548: Epoch time: 6.49 s 
2025-03-16 03:48:28.567343:  
2025-03-16 03:48:28.573532: Epoch 68 
2025-03-16 03:48:28.576040: Current learning rate: 0.00359 
2025-03-16 03:48:35.103590: train_loss -0.9181 
2025-03-16 03:48:35.109204: val_loss -0.8399 
2025-03-16 03:48:35.112792: Pseudo dice [np.float32(0.894), np.float32(0.8783)] 
2025-03-16 03:48:35.115847: Epoch time: 6.54 s 
2025-03-16 03:48:35.652385:  
2025-03-16 03:48:35.657920: Epoch 69 
2025-03-16 03:48:35.660955: Current learning rate: 0.00349 
2025-03-16 03:48:42.204401: train_loss -0.9182 
2025-03-16 03:48:42.212083: val_loss -0.8397 
2025-03-16 03:48:42.215136: Pseudo dice [np.float32(0.8966), np.float32(0.8771)] 
2025-03-16 03:48:42.218670: Epoch time: 6.55 s 
2025-03-16 03:48:42.752030:  
2025-03-16 03:48:42.757040: Epoch 70 
2025-03-16 03:48:42.760552: Current learning rate: 0.00338 
2025-03-16 03:48:49.265478: train_loss -0.9179 
2025-03-16 03:48:49.271617: val_loss -0.8404 
2025-03-16 03:48:49.275682: Pseudo dice [np.float32(0.8958), np.float32(0.8781)] 
2025-03-16 03:48:49.280743: Epoch time: 6.51 s 
2025-03-16 03:48:49.819296:  
2025-03-16 03:48:49.824893: Epoch 71 
2025-03-16 03:48:49.827427: Current learning rate: 0.00328 
2025-03-16 03:48:56.304621: train_loss -0.9192 
2025-03-16 03:48:56.309723: val_loss -0.8384 
2025-03-16 03:48:56.314316: Pseudo dice [np.float32(0.8948), np.float32(0.8774)] 
2025-03-16 03:48:56.317871: Epoch time: 6.49 s 
2025-03-16 03:48:56.860017:  
2025-03-16 03:48:56.865026: Epoch 72 
2025-03-16 03:48:56.868542: Current learning rate: 0.00318 
2025-03-16 03:49:03.380080: train_loss -0.919 
2025-03-16 03:49:03.385606: val_loss -0.8377 
2025-03-16 03:49:03.389619: Pseudo dice [np.float32(0.8951), np.float32(0.8762)] 
2025-03-16 03:49:03.392649: Epoch time: 6.52 s 
2025-03-16 03:49:03.937222:  
2025-03-16 03:49:03.942750: Epoch 73 
2025-03-16 03:49:03.945849: Current learning rate: 0.00308 
2025-03-16 03:49:10.426436: train_loss -0.9185 
2025-03-16 03:49:10.432468: val_loss -0.841 
2025-03-16 03:49:10.435995: Pseudo dice [np.float32(0.8961), np.float32(0.8781)] 
2025-03-16 03:49:10.439545: Epoch time: 6.49 s 
2025-03-16 03:49:10.987825:  
2025-03-16 03:49:10.993337: Epoch 74 
2025-03-16 03:49:10.996847: Current learning rate: 0.00297 
2025-03-16 03:49:17.530279: train_loss -0.9191 
2025-03-16 03:49:17.535879: val_loss -0.8393 
2025-03-16 03:49:17.540037: Pseudo dice [np.float32(0.8964), np.float32(0.8773)] 
2025-03-16 03:49:17.543098: Epoch time: 6.54 s 
2025-03-16 03:49:18.221811:  
2025-03-16 03:49:18.226820: Epoch 75 
2025-03-16 03:49:18.230332: Current learning rate: 0.00287 
2025-03-16 03:49:24.711790: train_loss -0.9191 
2025-03-16 03:49:24.717868: val_loss -0.8372 
2025-03-16 03:49:24.721503: Pseudo dice [np.float32(0.8945), np.float32(0.8782)] 
2025-03-16 03:49:24.724056: Epoch time: 6.49 s 
2025-03-16 03:49:25.265357:  
2025-03-16 03:49:25.270869: Epoch 76 
2025-03-16 03:49:25.273377: Current learning rate: 0.00277 
2025-03-16 03:49:31.768363: train_loss -0.9197 
2025-03-16 03:49:31.774405: val_loss -0.8374 
2025-03-16 03:49:31.776862: Pseudo dice [np.float32(0.8952), np.float32(0.8762)] 
2025-03-16 03:49:31.781470: Epoch time: 6.5 s 
2025-03-16 03:49:32.323262:  
2025-03-16 03:49:32.328810: Epoch 77 
2025-03-16 03:49:32.332345: Current learning rate: 0.00266 
2025-03-16 03:49:38.876655: train_loss -0.9204 
2025-03-16 03:49:38.882267: val_loss -0.8368 
2025-03-16 03:49:38.885833: Pseudo dice [np.float32(0.8958), np.float32(0.877)] 
2025-03-16 03:49:38.889376: Epoch time: 6.55 s 
2025-03-16 03:49:39.433988:  
2025-03-16 03:49:39.439515: Epoch 78 
2025-03-16 03:49:39.442571: Current learning rate: 0.00256 
2025-03-16 03:49:45.942233: train_loss -0.9203 
2025-03-16 03:49:45.949873: val_loss -0.8416 
2025-03-16 03:49:45.953421: Pseudo dice [np.float32(0.897), np.float32(0.8795)] 
2025-03-16 03:49:45.956996: Epoch time: 6.51 s 
2025-03-16 03:49:46.499357:  
2025-03-16 03:49:46.504869: Epoch 79 
2025-03-16 03:49:46.508382: Current learning rate: 0.00245 
2025-03-16 03:49:53.004215: train_loss -0.9201 
2025-03-16 03:49:53.010233: val_loss -0.8359 
2025-03-16 03:49:53.013741: Pseudo dice [np.float32(0.8951), np.float32(0.8749)] 
2025-03-16 03:49:53.016783: Epoch time: 6.51 s 
2025-03-16 03:49:53.564324:  
2025-03-16 03:49:53.569859: Epoch 80 
2025-03-16 03:49:53.573377: Current learning rate: 0.00235 
2025-03-16 03:50:00.100787: train_loss -0.921 
2025-03-16 03:50:00.106364: val_loss -0.8372 
2025-03-16 03:50:00.110420: Pseudo dice [np.float32(0.8952), np.float32(0.8784)] 
2025-03-16 03:50:00.113466: Epoch time: 6.54 s 
2025-03-16 03:50:00.659624:  
2025-03-16 03:50:00.665277: Epoch 81 
2025-03-16 03:50:00.669346: Current learning rate: 0.00224 
2025-03-16 03:50:07.197085: train_loss -0.9216 
2025-03-16 03:50:07.202606: val_loss -0.8363 
2025-03-16 03:50:07.206118: Pseudo dice [np.float32(0.8945), np.float32(0.8757)] 
2025-03-16 03:50:07.210246: Epoch time: 6.54 s 
2025-03-16 03:50:07.897792:  
2025-03-16 03:50:07.902801: Epoch 82 
2025-03-16 03:50:07.905812: Current learning rate: 0.00214 
2025-03-16 03:50:14.395700: train_loss -0.92 
2025-03-16 03:50:14.402245: val_loss -0.8341 
2025-03-16 03:50:14.404771: Pseudo dice [np.float32(0.893), np.float32(0.8746)] 
2025-03-16 03:50:14.408535: Epoch time: 6.5 s 
2025-03-16 03:50:14.925858:  
2025-03-16 03:50:14.929881: Epoch 83 
2025-03-16 03:50:14.932387: Current learning rate: 0.00203 
2025-03-16 03:50:21.446983: train_loss -0.9223 
2025-03-16 03:50:21.452077: val_loss -0.8389 
2025-03-16 03:50:21.456244: Pseudo dice [np.float32(0.8958), np.float32(0.8792)] 
2025-03-16 03:50:21.459868: Epoch time: 6.52 s 
2025-03-16 03:50:21.988744:  
2025-03-16 03:50:21.993767: Epoch 84 
2025-03-16 03:50:21.996812: Current learning rate: 0.00192 
2025-03-16 03:50:28.525876: train_loss -0.9228 
2025-03-16 03:50:28.531960: val_loss -0.8435 
2025-03-16 03:50:28.535018: Pseudo dice [np.float32(0.8995), np.float32(0.8807)] 
2025-03-16 03:50:28.538072: Epoch time: 6.54 s 
2025-03-16 03:50:29.051127:  
2025-03-16 03:50:29.055161: Epoch 85 
2025-03-16 03:50:29.060284: Current learning rate: 0.00181 
2025-03-16 03:50:35.572837: train_loss -0.9222 
2025-03-16 03:50:35.578411: val_loss -0.8406 
2025-03-16 03:50:35.581985: Pseudo dice [np.float32(0.898), np.float32(0.8778)] 
2025-03-16 03:50:35.585101: Epoch time: 6.52 s 
2025-03-16 03:50:36.099645:  
2025-03-16 03:50:36.103155: Epoch 86 
2025-03-16 03:50:36.107164: Current learning rate: 0.0017 
2025-03-16 03:50:42.630543: train_loss -0.9225 
2025-03-16 03:50:42.637093: val_loss -0.8374 
2025-03-16 03:50:42.640203: Pseudo dice [np.float32(0.895), np.float32(0.878)] 
2025-03-16 03:50:42.643270: Epoch time: 6.53 s 
2025-03-16 03:50:43.154559:  
2025-03-16 03:50:43.157566: Epoch 87 
2025-03-16 03:50:43.161078: Current learning rate: 0.00159 
2025-03-16 03:50:49.680383: train_loss -0.9231 
2025-03-16 03:50:49.685944: val_loss -0.8336 
2025-03-16 03:50:49.688983: Pseudo dice [np.float32(0.8947), np.float32(0.8743)] 
2025-03-16 03:50:49.692020: Epoch time: 6.53 s 
2025-03-16 03:50:50.203488:  
2025-03-16 03:50:50.207512: Epoch 88 
2025-03-16 03:50:50.211180: Current learning rate: 0.00148 
2025-03-16 03:50:56.738585: train_loss -0.9246 
2025-03-16 03:50:56.744635: val_loss -0.837 
2025-03-16 03:50:56.748216: Pseudo dice [np.float32(0.8953), np.float32(0.8772)] 
2025-03-16 03:50:56.751340: Epoch time: 6.54 s 
2025-03-16 03:50:57.262914:  
2025-03-16 03:50:57.268426: Epoch 89 
2025-03-16 03:50:57.271940: Current learning rate: 0.00137 
2025-03-16 03:51:03.797819: train_loss -0.9215 
2025-03-16 03:51:03.803841: val_loss -0.8411 
2025-03-16 03:51:03.807348: Pseudo dice [np.float32(0.8958), np.float32(0.8801)] 
2025-03-16 03:51:03.810369: Epoch time: 6.54 s 
2025-03-16 03:51:04.322302:  
2025-03-16 03:51:04.328383: Epoch 90 
2025-03-16 03:51:04.330920: Current learning rate: 0.00126 
2025-03-16 03:51:10.853274: train_loss -0.9249 
2025-03-16 03:51:10.858816: val_loss -0.8366 
2025-03-16 03:51:10.862336: Pseudo dice [np.float32(0.8953), np.float32(0.877)] 
2025-03-16 03:51:10.865864: Epoch time: 6.53 s 
2025-03-16 03:51:11.518750:  
2025-03-16 03:51:11.524292: Epoch 91 
2025-03-16 03:51:11.526839: Current learning rate: 0.00115 
2025-03-16 03:51:18.015058: train_loss -0.9237 
2025-03-16 03:51:18.020665: val_loss -0.8348 
2025-03-16 03:51:18.023229: Pseudo dice [np.float32(0.8941), np.float32(0.8756)] 
2025-03-16 03:51:18.028328: Epoch time: 6.5 s 
2025-03-16 03:51:18.539846:  
2025-03-16 03:51:18.544860: Epoch 92 
2025-03-16 03:51:18.548366: Current learning rate: 0.00103 
2025-03-16 03:51:25.066678: train_loss -0.9247 
2025-03-16 03:51:25.072216: val_loss -0.8286 
2025-03-16 03:51:25.075761: Pseudo dice [np.float32(0.891), np.float32(0.8728)] 
2025-03-16 03:51:25.078810: Epoch time: 6.53 s 
2025-03-16 03:51:25.595107:  
2025-03-16 03:51:25.600257: Epoch 93 
2025-03-16 03:51:25.603836: Current learning rate: 0.00091 
2025-03-16 03:51:32.126452: train_loss -0.9245 
2025-03-16 03:51:32.132664: val_loss -0.8377 
2025-03-16 03:51:32.135735: Pseudo dice [np.float32(0.8969), np.float32(0.8777)] 
2025-03-16 03:51:32.139775: Epoch time: 6.53 s 
2025-03-16 03:51:32.663531:  
2025-03-16 03:51:32.668547: Epoch 94 
2025-03-16 03:51:32.672060: Current learning rate: 0.00079 
2025-03-16 03:51:39.190645: train_loss -0.9241 
2025-03-16 03:51:39.196176: val_loss -0.832 
2025-03-16 03:51:39.200201: Pseudo dice [np.float32(0.8917), np.float32(0.8753)] 
2025-03-16 03:51:39.203212: Epoch time: 6.53 s 
2025-03-16 03:51:39.715092:  
2025-03-16 03:51:39.720678: Epoch 95 
2025-03-16 03:51:39.724236: Current learning rate: 0.00067 
2025-03-16 03:51:46.262992: train_loss -0.9245 
2025-03-16 03:51:46.269088: val_loss -0.8346 
2025-03-16 03:51:46.272641: Pseudo dice [np.float32(0.8936), np.float32(0.8763)] 
2025-03-16 03:51:46.275708: Epoch time: 6.55 s 
2025-03-16 03:51:46.787162:  
2025-03-16 03:51:46.793672: Epoch 96 
2025-03-16 03:51:46.797184: Current learning rate: 0.00055 
2025-03-16 03:51:53.330003: train_loss -0.9247 
2025-03-16 03:51:53.336150: val_loss -0.8305 
2025-03-16 03:51:53.341206: Pseudo dice [np.float32(0.892), np.float32(0.8734)] 
2025-03-16 03:51:53.344725: Epoch time: 6.54 s 
2025-03-16 03:51:53.860910:  
2025-03-16 03:51:53.866939: Epoch 97 
2025-03-16 03:51:53.870540: Current learning rate: 0.00043 
2025-03-16 03:52:00.412187: train_loss -0.9245 
2025-03-16 03:52:00.418205: val_loss -0.837 
2025-03-16 03:52:00.422216: Pseudo dice [np.float32(0.8961), np.float32(0.8785)] 
2025-03-16 03:52:00.424755: Epoch time: 6.55 s 
2025-03-16 03:52:00.947671:  
2025-03-16 03:52:00.953711: Epoch 98 
2025-03-16 03:52:00.956261: Current learning rate: 0.0003 
2025-03-16 03:52:07.455034: train_loss -0.9245 
2025-03-16 03:52:07.460605: val_loss -0.8422 
2025-03-16 03:52:07.464666: Pseudo dice [np.float32(0.8989), np.float32(0.8798)] 
2025-03-16 03:52:07.467176: Epoch time: 6.51 s 
2025-03-16 03:52:08.128920:  
2025-03-16 03:52:08.134493: Epoch 99 
2025-03-16 03:52:08.137536: Current learning rate: 0.00016 
2025-03-16 03:52:14.674736: train_loss -0.9252 
2025-03-16 03:52:14.681296: val_loss -0.8324 
2025-03-16 03:52:14.684806: Pseudo dice [np.float32(0.8924), np.float32(0.8753)] 
2025-03-16 03:52:14.687991: Epoch time: 6.55 s 
2025-03-16 03:52:15.259946: Training done. 
2025-03-16 03:52:15.293951: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-16 03:52:15.301461: The split file contains 5 splits. 
2025-03-16 03:52:15.305462: Desired fold for training: 0 
2025-03-16 03:52:15.310463: This split has 208 training and 52 validation cases. 
2025-03-16 03:52:15.316461: predicting hippocampus_017 
2025-03-16 03:52:15.321460: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-16 03:52:15.472969: predicting hippocampus_019 
2025-03-16 03:52:15.478969: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-16 03:52:15.555480: predicting hippocampus_033 
2025-03-16 03:52:15.562479: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-16 03:52:15.605993: predicting hippocampus_035 
2025-03-16 03:52:15.611993: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-16 03:52:15.656995: predicting hippocampus_037 
2025-03-16 03:52:15.662993: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-16 03:52:15.708506: predicting hippocampus_049 
2025-03-16 03:52:15.714505: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-16 03:52:15.760506: predicting hippocampus_052 
2025-03-16 03:52:15.767506: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-16 03:52:15.815019: predicting hippocampus_065 
2025-03-16 03:52:15.822019: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-16 03:52:15.869019: predicting hippocampus_083 
2025-03-16 03:52:15.876019: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-16 03:52:15.902100: predicting hippocampus_088 
2025-03-16 03:52:15.908100: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-16 03:52:19.046945: predicting hippocampus_090 
2025-03-16 03:52:19.052951: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-16 03:52:19.107460: predicting hippocampus_092 
2025-03-16 03:52:19.115460: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-16 03:52:19.178460: predicting hippocampus_095 
2025-03-16 03:52:19.186465: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-16 03:52:19.246974: predicting hippocampus_107 
2025-03-16 03:52:19.253973: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-16 03:52:19.295975: predicting hippocampus_108 
2025-03-16 03:52:19.304486: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-16 03:52:19.344485: predicting hippocampus_123 
2025-03-16 03:52:19.359487: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-16 03:52:19.397489: predicting hippocampus_125 
2025-03-16 03:52:19.411994: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-16 03:52:19.484997: predicting hippocampus_157 
2025-03-16 03:52:19.498502: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-16 03:52:19.531508: predicting hippocampus_164 
2025-03-16 03:52:19.546509: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-16 03:52:19.652018: predicting hippocampus_169 
2025-03-16 03:52:19.659019: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-16 03:52:19.688020: predicting hippocampus_175 
2025-03-16 03:52:19.694020: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-16 03:52:19.725526: predicting hippocampus_185 
2025-03-16 03:52:19.731527: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-16 03:52:19.760527: predicting hippocampus_190 
2025-03-16 03:52:19.765527: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-16 03:52:19.795530: predicting hippocampus_194 
2025-03-16 03:52:19.800037: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-16 03:52:19.832037: predicting hippocampus_204 
2025-03-16 03:52:19.839038: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-16 03:52:19.873037: predicting hippocampus_205 
2025-03-16 03:52:19.878038: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-16 03:52:19.911548: predicting hippocampus_210 
2025-03-16 03:52:19.917548: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-16 03:52:19.948548: predicting hippocampus_217 
2025-03-16 03:52:19.953548: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-16 03:52:19.982548: predicting hippocampus_219 
2025-03-16 03:52:19.987551: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-16 03:52:20.017058: predicting hippocampus_229 
2025-03-16 03:52:20.022058: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-16 03:52:20.052058: predicting hippocampus_244 
2025-03-16 03:52:20.057058: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-16 03:52:20.089060: predicting hippocampus_261 
2025-03-16 03:52:20.094060: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-16 03:52:20.143566: predicting hippocampus_264 
2025-03-16 03:52:20.149566: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-16 03:52:20.183566: predicting hippocampus_277 
2025-03-16 03:52:20.189569: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-16 03:52:20.242075: predicting hippocampus_280 
2025-03-16 03:52:20.250074: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-16 03:52:20.277074: predicting hippocampus_286 
2025-03-16 03:52:20.283074: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-16 03:52:20.328582: predicting hippocampus_288 
2025-03-16 03:52:20.333582: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-16 03:52:20.381583: predicting hippocampus_289 
2025-03-16 03:52:20.388585: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-16 03:52:20.417091: predicting hippocampus_296 
2025-03-16 03:52:20.422091: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-16 03:52:20.451091: predicting hippocampus_305 
2025-03-16 03:52:20.456091: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-16 03:52:20.496094: predicting hippocampus_308 
2025-03-16 03:52:20.501599: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-16 03:52:20.535599: predicting hippocampus_317 
2025-03-16 03:52:20.541599: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-16 03:52:20.577599: predicting hippocampus_327 
2025-03-16 03:52:20.584602: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-16 03:52:20.612108: predicting hippocampus_330 
2025-03-16 03:52:20.617108: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-16 03:52:20.649107: predicting hippocampus_332 
2025-03-16 03:52:20.655107: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-16 03:52:20.684110: predicting hippocampus_338 
2025-03-16 03:52:20.690110: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-16 03:52:20.738615: predicting hippocampus_349 
2025-03-16 03:52:20.744615: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-16 03:52:20.771615: predicting hippocampus_350 
2025-03-16 03:52:20.777615: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-16 03:52:20.805124: predicting hippocampus_356 
2025-03-16 03:52:20.812125: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-16 03:52:20.841124: predicting hippocampus_358 
2025-03-16 03:52:20.846126: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-16 03:52:20.876124: predicting hippocampus_374 
2025-03-16 03:52:20.881125: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-16 03:52:20.912632: predicting hippocampus_394 
2025-03-16 03:52:20.918633: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-16 03:52:24.461454: Validation complete 
2025-03-16 03:52:24.467454: Mean Validation Dice:  0.2601190545651646 
