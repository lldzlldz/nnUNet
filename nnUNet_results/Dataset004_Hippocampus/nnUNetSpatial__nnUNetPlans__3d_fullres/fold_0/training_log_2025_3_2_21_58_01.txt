
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-02 21:58:01.489484: do_dummy_2d_data_aug: False 
2025-03-02 21:58:01.491483: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-02 21:58:01.496485: The split file contains 5 splits. 
2025-03-02 21:58:01.499482: Desired fold for training: 0 
2025-03-02 21:58:01.502486: This split has 208 training and 52 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 9, 'patch_size': [40, 56, 40], 'median_image_size_in_voxels': [36.0, 50.0, 35.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 4, 'features_per_stage': [32, 64, 128, 256], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004_Hippocampus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [36, 50, 35], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 486420.21875, 'mean': 22360.326171875, 'median': 362.88250732421875, 'min': 0.0, 'percentile_00_5': 28.0, 'percentile_99_5': 277682.03125, 'std': 60656.1328125}}} 
 
2025-03-02 21:58:07.721857: unpacking dataset... 
2025-03-02 21:58:07.916034: unpacking done... 
2025-03-02 21:58:09.143018:  
2025-03-02 21:58:09.147914: Epoch 0 
2025-03-02 21:58:09.151926: Current learning rate: 0.01 
2025-03-02 21:58:16.704053: train_loss -0.6036 
2025-03-02 21:58:16.709639: val_loss -0.8114 
2025-03-02 21:58:16.713210: Pseudo dice [np.float32(0.8664), np.float32(0.8558)] 
2025-03-02 21:58:16.716835: Epoch time: 7.56 s 
2025-03-02 21:58:16.720361: Yayy! New best EMA pseudo Dice: 0.8611000180244446 
2025-03-02 21:58:17.250957:  
2025-03-02 21:58:17.256491: Epoch 1 
2025-03-02 21:58:17.260001: Current learning rate: 0.00991 
2025-03-02 21:58:23.894833: train_loss -0.827 
2025-03-02 21:58:23.899898: val_loss -0.8333 
2025-03-02 21:58:23.903248: Pseudo dice [np.float32(0.8829), np.float32(0.8697)] 
2025-03-02 21:58:23.906282: Epoch time: 6.64 s 
2025-03-02 21:58:23.909318: Yayy! New best EMA pseudo Dice: 0.8626000285148621 
2025-03-02 21:58:24.510591:  
2025-03-02 21:58:24.515672: Epoch 2 
2025-03-02 21:58:24.519185: Current learning rate: 0.00982 
2025-03-02 21:58:31.144696: train_loss -0.8459 
2025-03-02 21:58:31.151277: val_loss -0.8431 
2025-03-02 21:58:31.154315: Pseudo dice [np.float32(0.8907), np.float32(0.8775)] 
2025-03-02 21:58:31.157845: Epoch time: 6.63 s 
2025-03-02 21:58:31.160876: Yayy! New best EMA pseudo Dice: 0.864799976348877 
2025-03-02 21:58:31.799099:  
2025-03-02 21:58:31.804618: Epoch 3 
2025-03-02 21:58:31.808129: Current learning rate: 0.00973 
2025-03-02 21:58:38.423975: train_loss -0.8549 
2025-03-02 21:58:38.430462: val_loss -0.8451 
2025-03-02 21:58:38.434482: Pseudo dice [np.float32(0.8943), np.float32(0.8763)] 
2025-03-02 21:58:38.438395: Epoch time: 6.63 s 
2025-03-02 21:58:38.441520: Yayy! New best EMA pseudo Dice: 0.8668000102043152 
2025-03-02 21:58:39.052976:  
2025-03-02 21:58:39.058552: Epoch 4 
2025-03-02 21:58:39.062200: Current learning rate: 0.00964 
2025-03-02 21:58:45.688066: train_loss -0.8645 
2025-03-02 21:58:45.694398: val_loss -0.8491 
2025-03-02 21:58:45.697917: Pseudo dice [np.float32(0.8964), np.float32(0.8787)] 
2025-03-02 21:58:45.700698: Epoch time: 6.64 s 
2025-03-02 21:58:45.703733: Yayy! New best EMA pseudo Dice: 0.8689000010490417 
2025-03-02 21:58:46.464906:  
2025-03-02 21:58:46.470474: Epoch 5 
2025-03-02 21:58:46.474528: Current learning rate: 0.00955 
2025-03-02 21:58:53.130133: train_loss -0.8722 
2025-03-02 21:58:53.135779: val_loss -0.845 
2025-03-02 21:58:53.139817: Pseudo dice [np.float32(0.8922), np.float32(0.8782)] 
2025-03-02 21:58:53.142860: Epoch time: 6.67 s 
2025-03-02 21:58:53.145920: Yayy! New best EMA pseudo Dice: 0.8705000281333923 
2025-03-02 21:58:53.741842:  
2025-03-02 21:58:53.747475: Epoch 6 
2025-03-02 21:58:53.750025: Current learning rate: 0.00946 
2025-03-02 21:59:00.384263: train_loss -0.8754 
2025-03-02 21:59:00.390322: val_loss -0.849 
2025-03-02 21:59:00.394335: Pseudo dice [np.float32(0.8967), np.float32(0.8801)] 
2025-03-02 21:59:00.397951: Epoch time: 6.64 s 
2025-03-02 21:59:00.400459: Yayy! New best EMA pseudo Dice: 0.8723000288009644 
2025-03-02 21:59:01.001662:  
2025-03-02 21:59:01.007766: Epoch 7 
2025-03-02 21:59:01.011279: Current learning rate: 0.00937 
2025-03-02 21:59:07.658102: train_loss -0.8804 
2025-03-02 21:59:07.664223: val_loss -0.8476 
2025-03-02 21:59:07.667254: Pseudo dice [np.float32(0.8958), np.float32(0.8789)] 
2025-03-02 21:59:07.669781: Epoch time: 6.66 s 
2025-03-02 21:59:07.673301: Yayy! New best EMA pseudo Dice: 0.8737999796867371 
2025-03-02 21:59:08.298226:  
2025-03-02 21:59:08.303743: Epoch 8 
2025-03-02 21:59:08.307255: Current learning rate: 0.00928 
2025-03-02 21:59:14.952071: train_loss -0.8863 
2025-03-02 21:59:14.958112: val_loss -0.8434 
2025-03-02 21:59:14.961685: Pseudo dice [np.float32(0.8938), np.float32(0.876)] 
2025-03-02 21:59:14.965296: Epoch time: 6.65 s 
2025-03-02 21:59:14.968375: Yayy! New best EMA pseudo Dice: 0.8748999834060669 
2025-03-02 21:59:15.601402:  
2025-03-02 21:59:15.606951: Epoch 9 
2025-03-02 21:59:15.609997: Current learning rate: 0.00919 
2025-03-02 21:59:22.244647: train_loss -0.888 
2025-03-02 21:59:22.251697: val_loss -0.8493 
2025-03-02 21:59:22.255298: Pseudo dice [np.float32(0.8975), np.float32(0.8804)] 
2025-03-02 21:59:22.258347: Epoch time: 6.64 s 
2025-03-02 21:59:22.261396: Yayy! New best EMA pseudo Dice: 0.8762999773025513 
2025-03-02 21:59:22.862769:  
2025-03-02 21:59:22.868288: Epoch 10 
2025-03-02 21:59:22.871801: Current learning rate: 0.0091 
2025-03-02 21:59:29.471419: train_loss -0.8932 
2025-03-02 21:59:29.476963: val_loss -0.8435 
2025-03-02 21:59:29.479981: Pseudo dice [np.float32(0.8942), np.float32(0.876)] 
2025-03-02 21:59:29.484527: Epoch time: 6.61 s 
2025-03-02 21:59:29.487078: Yayy! New best EMA pseudo Dice: 0.8772000074386597 
2025-03-02 21:59:30.096449:  
2025-03-02 21:59:30.102554: Epoch 11 
2025-03-02 21:59:30.106612: Current learning rate: 0.009 
2025-03-02 21:59:36.727705: train_loss -0.8959 
2025-03-02 21:59:36.732755: val_loss -0.8495 
2025-03-02 21:59:36.736473: Pseudo dice [np.float32(0.8975), np.float32(0.8804)] 
2025-03-02 21:59:36.739989: Epoch time: 6.63 s 
2025-03-02 21:59:36.743520: Yayy! New best EMA pseudo Dice: 0.8784000277519226 
2025-03-02 21:59:37.357767:  
2025-03-02 21:59:37.366312: Epoch 12 
2025-03-02 21:59:37.371327: Current learning rate: 0.00891 
2025-03-02 21:59:43.991626: train_loss -0.8979 
2025-03-02 21:59:43.998248: val_loss -0.8477 
2025-03-02 21:59:44.001330: Pseudo dice [np.float32(0.8975), np.float32(0.8797)] 
2025-03-02 21:59:44.004894: Epoch time: 6.63 s 
2025-03-02 21:59:44.007402: Yayy! New best EMA pseudo Dice: 0.8794000148773193 
2025-03-02 21:59:44.788251:  
2025-03-02 21:59:44.793267: Epoch 13 
2025-03-02 21:59:44.796278: Current learning rate: 0.00882 
2025-03-02 21:59:51.441529: train_loss -0.8985 
2025-03-02 21:59:51.446840: val_loss -0.8453 
2025-03-02 21:59:51.451388: Pseudo dice [np.float32(0.8955), np.float32(0.8777)] 
2025-03-02 21:59:51.454440: Epoch time: 6.65 s 
2025-03-02 21:59:51.457488: Yayy! New best EMA pseudo Dice: 0.8801000118255615 
2025-03-02 21:59:52.067757:  
2025-03-02 21:59:52.073319: Epoch 14 
2025-03-02 21:59:52.077343: Current learning rate: 0.00873 
2025-03-02 21:59:58.724623: train_loss -0.9019 
2025-03-02 21:59:58.731180: val_loss -0.8382 
2025-03-02 21:59:58.734724: Pseudo dice [np.float32(0.892), np.float32(0.8711)] 
2025-03-02 21:59:58.737757: Epoch time: 6.66 s 
2025-03-02 21:59:58.741321: Yayy! New best EMA pseudo Dice: 0.880299985408783 
2025-03-02 21:59:59.361742:  
2025-03-02 21:59:59.367792: Epoch 15 
2025-03-02 21:59:59.370852: Current learning rate: 0.00864 
2025-03-02 22:00:06.198619: train_loss -0.9039 
2025-03-02 22:00:06.204751: val_loss -0.8492 
2025-03-02 22:00:06.207838: Pseudo dice [np.float32(0.8983), np.float32(0.8801)] 
2025-03-02 22:00:06.211437: Epoch time: 6.84 s 
2025-03-02 22:00:06.214529: Yayy! New best EMA pseudo Dice: 0.8812000155448914 
2025-03-02 22:00:06.838525:  
2025-03-02 22:00:06.844059: Epoch 16 
2025-03-02 22:00:06.847630: Current learning rate: 0.00855 
2025-03-02 22:00:13.629516: train_loss -0.905 
2025-03-02 22:00:13.635666: val_loss -0.8379 
2025-03-02 22:00:13.639203: Pseudo dice [np.float32(0.8927), np.float32(0.8738)] 
2025-03-02 22:00:13.642238: Epoch time: 6.79 s 
2025-03-02 22:00:13.645765: Yayy! New best EMA pseudo Dice: 0.8813999891281128 
2025-03-02 22:00:14.276993:  
2025-03-02 22:00:14.282568: Epoch 17 
2025-03-02 22:00:14.285149: Current learning rate: 0.00846 
2025-03-02 22:00:20.916738: train_loss -0.9066 
2025-03-02 22:00:20.923306: val_loss -0.8437 
2025-03-02 22:00:20.927123: Pseudo dice [np.float32(0.8952), np.float32(0.8755)] 
2025-03-02 22:00:20.930782: Epoch time: 6.64 s 
2025-03-02 22:00:20.934332: Yayy! New best EMA pseudo Dice: 0.8817999958992004 
2025-03-02 22:00:21.563791:  
2025-03-02 22:00:21.569808: Epoch 18 
2025-03-02 22:00:21.573818: Current learning rate: 0.00836 
2025-03-02 22:00:28.223324: train_loss -0.9105 
2025-03-02 22:00:28.229503: val_loss -0.8409 
2025-03-02 22:00:28.232558: Pseudo dice [np.float32(0.8938), np.float32(0.8764)] 
2025-03-02 22:00:28.236672: Epoch time: 6.66 s 
2025-03-02 22:00:28.240244: Yayy! New best EMA pseudo Dice: 0.882099986076355 
2025-03-02 22:00:28.870667:  
2025-03-02 22:00:28.875682: Epoch 19 
2025-03-02 22:00:28.879282: Current learning rate: 0.00827 
2025-03-02 22:00:35.532173: train_loss -0.9093 
2025-03-02 22:00:35.538244: val_loss -0.84 
2025-03-02 22:00:35.541773: Pseudo dice [np.float32(0.8929), np.float32(0.8749)] 
2025-03-02 22:00:35.544817: Epoch time: 6.66 s 
2025-03-02 22:00:35.548330: Yayy! New best EMA pseudo Dice: 0.8823000192642212 
2025-03-02 22:00:36.324234:  
2025-03-02 22:00:36.330306: Epoch 20 
2025-03-02 22:00:36.333400: Current learning rate: 0.00818 
2025-03-02 22:00:42.976642: train_loss -0.9121 
2025-03-02 22:00:42.983238: val_loss -0.843 
2025-03-02 22:00:42.986814: Pseudo dice [np.float32(0.8954), np.float32(0.8769)] 
2025-03-02 22:00:42.990363: Epoch time: 6.65 s 
2025-03-02 22:00:42.993176: Yayy! New best EMA pseudo Dice: 0.8827000260353088 
2025-03-02 22:00:43.627950:  
2025-03-02 22:00:43.634036: Epoch 21 
2025-03-02 22:00:43.637073: Current learning rate: 0.00809 
2025-03-02 22:00:50.275827: train_loss -0.9125 
2025-03-02 22:00:50.281886: val_loss -0.839 
2025-03-02 22:00:50.284920: Pseudo dice [np.float32(0.8945), np.float32(0.8748)] 
2025-03-02 22:00:50.288456: Epoch time: 6.65 s 
2025-03-02 22:00:50.292015: Yayy! New best EMA pseudo Dice: 0.8828999996185303 
2025-03-02 22:00:50.890749:  
2025-03-02 22:00:50.895761: Epoch 22 
2025-03-02 22:00:50.899275: Current learning rate: 0.008 
2025-03-02 22:00:57.598367: train_loss -0.9158 
2025-03-02 22:00:57.605029: val_loss -0.8402 
2025-03-02 22:00:57.609072: Pseudo dice [np.float32(0.8941), np.float32(0.877)] 
2025-03-02 22:00:57.612646: Epoch time: 6.71 s 
2025-03-02 22:00:57.615731: Yayy! New best EMA pseudo Dice: 0.8830999732017517 
2025-03-02 22:00:58.219893:  
2025-03-02 22:00:58.225436: Epoch 23 
2025-03-02 22:00:58.229005: Current learning rate: 0.0079 
2025-03-02 22:01:04.873910: train_loss -0.9154 
2025-03-02 22:01:04.879469: val_loss -0.8462 
2025-03-02 22:01:04.882995: Pseudo dice [np.float32(0.8987), np.float32(0.8795)] 
2025-03-02 22:01:04.886534: Epoch time: 6.66 s 
2025-03-02 22:01:04.889565: Yayy! New best EMA pseudo Dice: 0.8837000131607056 
2025-03-02 22:01:05.489392:  
2025-03-02 22:01:05.494954: Epoch 24 
2025-03-02 22:01:05.498556: Current learning rate: 0.00781 
2025-03-02 22:01:12.125161: train_loss -0.9158 
2025-03-02 22:01:12.130802: val_loss -0.8454 
2025-03-02 22:01:12.133880: Pseudo dice [np.float32(0.8968), np.float32(0.8788)] 
2025-03-02 22:01:12.136434: Epoch time: 6.64 s 
2025-03-02 22:01:12.141015: Yayy! New best EMA pseudo Dice: 0.8841000199317932 
2025-03-02 22:01:12.756345:  
2025-03-02 22:01:12.762381: Epoch 25 
2025-03-02 22:01:12.765416: Current learning rate: 0.00772 
2025-03-02 22:01:19.399059: train_loss -0.9186 
2025-03-02 22:01:19.404663: val_loss -0.8414 
2025-03-02 22:01:19.408277: Pseudo dice [np.float32(0.8963), np.float32(0.8777)] 
2025-03-02 22:01:19.410836: Epoch time: 6.64 s 
2025-03-02 22:01:19.414902: Yayy! New best EMA pseudo Dice: 0.8844000101089478 
2025-03-02 22:01:20.020694:  
2025-03-02 22:01:20.027284: Epoch 26 
2025-03-02 22:01:20.030807: Current learning rate: 0.00763 
2025-03-02 22:01:26.657171: train_loss -0.9188 
2025-03-02 22:01:26.663309: val_loss -0.8443 
2025-03-02 22:01:26.666368: Pseudo dice [np.float32(0.8984), np.float32(0.8799)] 
2025-03-02 22:01:26.669974: Epoch time: 6.64 s 
2025-03-02 22:01:26.673511: Yayy! New best EMA pseudo Dice: 0.8848999738693237 
2025-03-02 22:01:27.289726:  
2025-03-02 22:01:27.295286: Epoch 27 
2025-03-02 22:01:27.298867: Current learning rate: 0.00753 
2025-03-02 22:01:33.926041: train_loss -0.922 
2025-03-02 22:01:33.932600: val_loss -0.8372 
2025-03-02 22:01:33.936151: Pseudo dice [np.float32(0.8949), np.float32(0.8747)] 
2025-03-02 22:01:33.939690: Epoch time: 6.64 s 
2025-03-02 22:01:34.512152:  
2025-03-02 22:01:34.517667: Epoch 28 
2025-03-02 22:01:34.521179: Current learning rate: 0.00744 
2025-03-02 22:01:41.158107: train_loss -0.9222 
2025-03-02 22:01:41.164216: val_loss -0.8336 
2025-03-02 22:01:41.166728: Pseudo dice [np.float32(0.8917), np.float32(0.8732)] 
2025-03-02 22:01:41.171157: Epoch time: 6.65 s 
2025-03-02 22:01:41.911835:  
2025-03-02 22:01:41.917354: Epoch 29 
2025-03-02 22:01:41.920864: Current learning rate: 0.00735 
2025-03-02 22:01:48.570047: train_loss -0.9209 
2025-03-02 22:01:48.576606: val_loss -0.8415 
2025-03-02 22:01:48.580167: Pseudo dice [np.float32(0.8967), np.float32(0.8778)] 
2025-03-02 22:01:48.584495: Epoch time: 6.66 s 
2025-03-02 22:01:48.588026: Yayy! New best EMA pseudo Dice: 0.8848999738693237 
2025-03-02 22:01:49.384215:  
2025-03-02 22:01:49.390233: Epoch 30 
2025-03-02 22:01:49.393744: Current learning rate: 0.00725 
2025-03-02 22:01:56.076251: train_loss -0.9237 
2025-03-02 22:01:56.082916: val_loss -0.8355 
2025-03-02 22:01:56.085981: Pseudo dice [np.float32(0.893), np.float32(0.8761)] 
2025-03-02 22:01:56.089626: Epoch time: 6.69 s 
2025-03-02 22:01:56.655840:  
2025-03-02 22:01:56.660879: Epoch 31 
2025-03-02 22:01:56.664445: Current learning rate: 0.00716 
2025-03-02 22:02:03.373237: train_loss -0.924 
2025-03-02 22:02:03.379882: val_loss -0.8426 
2025-03-02 22:02:03.382957: Pseudo dice [np.float32(0.8973), np.float32(0.8802)] 
2025-03-02 22:02:03.386513: Epoch time: 6.72 s 
2025-03-02 22:02:03.389578: Yayy! New best EMA pseudo Dice: 0.8852999806404114 
2025-03-02 22:02:04.001981:  
2025-03-02 22:02:04.007001: Epoch 32 
2025-03-02 22:02:04.010511: Current learning rate: 0.00707 
2025-03-02 22:02:10.693512: train_loss -0.9229 
2025-03-02 22:02:10.700605: val_loss -0.8353 
2025-03-02 22:02:10.704118: Pseudo dice [np.float32(0.8941), np.float32(0.8738)] 
2025-03-02 22:02:10.706626: Epoch time: 6.69 s 
2025-03-02 22:02:11.289119:  
2025-03-02 22:02:11.294636: Epoch 33 
2025-03-02 22:02:11.298147: Current learning rate: 0.00697 
2025-03-02 22:02:17.990695: train_loss -0.9248 
2025-03-02 22:02:17.996844: val_loss -0.8352 
2025-03-02 22:02:17.999867: Pseudo dice [np.float32(0.8934), np.float32(0.8741)] 
2025-03-02 22:02:18.003892: Epoch time: 6.7 s 
2025-03-02 22:02:18.587700:  
2025-03-02 22:02:18.593785: Epoch 34 
2025-03-02 22:02:18.596861: Current learning rate: 0.00688 
2025-03-02 22:02:25.294451: train_loss -0.9258 
2025-03-02 22:02:25.300083: val_loss -0.8317 
2025-03-02 22:02:25.303150: Pseudo dice [np.float32(0.8918), np.float32(0.8737)] 
2025-03-02 22:02:25.307183: Epoch time: 6.71 s 
2025-03-02 22:02:25.886031:  
2025-03-02 22:02:25.891548: Epoch 35 
2025-03-02 22:02:25.895061: Current learning rate: 0.00679 
2025-03-02 22:02:32.583929: train_loss -0.9263 
2025-03-02 22:02:32.590619: val_loss -0.8401 
2025-03-02 22:02:32.594152: Pseudo dice [np.float32(0.897), np.float32(0.879)] 
2025-03-02 22:02:32.597678: Epoch time: 6.7 s 
2025-03-02 22:02:33.330124:  
2025-03-02 22:02:33.336672: Epoch 36 
2025-03-02 22:02:33.339708: Current learning rate: 0.00669 
2025-03-02 22:02:40.018672: train_loss -0.926 
2025-03-02 22:02:40.026294: val_loss -0.838 
2025-03-02 22:02:40.030335: Pseudo dice [np.float32(0.895), np.float32(0.8768)] 
2025-03-02 22:02:40.033359: Epoch time: 6.69 s 
2025-03-02 22:02:40.649920:  
2025-03-02 22:02:40.656528: Epoch 37 
2025-03-02 22:02:40.659098: Current learning rate: 0.0066 
2025-03-02 22:02:47.347265: train_loss -0.9289 
2025-03-02 22:02:47.353336: val_loss -0.8371 
2025-03-02 22:02:47.357371: Pseudo dice [np.float32(0.8951), np.float32(0.8772)] 
2025-03-02 22:02:47.360414: Epoch time: 6.7 s 
2025-03-02 22:02:47.363435: Yayy! New best EMA pseudo Dice: 0.8852999806404114 
2025-03-02 22:02:47.987065:  
2025-03-02 22:02:47.992078: Epoch 38 
2025-03-02 22:02:47.995590: Current learning rate: 0.0065 
2025-03-02 22:02:54.674129: train_loss -0.9279 
2025-03-02 22:02:54.680561: val_loss -0.8312 
2025-03-02 22:02:54.684076: Pseudo dice [np.float32(0.8922), np.float32(0.8746)] 
2025-03-02 22:02:54.687603: Epoch time: 6.69 s 
2025-03-02 22:02:55.273619:  
2025-03-02 22:02:55.279136: Epoch 39 
2025-03-02 22:02:55.282645: Current learning rate: 0.00641 
2025-03-02 22:03:01.959661: train_loss -0.9295 
2025-03-02 22:03:01.965272: val_loss -0.8385 
2025-03-02 22:03:01.969341: Pseudo dice [np.float32(0.8969), np.float32(0.8781)] 
2025-03-02 22:03:01.971947: Epoch time: 6.69 s 
2025-03-02 22:03:01.975498: Yayy! New best EMA pseudo Dice: 0.8852999806404114 
2025-03-02 22:03:02.608436:  
2025-03-02 22:03:02.613957: Epoch 40 
2025-03-02 22:03:02.616966: Current learning rate: 0.00631 
2025-03-02 22:03:09.664719: train_loss -0.9286 
2025-03-02 22:03:09.671825: val_loss -0.8319 
2025-03-02 22:03:09.674887: Pseudo dice [np.float32(0.8934), np.float32(0.8752)] 
2025-03-02 22:03:09.677497: Epoch time: 7.06 s 
2025-03-02 22:03:10.285092:  
2025-03-02 22:03:10.290158: Epoch 41 
2025-03-02 22:03:10.293764: Current learning rate: 0.00622 
2025-03-02 22:03:16.992608: train_loss -0.9297 
2025-03-02 22:03:16.998175: val_loss -0.8387 
2025-03-02 22:03:17.001736: Pseudo dice [np.float32(0.8975), np.float32(0.8788)] 
2025-03-02 22:03:17.005273: Epoch time: 6.71 s 
2025-03-02 22:03:17.008316: Yayy! New best EMA pseudo Dice: 0.8855000138282776 
2025-03-02 22:03:17.601537:  
2025-03-02 22:03:17.606551: Epoch 42 
2025-03-02 22:03:17.610569: Current learning rate: 0.00612 
2025-03-02 22:03:24.298106: train_loss -0.9306 
2025-03-02 22:03:24.304624: val_loss -0.8369 
2025-03-02 22:03:24.308136: Pseudo dice [np.float32(0.896), np.float32(0.8769)] 
2025-03-02 22:03:24.311645: Epoch time: 6.7 s 
2025-03-02 22:03:24.314656: Yayy! New best EMA pseudo Dice: 0.8855999708175659 
2025-03-02 22:03:24.921770:  
2025-03-02 22:03:24.927311: Epoch 43 
2025-03-02 22:03:24.930436: Current learning rate: 0.00603 
2025-03-02 22:03:31.587355: train_loss -0.9312 
2025-03-02 22:03:31.594010: val_loss -0.8356 
2025-03-02 22:03:31.597576: Pseudo dice [np.float32(0.8952), np.float32(0.8774)] 
2025-03-02 22:03:31.600607: Epoch time: 6.67 s 
2025-03-02 22:03:31.603153: Yayy! New best EMA pseudo Dice: 0.885699987411499 
2025-03-02 22:03:32.351010:  
2025-03-02 22:03:32.355024: Epoch 44 
2025-03-02 22:03:32.358529: Current learning rate: 0.00593 
2025-03-02 22:03:39.035598: train_loss -0.9309 
2025-03-02 22:03:39.042637: val_loss -0.8337 
2025-03-02 22:03:39.045746: Pseudo dice [np.float32(0.8957), np.float32(0.8754)] 
2025-03-02 22:03:39.049781: Epoch time: 6.69 s 
2025-03-02 22:03:39.681081:  
2025-03-02 22:03:39.686603: Epoch 45 
2025-03-02 22:03:39.690115: Current learning rate: 0.00584 
2025-03-02 22:03:46.385581: train_loss -0.9327 
2025-03-02 22:03:46.390376: val_loss -0.8337 
2025-03-02 22:03:46.394914: Pseudo dice [np.float32(0.8951), np.float32(0.8771)] 
2025-03-02 22:03:46.398425: Epoch time: 6.71 s 
2025-03-02 22:03:46.400932: Yayy! New best EMA pseudo Dice: 0.885699987411499 
2025-03-02 22:03:46.996431:  
2025-03-02 22:03:47.002011: Epoch 46 
2025-03-02 22:03:47.005047: Current learning rate: 0.00574 
2025-03-02 22:03:53.671662: train_loss -0.932 
2025-03-02 22:03:53.677225: val_loss -0.8365 
2025-03-02 22:03:53.680761: Pseudo dice [np.float32(0.8961), np.float32(0.8789)] 
2025-03-02 22:03:53.684369: Epoch time: 6.68 s 
2025-03-02 22:03:53.687914: Yayy! New best EMA pseudo Dice: 0.8859000205993652 
2025-03-02 22:03:54.278476:  
2025-03-02 22:03:54.284050: Epoch 47 
2025-03-02 22:03:54.287563: Current learning rate: 0.00565 
2025-03-02 22:04:00.959012: train_loss -0.9347 
2025-03-02 22:04:00.965644: val_loss -0.837 
2025-03-02 22:04:00.969279: Pseudo dice [np.float32(0.8963), np.float32(0.8801)] 
2025-03-02 22:04:00.972360: Epoch time: 6.68 s 
2025-03-02 22:04:00.974930: Yayy! New best EMA pseudo Dice: 0.8860999941825867 
2025-03-02 22:04:01.572032:  
2025-03-02 22:04:01.578047: Epoch 48 
2025-03-02 22:04:01.581057: Current learning rate: 0.00555 
2025-03-02 22:04:08.266830: train_loss -0.9329 
2025-03-02 22:04:08.272350: val_loss -0.8365 
2025-03-02 22:04:08.275860: Pseudo dice [np.float32(0.8961), np.float32(0.879)] 
2025-03-02 22:04:08.279367: Epoch time: 6.69 s 
2025-03-02 22:04:08.282378: Yayy! New best EMA pseudo Dice: 0.8863000273704529 
2025-03-02 22:04:08.885829:  
2025-03-02 22:04:08.890874: Epoch 49 
2025-03-02 22:04:08.894012: Current learning rate: 0.00546 
2025-03-02 22:04:16.099830: train_loss -0.9341 
2025-03-02 22:04:16.105929: val_loss -0.8326 
2025-03-02 22:04:16.109991: Pseudo dice [np.float32(0.8952), np.float32(0.8781)] 
2025-03-02 22:04:16.112548: Epoch time: 7.21 s 
2025-03-02 22:04:16.152466: Yayy! New best EMA pseudo Dice: 0.8863000273704529 
2025-03-02 22:04:16.779025:  
2025-03-02 22:04:16.785638: Epoch 50 
2025-03-02 22:04:16.788696: Current learning rate: 0.00536 
2025-03-02 22:04:23.992521: train_loss -0.9336 
2025-03-02 22:04:23.998686: val_loss -0.8345 
2025-03-02 22:04:24.002218: Pseudo dice [np.float32(0.8952), np.float32(0.8775)] 
2025-03-02 22:04:24.005733: Epoch time: 7.21 s 
2025-03-02 22:04:24.008666: Yayy! New best EMA pseudo Dice: 0.8863000273704529 
2025-03-02 22:04:24.611782:  
2025-03-02 22:04:24.617425: Epoch 51 
2025-03-02 22:04:24.619966: Current learning rate: 0.00526 
2025-03-02 22:04:31.738423: train_loss -0.9341 
2025-03-02 22:04:31.744444: val_loss -0.8332 
2025-03-02 22:04:31.748456: Pseudo dice [np.float32(0.8943), np.float32(0.8756)] 
2025-03-02 22:04:31.751968: Epoch time: 7.13 s 
2025-03-02 22:04:32.325644:  
2025-03-02 22:04:32.331675: Epoch 52 
2025-03-02 22:04:32.335187: Current learning rate: 0.00517 
2025-03-02 22:04:39.361639: train_loss -0.9342 
2025-03-02 22:04:39.368161: val_loss -0.8307 
2025-03-02 22:04:39.372173: Pseudo dice [np.float32(0.8935), np.float32(0.8772)] 
2025-03-02 22:04:39.374681: Epoch time: 7.03 s 
2025-03-02 22:04:40.145964:  
2025-03-02 22:04:40.151485: Epoch 53 
2025-03-02 22:04:40.153991: Current learning rate: 0.00507 
2025-03-02 22:04:47.124728: train_loss -0.9358 
2025-03-02 22:04:47.130386: val_loss -0.8324 
2025-03-02 22:04:47.134438: Pseudo dice [np.float32(0.8954), np.float32(0.8756)] 
2025-03-02 22:04:47.137483: Epoch time: 6.98 s 
2025-03-02 22:04:47.711973:  
2025-03-02 22:04:47.717504: Epoch 54 
2025-03-02 22:04:47.721046: Current learning rate: 0.00497 
2025-03-02 22:04:54.875630: train_loss -0.9365 
2025-03-02 22:04:54.881861: val_loss -0.8322 
2025-03-02 22:04:54.885918: Pseudo dice [np.float32(0.8946), np.float32(0.8766)] 
2025-03-02 22:04:54.889965: Epoch time: 7.16 s 
2025-03-02 22:04:55.480807:  
2025-03-02 22:04:55.486345: Epoch 55 
2025-03-02 22:04:55.490508: Current learning rate: 0.00487 
2025-03-02 22:05:02.492281: train_loss -0.9376 
2025-03-02 22:05:02.497900: val_loss -0.8397 
2025-03-02 22:05:02.503993: Pseudo dice [np.float32(0.8981), np.float32(0.8817)] 
2025-03-02 22:05:02.506956: Epoch time: 7.01 s 
2025-03-02 22:05:02.510480: Yayy! New best EMA pseudo Dice: 0.8863999843597412 
2025-03-02 22:05:03.180487:  
2025-03-02 22:05:03.184550: Epoch 56 
2025-03-02 22:05:03.190098: Current learning rate: 0.00478 
2025-03-02 22:05:10.303344: train_loss -0.9391 
2025-03-02 22:05:10.309994: val_loss -0.8295 
2025-03-02 22:05:10.314222: Pseudo dice [np.float32(0.8936), np.float32(0.8764)] 
2025-03-02 22:05:10.318292: Epoch time: 7.12 s 
2025-03-02 22:05:10.979290:  
2025-03-02 22:05:10.985841: Epoch 57 
2025-03-02 22:05:10.993961: Current learning rate: 0.00468 
2025-03-02 22:05:19.300306: train_loss -0.937 
2025-03-02 22:05:19.305471: val_loss -0.8347 
2025-03-02 22:05:19.311004: Pseudo dice [np.float32(0.8962), np.float32(0.8778)] 
2025-03-02 22:05:19.316252: Epoch time: 8.32 s 
2025-03-02 22:05:19.929886:  
2025-03-02 22:05:19.935484: Epoch 58 
2025-03-02 22:05:19.938995: Current learning rate: 0.00458 
2025-03-02 22:05:28.110993: train_loss -0.9372 
2025-03-02 22:05:28.116634: val_loss -0.8337 
2025-03-02 22:05:28.121338: Pseudo dice [np.float32(0.8955), np.float32(0.8768)] 
2025-03-02 22:05:28.124400: Epoch time: 8.18 s 
2025-03-02 22:05:28.770455:  
2025-03-02 22:05:28.776535: Epoch 59 
2025-03-02 22:05:28.779608: Current learning rate: 0.00448 
2025-03-02 22:05:36.936381: train_loss -0.9383 
2025-03-02 22:05:36.942521: val_loss -0.832 
2025-03-02 22:05:36.944546: Pseudo dice [np.float32(0.8958), np.float32(0.8767)] 
2025-03-02 22:05:36.950081: Epoch time: 8.17 s 
2025-03-02 22:05:37.702618:  
2025-03-02 22:05:37.708686: Epoch 60 
2025-03-02 22:05:37.712196: Current learning rate: 0.00438 
2025-03-02 22:05:45.776147: train_loss -0.9389 
2025-03-02 22:05:45.782352: val_loss -0.8368 
2025-03-02 22:05:45.786514: Pseudo dice [np.float32(0.8989), np.float32(0.8798)] 
2025-03-02 22:05:45.790137: Epoch time: 8.07 s 
2025-03-02 22:05:45.793768: Yayy! New best EMA pseudo Dice: 0.8866000175476074 
2025-03-02 22:05:46.425697:  
2025-03-02 22:05:46.431743: Epoch 61 
2025-03-02 22:05:46.435770: Current learning rate: 0.00429 
2025-03-02 22:05:54.597172: train_loss -0.9393 
2025-03-02 22:05:54.603763: val_loss -0.8266 
2025-03-02 22:05:54.608340: Pseudo dice [np.float32(0.8927), np.float32(0.8745)] 
2025-03-02 22:05:54.611351: Epoch time: 8.17 s 
2025-03-02 22:05:55.198502:  
2025-03-02 22:05:55.204026: Epoch 62 
2025-03-02 22:05:55.208047: Current learning rate: 0.00419 
2025-03-02 22:06:03.382264: train_loss -0.9379 
2025-03-02 22:06:03.386834: val_loss -0.8315 
2025-03-02 22:06:03.391513: Pseudo dice [np.float32(0.8945), np.float32(0.8775)] 
2025-03-02 22:06:03.394622: Epoch time: 8.18 s 
2025-03-02 22:06:04.008707:  
2025-03-02 22:06:04.014806: Epoch 63 
2025-03-02 22:06:04.017884: Current learning rate: 0.00409 
2025-03-02 22:06:12.228992: train_loss -0.94 
2025-03-02 22:06:12.235085: val_loss -0.8331 
2025-03-02 22:06:12.238659: Pseudo dice [np.float32(0.8949), np.float32(0.8789)] 
2025-03-02 22:06:12.242190: Epoch time: 8.22 s 
2025-03-02 22:06:12.835153:  
2025-03-02 22:06:12.840676: Epoch 64 
2025-03-02 22:06:12.844188: Current learning rate: 0.00399 
2025-03-02 22:06:21.031847: train_loss -0.9408 
2025-03-02 22:06:21.037972: val_loss -0.8282 
2025-03-02 22:06:21.040712: Pseudo dice [np.float32(0.8946), np.float32(0.8751)] 
2025-03-02 22:06:21.045251: Epoch time: 8.2 s 
2025-03-02 22:06:21.638728:  
2025-03-02 22:06:21.644741: Epoch 65 
2025-03-02 22:06:21.647755: Current learning rate: 0.00389 
2025-03-02 22:06:29.787586: train_loss -0.9405 
2025-03-02 22:06:29.793105: val_loss -0.8301 
2025-03-02 22:06:29.798119: Pseudo dice [np.float32(0.8938), np.float32(0.8758)] 
2025-03-02 22:06:29.801630: Epoch time: 8.15 s 
2025-03-02 22:06:30.405392:  
2025-03-02 22:06:30.410983: Epoch 66 
2025-03-02 22:06:30.415505: Current learning rate: 0.00379 
2025-03-02 22:06:38.634105: train_loss -0.9401 
2025-03-02 22:06:38.642771: val_loss -0.832 
2025-03-02 22:06:38.648845: Pseudo dice [np.float32(0.8945), np.float32(0.878)] 
2025-03-02 22:06:38.654366: Epoch time: 8.23 s 
2025-03-02 22:06:39.413574:  
2025-03-02 22:06:39.419182: Epoch 67 
2025-03-02 22:06:39.424863: Current learning rate: 0.00369 
2025-03-02 22:06:53.724577: train_loss -0.9401 
2025-03-02 22:06:53.731625: val_loss -0.8275 
2025-03-02 22:06:53.736135: Pseudo dice [np.float32(0.8941), np.float32(0.8748)] 
2025-03-02 22:06:53.741672: Epoch time: 14.31 s 
2025-03-02 22:06:54.403663:  
2025-03-02 22:06:54.409244: Epoch 68 
2025-03-02 22:06:54.414335: Current learning rate: 0.00359 
2025-03-02 22:07:08.921985: train_loss -0.9406 
2025-03-02 22:07:08.926500: val_loss -0.8328 
2025-03-02 22:07:08.931760: Pseudo dice [np.float32(0.8955), np.float32(0.8783)] 
2025-03-02 22:07:08.937857: Epoch time: 14.52 s 
2025-03-02 22:07:09.788024:  
2025-03-02 22:07:09.794177: Epoch 69 
2025-03-02 22:07:09.798731: Current learning rate: 0.00349 
2025-03-02 22:07:24.007344: train_loss -0.9427 
2025-03-02 22:07:24.015403: val_loss -0.8304 
2025-03-02 22:07:24.020451: Pseudo dice [np.float32(0.8938), np.float32(0.8766)] 
2025-03-02 22:07:24.025466: Epoch time: 14.22 s 
2025-03-02 22:07:24.678703:  
2025-03-02 22:07:24.684903: Epoch 70 
2025-03-02 22:07:24.689484: Current learning rate: 0.00338 
2025-03-02 22:07:39.168857: train_loss -0.9426 
2025-03-02 22:07:39.176985: val_loss -0.8322 
2025-03-02 22:07:39.181560: Pseudo dice [np.float32(0.8962), np.float32(0.878)] 
2025-03-02 22:07:39.186101: Epoch time: 14.49 s 
2025-03-02 22:07:39.840910:  
2025-03-02 22:07:39.846501: Epoch 71 
2025-03-02 22:07:39.852087: Current learning rate: 0.00328 
2025-03-02 22:07:54.200534: train_loss -0.9412 
2025-03-02 22:07:54.206554: val_loss -0.8251 
2025-03-02 22:07:54.211571: Pseudo dice [np.float32(0.8925), np.float32(0.8751)] 
2025-03-02 22:07:54.216585: Epoch time: 14.36 s 
2025-03-02 22:07:54.865252:  
2025-03-02 22:07:54.872876: Epoch 72 
2025-03-02 22:07:54.877890: Current learning rate: 0.00318 
2025-03-02 22:08:09.188688: train_loss -0.943 
2025-03-02 22:08:09.194875: val_loss -0.8257 
2025-03-02 22:08:09.199911: Pseudo dice [np.float32(0.8925), np.float32(0.8742)] 
2025-03-02 22:08:09.204935: Epoch time: 14.32 s 
2025-03-02 22:08:09.847626:  
2025-03-02 22:08:09.854239: Epoch 73 
2025-03-02 22:08:09.859317: Current learning rate: 0.00308 
2025-03-02 22:08:23.920193: train_loss -0.942 
2025-03-02 22:08:23.925877: val_loss -0.835 
2025-03-02 22:08:23.931055: Pseudo dice [np.float32(0.8974), np.float32(0.8791)] 
2025-03-02 22:08:23.935084: Epoch time: 14.07 s 
2025-03-02 22:08:24.571290:  
2025-03-02 22:08:24.576940: Epoch 74 
2025-03-02 22:08:24.581573: Current learning rate: 0.00297 
2025-03-02 22:08:39.219428: train_loss -0.9435 
2025-03-02 22:08:39.227551: val_loss -0.8299 
2025-03-02 22:08:39.232111: Pseudo dice [np.float32(0.8953), np.float32(0.8769)] 
2025-03-02 22:08:39.238168: Epoch time: 14.65 s 
2025-03-02 22:08:39.893368:  
2025-03-02 22:08:39.900411: Epoch 75 
2025-03-02 22:08:39.904420: Current learning rate: 0.00287 
2025-03-02 22:08:54.530682: train_loss -0.9431 
2025-03-02 22:08:54.537257: val_loss -0.8327 
2025-03-02 22:08:54.541347: Pseudo dice [np.float32(0.8956), np.float32(0.8792)] 
2025-03-02 22:08:54.545968: Epoch time: 14.64 s 
2025-03-02 22:08:55.370394:  
2025-03-02 22:08:55.375949: Epoch 76 
2025-03-02 22:08:55.381034: Current learning rate: 0.00277 
2025-03-02 22:09:09.879121: train_loss -0.9428 
2025-03-02 22:09:09.885733: val_loss -0.8315 
2025-03-02 22:09:09.890859: Pseudo dice [np.float32(0.8964), np.float32(0.8789)] 
2025-03-02 22:09:09.894884: Epoch time: 14.51 s 
2025-03-02 22:09:10.518833:  
2025-03-02 22:09:10.524887: Epoch 77 
2025-03-02 22:09:10.528902: Current learning rate: 0.00266 
2025-03-02 22:09:24.753768: train_loss -0.943 
2025-03-02 22:09:24.760784: val_loss -0.8273 
2025-03-02 22:09:24.765799: Pseudo dice [np.float32(0.8936), np.float32(0.8742)] 
2025-03-02 22:09:24.770819: Epoch time: 14.24 s 
2025-03-02 22:09:25.374478:  
2025-03-02 22:09:25.381063: Epoch 78 
2025-03-02 22:09:25.386159: Current learning rate: 0.00256 
2025-03-02 22:09:39.960325: train_loss -0.9437 
2025-03-02 22:09:39.968475: val_loss -0.8294 
2025-03-02 22:09:39.973692: Pseudo dice [np.float32(0.8958), np.float32(0.8776)] 
2025-03-02 22:09:39.978804: Epoch time: 14.59 s 
2025-03-02 22:09:40.613661:  
2025-03-02 22:09:40.620714: Epoch 79 
2025-03-02 22:09:40.625737: Current learning rate: 0.00245 
2025-03-02 22:09:55.146465: train_loss -0.9451 
2025-03-02 22:09:55.153008: val_loss -0.8302 
2025-03-02 22:09:55.158019: Pseudo dice [np.float32(0.8961), np.float32(0.8771)] 
2025-03-02 22:09:55.163056: Epoch time: 14.53 s 
2025-03-02 22:09:55.761300:  
2025-03-02 22:09:55.766857: Epoch 80 
2025-03-02 22:09:55.772003: Current learning rate: 0.00235 
2025-03-02 22:10:10.222132: train_loss -0.9442 
2025-03-02 22:10:10.228716: val_loss -0.8295 
2025-03-02 22:10:10.234324: Pseudo dice [np.float32(0.895), np.float32(0.8777)] 
2025-03-02 22:10:10.240344: Epoch time: 14.46 s 
2025-03-02 22:10:10.892212:  
2025-03-02 22:10:10.898771: Epoch 81 
2025-03-02 22:10:10.903328: Current learning rate: 0.00224 
2025-03-02 22:10:25.519583: train_loss -0.9443 
2025-03-02 22:10:25.526178: val_loss -0.8304 
2025-03-02 22:10:25.531353: Pseudo dice [np.float32(0.8956), np.float32(0.8786)] 
2025-03-02 22:10:25.536937: Epoch time: 14.63 s 
2025-03-02 22:10:26.135412:  
2025-03-02 22:10:26.142444: Epoch 82 
2025-03-02 22:10:26.146471: Current learning rate: 0.00214 
2025-03-02 22:10:40.760367: train_loss -0.9439 
2025-03-02 22:10:40.768901: val_loss -0.8281 
2025-03-02 22:10:40.773918: Pseudo dice [np.float32(0.8949), np.float32(0.8759)] 
2025-03-02 22:10:40.778939: Epoch time: 14.62 s 
2025-03-02 22:10:41.393130:  
2025-03-02 22:10:41.399702: Epoch 83 
2025-03-02 22:10:41.403764: Current learning rate: 0.00203 
2025-03-02 22:10:55.269816: train_loss -0.9456 
2025-03-02 22:10:55.276024: val_loss -0.8327 
2025-03-02 22:10:55.279601: Pseudo dice [np.float32(0.8973), np.float32(0.879)] 
2025-03-02 22:10:55.283654: Epoch time: 13.88 s 
2025-03-02 22:10:56.014443:  
2025-03-02 22:10:56.021033: Epoch 84 
2025-03-02 22:10:56.025090: Current learning rate: 0.00192 
2025-03-02 22:11:07.642132: train_loss -0.9476 
2025-03-02 22:11:07.648733: val_loss -0.8249 
2025-03-02 22:11:07.654296: Pseudo dice [np.float32(0.892), np.float32(0.8746)] 
2025-03-02 22:11:07.659349: Epoch time: 11.63 s 
2025-03-02 22:11:08.258704:  
2025-03-02 22:11:08.265795: Epoch 85 
2025-03-02 22:11:08.270379: Current learning rate: 0.00181 
2025-03-02 22:11:19.868361: train_loss -0.9477 
2025-03-02 22:11:19.874913: val_loss -0.8277 
2025-03-02 22:11:19.879476: Pseudo dice [np.float32(0.8944), np.float32(0.8767)] 
2025-03-02 22:11:19.882489: Epoch time: 11.61 s 
2025-03-02 22:11:20.505250:  
2025-03-02 22:11:20.511268: Epoch 86 
2025-03-02 22:11:20.515326: Current learning rate: 0.0017 
2025-03-02 22:11:28.381577: train_loss -0.9454 
2025-03-02 22:11:28.388191: val_loss -0.8292 
2025-03-02 22:11:28.392782: Pseudo dice [np.float32(0.8955), np.float32(0.8777)] 
2025-03-02 22:11:28.397862: Epoch time: 7.88 s 
2025-03-02 22:11:28.948785:  
2025-03-02 22:11:28.953793: Epoch 87 
2025-03-02 22:11:28.958301: Current learning rate: 0.00159 
2025-03-02 22:11:38.699884: train_loss -0.9463 
2025-03-02 22:11:38.706008: val_loss -0.831 
2025-03-02 22:11:38.710083: Pseudo dice [np.float32(0.8958), np.float32(0.8787)] 
2025-03-02 22:11:38.714732: Epoch time: 9.75 s 
2025-03-02 22:11:39.296630:  
2025-03-02 22:11:39.301251: Epoch 88 
2025-03-02 22:11:39.305823: Current learning rate: 0.00148 
2025-03-02 22:11:46.228805: train_loss -0.9472 
2025-03-02 22:11:46.234934: val_loss -0.8288 
2025-03-02 22:11:46.239015: Pseudo dice [np.float32(0.8948), np.float32(0.8777)] 
2025-03-02 22:11:46.243615: Epoch time: 6.93 s 
2025-03-02 22:11:46.800754:  
2025-03-02 22:11:46.806410: Epoch 89 
2025-03-02 22:11:46.809991: Current learning rate: 0.00137 
2025-03-02 22:11:57.399370: train_loss -0.9466 
2025-03-02 22:11:57.407064: val_loss -0.8241 
2025-03-02 22:11:57.412185: Pseudo dice [np.float32(0.8918), np.float32(0.8745)] 
2025-03-02 22:11:57.418209: Epoch time: 10.6 s 
2025-03-02 22:11:58.037085:  
2025-03-02 22:11:58.043671: Epoch 90 
2025-03-02 22:11:58.049281: Current learning rate: 0.00126 
2025-03-02 22:12:11.457316: train_loss -0.9469 
2025-03-02 22:12:11.463913: val_loss -0.8292 
2025-03-02 22:12:11.469308: Pseudo dice [np.float32(0.8951), np.float32(0.8777)] 
2025-03-02 22:12:11.475892: Epoch time: 13.42 s 
2025-03-02 22:12:12.087585:  
2025-03-02 22:12:12.093265: Epoch 91 
2025-03-02 22:12:12.098282: Current learning rate: 0.00115 
2025-03-02 22:12:25.580053: train_loss -0.9469 
2025-03-02 22:12:25.586613: val_loss -0.8309 
2025-03-02 22:12:25.591627: Pseudo dice [np.float32(0.8956), np.float32(0.8785)] 
2025-03-02 22:12:25.596661: Epoch time: 13.49 s 
2025-03-02 22:12:26.393103:  
2025-03-02 22:12:26.398720: Epoch 92 
2025-03-02 22:12:26.403275: Current learning rate: 0.00103 
2025-03-02 22:12:39.482834: train_loss -0.9473 
2025-03-02 22:12:39.488871: val_loss -0.8311 
2025-03-02 22:12:39.493921: Pseudo dice [np.float32(0.8962), np.float32(0.8786)] 
2025-03-02 22:12:39.501449: Epoch time: 13.09 s 
2025-03-02 22:12:40.098134:  
2025-03-02 22:12:40.105657: Epoch 93 
2025-03-02 22:12:40.110677: Current learning rate: 0.00091 
2025-03-02 22:12:53.316064: train_loss -0.9458 
2025-03-02 22:12:53.322677: val_loss -0.8182 
2025-03-02 22:12:53.327227: Pseudo dice [np.float32(0.8906), np.float32(0.8723)] 
2025-03-02 22:12:53.332342: Epoch time: 13.22 s 
2025-03-02 22:12:53.913026:  
2025-03-02 22:12:53.919571: Epoch 94 
2025-03-02 22:12:53.924590: Current learning rate: 0.00079 
2025-03-02 22:13:06.783971: train_loss -0.9482 
2025-03-02 22:13:06.791521: val_loss -0.824 
2025-03-02 22:13:06.797167: Pseudo dice [np.float32(0.8917), np.float32(0.8745)] 
2025-03-02 22:13:06.802271: Epoch time: 12.87 s 
2025-03-02 22:13:07.390171:  
2025-03-02 22:13:07.396864: Epoch 95 
2025-03-02 22:13:07.401958: Current learning rate: 0.00067 
2025-03-02 22:13:20.885035: train_loss -0.9493 
2025-03-02 22:13:20.891160: val_loss -0.8251 
2025-03-02 22:13:20.897419: Pseudo dice [np.float32(0.8923), np.float32(0.8756)] 
2025-03-02 22:13:20.903008: Epoch time: 13.49 s 
2025-03-02 22:13:21.484402:  
2025-03-02 22:13:21.490477: Epoch 96 
2025-03-02 22:13:21.495494: Current learning rate: 0.00055 
2025-03-02 22:13:35.031758: train_loss -0.9486 
2025-03-02 22:13:35.037827: val_loss -0.8298 
2025-03-02 22:13:35.043967: Pseudo dice [np.float32(0.8954), np.float32(0.8774)] 
2025-03-02 22:13:35.049648: Epoch time: 13.55 s 
2025-03-02 22:13:35.655385:  
2025-03-02 22:13:35.664969: Epoch 97 
2025-03-02 22:13:35.672762: Current learning rate: 0.00043 
2025-03-02 22:13:49.412323: train_loss -0.9484 
2025-03-02 22:13:49.419460: val_loss -0.8291 
2025-03-02 22:13:49.425045: Pseudo dice [np.float32(0.8949), np.float32(0.8783)] 
2025-03-02 22:13:49.430634: Epoch time: 13.76 s 
2025-03-02 22:13:50.041675:  
2025-03-02 22:13:50.048850: Epoch 98 
2025-03-02 22:13:50.055558: Current learning rate: 0.0003 
2025-03-02 22:14:03.406240: train_loss -0.9512 
2025-03-02 22:14:03.413874: val_loss -0.8204 
2025-03-02 22:14:03.419478: Pseudo dice [np.float32(0.8898), np.float32(0.8736)] 
2025-03-02 22:14:03.424577: Epoch time: 13.37 s 
2025-03-02 22:14:04.029059:  
2025-03-02 22:14:04.035153: Epoch 99 
2025-03-02 22:14:04.041330: Current learning rate: 0.00016 
2025-03-02 22:14:17.294601: train_loss -0.9474 
2025-03-02 22:14:17.301745: val_loss -0.8237 
2025-03-02 22:14:17.307351: Pseudo dice [np.float32(0.8918), np.float32(0.8744)] 
2025-03-02 22:14:17.312950: Epoch time: 13.27 s 
2025-03-02 22:14:18.164418: Training done. 
2025-03-02 22:14:18.216556: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset004_Hippocampus\splits_final.json 
2025-03-02 22:14:18.228126: The split file contains 5 splits. 
2025-03-02 22:14:18.239161: Desired fold for training: 0 
2025-03-02 22:14:18.250186: This split has 208 training and 52 validation cases. 
2025-03-02 22:14:18.259206: predicting hippocampus_017 
2025-03-02 22:14:18.266766: hippocampus_017, shape torch.Size([1, 32, 48, 35]), rank 0 
2025-03-02 22:14:18.475753: predicting hippocampus_019 
2025-03-02 22:14:18.482263: hippocampus_019, shape torch.Size([1, 41, 47, 36]), rank 0 
2025-03-02 22:14:18.549993: predicting hippocampus_033 
2025-03-02 22:14:18.558504: hippocampus_033, shape torch.Size([1, 38, 48, 33]), rank 0 
2025-03-02 22:14:18.593494: predicting hippocampus_035 
2025-03-02 22:14:18.601536: hippocampus_035, shape torch.Size([1, 37, 47, 35]), rank 0 
2025-03-02 22:14:18.641438: predicting hippocampus_037 
2025-03-02 22:14:18.648440: hippocampus_037, shape torch.Size([1, 32, 51, 34]), rank 0 
2025-03-02 22:14:18.691000: predicting hippocampus_049 
2025-03-02 22:14:18.700776: hippocampus_049, shape torch.Size([1, 36, 51, 35]), rank 0 
2025-03-02 22:14:18.739963: predicting hippocampus_052 
2025-03-02 22:14:18.749592: hippocampus_052, shape torch.Size([1, 40, 52, 34]), rank 0 
2025-03-02 22:14:18.788263: predicting hippocampus_065 
2025-03-02 22:14:18.796787: hippocampus_065, shape torch.Size([1, 37, 52, 39]), rank 0 
2025-03-02 22:14:18.838274: predicting hippocampus_083 
2025-03-02 22:14:18.847296: hippocampus_083, shape torch.Size([1, 37, 52, 33]), rank 0 
2025-03-02 22:14:18.890569: predicting hippocampus_088 
2025-03-02 22:14:18.899866: hippocampus_088, shape torch.Size([1, 35, 52, 40]), rank 0 
2025-03-02 22:14:23.461211: predicting hippocampus_090 
2025-03-02 22:14:23.469743: hippocampus_090, shape torch.Size([1, 40, 50, 37]), rank 0 
2025-03-02 22:14:23.513402: predicting hippocampus_092 
2025-03-02 22:14:23.528928: hippocampus_092, shape torch.Size([1, 28, 49, 38]), rank 0 
2025-03-02 22:14:23.594072: predicting hippocampus_095 
2025-03-02 22:14:23.604096: hippocampus_095, shape torch.Size([1, 39, 49, 34]), rank 0 
2025-03-02 22:14:23.667253: predicting hippocampus_107 
2025-03-02 22:14:23.676769: hippocampus_107, shape torch.Size([1, 34, 55, 35]), rank 0 
2025-03-02 22:14:23.755455: predicting hippocampus_108 
2025-03-02 22:14:23.763975: hippocampus_108, shape torch.Size([1, 37, 53, 36]), rank 0 
2025-03-02 22:14:23.808569: predicting hippocampus_123 
2025-03-02 22:14:23.817594: hippocampus_123, shape torch.Size([1, 38, 53, 32]), rank 0 
2025-03-02 22:14:23.858676: predicting hippocampus_125 
2025-03-02 22:14:23.867704: hippocampus_125, shape torch.Size([1, 39, 42, 43]), rank 0 
2025-03-02 22:14:23.934341: predicting hippocampus_157 
2025-03-02 22:14:23.945867: hippocampus_157, shape torch.Size([1, 35, 51, 36]), rank 0 
2025-03-02 22:14:23.989973: predicting hippocampus_164 
2025-03-02 22:14:23.999994: hippocampus_164, shape torch.Size([1, 47, 48, 41]), rank 0 
2025-03-02 22:14:24.115693: predicting hippocampus_169 
2025-03-02 22:14:24.125644: hippocampus_169, shape torch.Size([1, 39, 45, 36]), rank 0 
2025-03-02 22:14:24.167292: predicting hippocampus_175 
2025-03-02 22:14:24.175808: hippocampus_175, shape torch.Size([1, 35, 47, 33]), rank 0 
2025-03-02 22:14:24.215898: predicting hippocampus_185 
2025-03-02 22:14:24.224412: hippocampus_185, shape torch.Size([1, 33, 49, 35]), rank 0 
2025-03-02 22:14:24.262162: predicting hippocampus_190 
2025-03-02 22:14:24.274189: hippocampus_190, shape torch.Size([1, 30, 52, 37]), rank 0 
2025-03-02 22:14:24.315778: predicting hippocampus_194 
2025-03-02 22:14:24.324297: hippocampus_194, shape torch.Size([1, 30, 50, 35]), rank 0 
2025-03-02 22:14:24.366889: predicting hippocampus_204 
2025-03-02 22:14:24.377427: hippocampus_204, shape torch.Size([1, 39, 48, 36]), rank 0 
2025-03-02 22:14:24.419009: predicting hippocampus_205 
2025-03-02 22:14:24.427525: hippocampus_205, shape torch.Size([1, 32, 47, 32]), rank 0 
2025-03-02 22:14:24.465600: predicting hippocampus_210 
2025-03-02 22:14:24.473647: hippocampus_210, shape torch.Size([1, 40, 48, 34]), rank 0 
2025-03-02 22:14:24.512189: predicting hippocampus_217 
2025-03-02 22:14:24.521209: hippocampus_217, shape torch.Size([1, 27, 53, 38]), rank 0 
2025-03-02 22:14:24.564284: predicting hippocampus_219 
2025-03-02 22:14:24.572827: hippocampus_219, shape torch.Size([1, 39, 45, 37]), rank 0 
2025-03-02 22:14:24.614412: predicting hippocampus_229 
2025-03-02 22:14:24.622927: hippocampus_229, shape torch.Size([1, 35, 50, 33]), rank 0 
2025-03-02 22:14:24.665512: predicting hippocampus_244 
2025-03-02 22:14:24.675529: hippocampus_244, shape torch.Size([1, 30, 53, 38]), rank 0 
2025-03-02 22:14:24.721249: predicting hippocampus_261 
2025-03-02 22:14:24.729264: hippocampus_261, shape torch.Size([1, 33, 58, 36]), rank 0 
2025-03-02 22:14:24.801405: predicting hippocampus_264 
2025-03-02 22:14:24.810427: hippocampus_264, shape torch.Size([1, 37, 51, 38]), rank 0 
2025-03-02 22:14:24.855509: predicting hippocampus_277 
2025-03-02 22:14:24.866533: hippocampus_277, shape torch.Size([1, 29, 59, 33]), rank 0 
2025-03-02 22:14:24.938344: predicting hippocampus_280 
2025-03-02 22:14:24.946855: hippocampus_280, shape torch.Size([1, 32, 47, 37]), rank 0 
2025-03-02 22:14:24.992966: predicting hippocampus_286 
2025-03-02 22:14:25.002988: hippocampus_286, shape torch.Size([1, 46, 45, 37]), rank 0 
2025-03-02 22:14:25.078367: predicting hippocampus_288 
2025-03-02 22:14:25.086879: hippocampus_288, shape torch.Size([1, 42, 50, 38]), rank 0 
2025-03-02 22:14:25.154003: predicting hippocampus_289 
2025-03-02 22:14:25.166535: hippocampus_289, shape torch.Size([1, 36, 49, 35]), rank 0 
2025-03-02 22:14:25.217627: predicting hippocampus_296 
2025-03-02 22:14:25.226656: hippocampus_296, shape torch.Size([1, 35, 54, 35]), rank 0 
2025-03-02 22:14:25.268942: predicting hippocampus_305 
2025-03-02 22:14:25.278966: hippocampus_305, shape torch.Size([1, 30, 49, 34]), rank 0 
2025-03-02 22:14:25.325600: predicting hippocampus_308 
2025-03-02 22:14:25.335490: hippocampus_308, shape torch.Size([1, 40, 48, 38]), rank 0 
2025-03-02 22:14:25.375639: predicting hippocampus_317 
2025-03-02 22:14:25.385657: hippocampus_317, shape torch.Size([1, 34, 51, 33]), rank 0 
2025-03-02 22:14:25.436909: predicting hippocampus_327 
2025-03-02 22:14:25.445922: hippocampus_327, shape torch.Size([1, 27, 54, 36]), rank 0 
2025-03-02 22:14:25.489007: predicting hippocampus_330 
2025-03-02 22:14:25.497530: hippocampus_330, shape torch.Size([1, 33, 55, 35]), rank 0 
2025-03-02 22:14:25.539105: predicting hippocampus_332 
2025-03-02 22:14:25.547619: hippocampus_332, shape torch.Size([1, 33, 52, 35]), rank 0 
2025-03-02 22:14:25.591059: predicting hippocampus_338 
2025-03-02 22:14:25.600075: hippocampus_338, shape torch.Size([1, 43, 43, 37]), rank 0 
2025-03-02 22:14:25.670734: predicting hippocampus_349 
2025-03-02 22:14:25.681257: hippocampus_349, shape torch.Size([1, 34, 50, 34]), rank 0 
2025-03-02 22:14:25.727587: predicting hippocampus_350 
2025-03-02 22:14:25.736606: hippocampus_350, shape torch.Size([1, 34, 49, 35]), rank 0 
2025-03-02 22:14:25.778669: predicting hippocampus_356 
2025-03-02 22:14:25.787768: hippocampus_356, shape torch.Size([1, 37, 51, 36]), rank 0 
2025-03-02 22:14:25.830348: predicting hippocampus_358 
2025-03-02 22:14:25.839371: hippocampus_358, shape torch.Size([1, 34, 50, 35]), rank 0 
2025-03-02 22:14:25.885461: predicting hippocampus_374 
2025-03-02 22:14:25.893975: hippocampus_374, shape torch.Size([1, 39, 48, 38]), rank 0 
2025-03-02 22:14:25.935566: predicting hippocampus_394 
2025-03-02 22:14:25.944085: hippocampus_394, shape torch.Size([1, 32, 52, 36]), rank 0 
2025-03-02 22:14:30.591517: Validation complete 
2025-03-02 22:14:30.598528: Mean Validation Dice:  0.44281074612586246 
