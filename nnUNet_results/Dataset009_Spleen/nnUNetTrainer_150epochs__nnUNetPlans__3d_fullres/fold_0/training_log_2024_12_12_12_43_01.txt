
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-12-12 12:43:02.422205: do_dummy_2d_data_aug: False 
2024-12-12 12:43:02.429206: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset009_Spleen\splits_final.json 
2024-12-12 12:43:02.432208: The split file contains 5 splits. 
2024-12-12 12:43:02.435208: Desired fold for training: 0 
2024-12-12 12:43:02.437206: This split has 32 training and 9 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [64, 192, 160], 'median_image_size_in_voxels': [187.0, 512.0, 512.0], 'spacing': [1.6000100374221802, 0.7929689884185791, 0.7929689884185791], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset009_Spleen', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [5.0, 0.7929689884185791, 0.7929689884185791], 'original_median_shape_after_transp': [90, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1038.0, 'mean': 93.19259643554688, 'median': 97.0, 'min': -620.0, 'percentile_00_5': -42.0, 'percentile_99_5': 176.0, 'std': 40.78370666503906}}} 
 
2024-12-12 12:43:10.432394: unpacking dataset... 
2024-12-12 12:43:10.688189: unpacking done... 
2024-12-12 12:43:13.526638:  
2024-12-12 12:43:13.531424: Epoch 100 
2024-12-12 12:43:13.534943: Current learning rate: 0.00372 
2024-12-12 12:43:59.563167: train_loss -0.8874 
2024-12-12 12:43:59.568177: val_loss -0.8614 
2024-12-12 12:43:59.573199: Pseudo dice [np.float32(0.9391)] 
2024-12-12 12:43:59.575713: Epoch time: 46.04 s 
2024-12-12 12:43:59.579760: Yayy! New best EMA pseudo Dice: 0.9355999827384949 
2024-12-12 12:44:00.320769:  
2024-12-12 12:44:00.325292: Epoch 101 
2024-12-12 12:44:00.328806: Current learning rate: 0.00365 
2024-12-12 12:44:41.591565: train_loss -0.8902 
2024-12-12 12:44:41.597646: val_loss -0.8849 
2024-12-12 12:44:41.600703: Pseudo dice [np.float32(0.9285)] 
2024-12-12 12:44:41.605802: Epoch time: 41.27 s 
2024-12-12 12:44:42.143622:  
2024-12-12 12:44:42.148643: Epoch 102 
2024-12-12 12:44:42.151154: Current learning rate: 0.00359 
2024-12-12 12:45:23.422482: train_loss -0.8068 
2024-12-12 12:45:23.428036: val_loss -0.8098 
2024-12-12 12:45:23.431597: Pseudo dice [np.float32(0.9209)] 
2024-12-12 12:45:23.434170: Epoch time: 41.28 s 
2024-12-12 12:45:23.967934:  
2024-12-12 12:45:23.973507: Epoch 103 
2024-12-12 12:45:23.976520: Current learning rate: 0.00352 
2024-12-12 12:46:05.180418: train_loss -0.8805 
2024-12-12 12:46:05.185430: val_loss -0.7976 
2024-12-12 12:46:05.188937: Pseudo dice [np.float32(0.9108)] 
2024-12-12 12:46:05.192949: Epoch time: 41.21 s 
2024-12-12 12:46:05.726673:  
2024-12-12 12:46:05.731686: Epoch 104 
2024-12-12 12:46:05.735237: Current learning rate: 0.00345 
2024-12-12 12:46:52.236247: train_loss -0.9127 
2024-12-12 12:46:52.242801: val_loss -0.871 
2024-12-12 12:46:52.245310: Pseudo dice [np.float32(0.923)] 
2024-12-12 12:46:52.248819: Epoch time: 46.51 s 
2024-12-12 12:46:52.904979:  
2024-12-12 12:46:52.910005: Epoch 105 
2024-12-12 12:46:52.912163: Current learning rate: 0.00338 
2024-12-12 12:47:34.488085: train_loss -0.903 
2024-12-12 12:47:34.493096: val_loss -0.8731 
2024-12-12 12:47:34.496605: Pseudo dice [np.float32(0.9393)] 
2024-12-12 12:47:34.499111: Epoch time: 41.58 s 
2024-12-12 12:47:35.033921:  
2024-12-12 12:47:35.040003: Epoch 106 
2024-12-12 12:47:35.043104: Current learning rate: 0.00332 
2024-12-12 12:48:16.615721: train_loss -0.9107 
2024-12-12 12:48:16.621260: val_loss -0.8709 
2024-12-12 12:48:16.625271: Pseudo dice [np.float32(0.9146)] 
2024-12-12 12:48:16.627780: Epoch time: 41.58 s 
2024-12-12 12:48:17.165498:  
2024-12-12 12:48:17.170510: Epoch 107 
2024-12-12 12:48:17.174023: Current learning rate: 0.00325 
2024-12-12 12:48:58.783762: train_loss -0.8948 
2024-12-12 12:48:58.790282: val_loss -0.8271 
2024-12-12 12:48:58.795296: Pseudo dice [np.float32(0.8802)] 
2024-12-12 12:48:58.799807: Epoch time: 41.62 s 
2024-12-12 12:48:59.347381:  
2024-12-12 12:48:59.352410: Epoch 108 
2024-12-12 12:48:59.355362: Current learning rate: 0.00318 
2024-12-12 12:49:40.902090: train_loss -0.9008 
2024-12-12 12:49:40.908605: val_loss -0.8456 
2024-12-12 12:49:40.911770: Pseudo dice [np.float32(0.9311)] 
2024-12-12 12:49:40.914860: Epoch time: 41.56 s 
2024-12-12 12:49:41.452531:  
2024-12-12 12:49:41.457545: Epoch 109 
2024-12-12 12:49:41.460556: Current learning rate: 0.00311 
2024-12-12 12:50:23.034105: train_loss -0.8909 
2024-12-12 12:50:23.039117: val_loss -0.7963 
2024-12-12 12:50:23.042626: Pseudo dice [np.float32(0.913)] 
2024-12-12 12:50:23.046637: Epoch time: 41.58 s 
2024-12-12 12:50:23.582596:  
2024-12-12 12:50:23.588168: Epoch 110 
2024-12-12 12:50:23.591758: Current learning rate: 0.00304 
2024-12-12 12:51:05.164006: train_loss -0.8989 
2024-12-12 12:51:05.170024: val_loss -0.8272 
2024-12-12 12:51:05.173542: Pseudo dice [np.float32(0.909)] 
2024-12-12 12:51:05.176585: Epoch time: 41.58 s 
2024-12-12 12:51:05.710128:  
2024-12-12 12:51:05.715150: Epoch 111 
2024-12-12 12:51:05.718743: Current learning rate: 0.00297 
2024-12-12 12:51:47.134595: train_loss -0.8942 
2024-12-12 12:51:47.139609: val_loss -0.722 
2024-12-12 12:51:47.143115: Pseudo dice [np.float32(0.8725)] 
2024-12-12 12:51:47.146126: Epoch time: 41.43 s 
2024-12-12 12:51:47.679852:  
2024-12-12 12:51:47.685381: Epoch 112 
2024-12-12 12:51:47.688935: Current learning rate: 0.00291 
2024-12-12 12:52:28.855749: train_loss -0.8924 
2024-12-12 12:52:28.861091: val_loss -0.7794 
2024-12-12 12:52:28.863101: Pseudo dice [np.float32(0.9035)] 
2024-12-12 12:52:28.867136: Epoch time: 41.18 s 
2024-12-12 12:52:29.551876:  
2024-12-12 12:52:29.558479: Epoch 113 
2024-12-12 12:52:29.561020: Current learning rate: 0.00284 
2024-12-12 12:53:10.695920: train_loss -0.9092 
2024-12-12 12:53:10.700994: val_loss -0.7994 
2024-12-12 12:53:10.704610: Pseudo dice [np.float32(0.9149)] 
2024-12-12 12:53:10.707677: Epoch time: 41.14 s 
2024-12-12 12:53:11.243893:  
2024-12-12 12:53:11.249447: Epoch 114 
2024-12-12 12:53:11.252027: Current learning rate: 0.00277 
2024-12-12 12:53:52.403535: train_loss -0.9187 
2024-12-12 12:53:52.409545: val_loss -0.7056 
2024-12-12 12:53:52.412555: Pseudo dice [np.float32(0.8887)] 
2024-12-12 12:53:52.415063: Epoch time: 41.16 s 
2024-12-12 12:53:52.950239:  
2024-12-12 12:53:52.955796: Epoch 115 
2024-12-12 12:53:52.959388: Current learning rate: 0.0027 
2024-12-12 12:54:34.106684: train_loss -0.9005 
2024-12-12 12:54:34.112228: val_loss -0.8685 
2024-12-12 12:54:34.115261: Pseudo dice [np.float32(0.9345)] 
2024-12-12 12:54:34.118769: Epoch time: 41.16 s 
2024-12-12 12:54:34.662131:  
2024-12-12 12:54:34.667722: Epoch 116 
2024-12-12 12:54:34.670792: Current learning rate: 0.00263 
2024-12-12 12:55:15.818778: train_loss -0.9137 
2024-12-12 12:55:15.824315: val_loss -0.8666 
2024-12-12 12:55:15.826645: Pseudo dice [np.float32(0.9244)] 
2024-12-12 12:55:15.831204: Epoch time: 41.16 s 
2024-12-12 12:55:16.377359:  
2024-12-12 12:55:16.382001: Epoch 117 
2024-12-12 12:55:16.384539: Current learning rate: 0.00256 
2024-12-12 12:55:57.569213: train_loss -0.9218 
2024-12-12 12:55:57.575843: val_loss -0.9142 
2024-12-12 12:55:57.578912: Pseudo dice [np.float32(0.9584)] 
2024-12-12 12:55:57.581981: Epoch time: 41.19 s 
2024-12-12 12:55:58.121421:  
2024-12-12 12:55:58.125431: Epoch 118 
2024-12-12 12:55:58.129442: Current learning rate: 0.00249 
2024-12-12 12:56:39.293005: train_loss -0.9304 
2024-12-12 12:56:39.298575: val_loss -0.8507 
2024-12-12 12:56:39.301610: Pseudo dice [np.float32(0.8992)] 
2024-12-12 12:56:39.304118: Epoch time: 41.17 s 
2024-12-12 12:56:39.852611:  
2024-12-12 12:56:39.857621: Epoch 119 
2024-12-12 12:56:39.860630: Current learning rate: 0.00242 
2024-12-12 12:57:21.033848: train_loss -0.9189 
2024-12-12 12:57:21.039860: val_loss -0.7848 
2024-12-12 12:57:21.042868: Pseudo dice [np.float32(0.9072)] 
2024-12-12 12:57:21.046377: Epoch time: 41.18 s 
2024-12-12 12:57:21.597226:  
2024-12-12 12:57:21.603243: Epoch 120 
2024-12-12 12:57:21.606253: Current learning rate: 0.00235 
2024-12-12 12:58:02.795874: train_loss -0.9139 
2024-12-12 12:58:02.801449: val_loss -0.8836 
2024-12-12 12:58:02.805474: Pseudo dice [np.float32(0.9419)] 
2024-12-12 12:58:02.807989: Epoch time: 41.2 s 
2024-12-12 12:58:03.497351:  
2024-12-12 12:58:03.502975: Epoch 121 
2024-12-12 12:58:03.506023: Current learning rate: 0.00228 
2024-12-12 12:58:44.662528: train_loss -0.9196 
2024-12-12 12:58:44.669041: val_loss -0.9119 
2024-12-12 12:58:44.671549: Pseudo dice [np.float32(0.9563)] 
2024-12-12 12:58:44.675060: Epoch time: 41.17 s 
2024-12-12 12:58:45.219413:  
2024-12-12 12:58:45.224931: Epoch 122 
2024-12-12 12:58:45.228445: Current learning rate: 0.00221 
2024-12-12 12:59:26.383440: train_loss -0.9125 
2024-12-12 12:59:26.389471: val_loss -0.8361 
2024-12-12 12:59:26.392991: Pseudo dice [np.float32(0.9308)] 
2024-12-12 12:59:26.396473: Epoch time: 41.17 s 
2024-12-12 12:59:26.938448:  
2024-12-12 12:59:26.944131: Epoch 123 
2024-12-12 12:59:26.947238: Current learning rate: 0.00214 
2024-12-12 13:00:08.121582: train_loss -0.9211 
2024-12-12 13:00:08.128129: val_loss -0.9179 
2024-12-12 13:00:08.131637: Pseudo dice [np.float32(0.9577)] 
2024-12-12 13:00:08.134644: Epoch time: 41.18 s 
2024-12-12 13:00:08.676373:  
2024-12-12 13:00:08.682441: Epoch 124 
2024-12-12 13:00:08.684947: Current learning rate: 0.00207 
2024-12-12 13:00:49.855722: train_loss -0.9199 
2024-12-12 13:00:49.862241: val_loss -0.9108 
2024-12-12 13:00:49.865298: Pseudo dice [np.float32(0.9553)] 
2024-12-12 13:00:49.868325: Epoch time: 41.18 s 
2024-12-12 13:00:50.411523:  
2024-12-12 13:00:50.417088: Epoch 125 
2024-12-12 13:00:50.420141: Current learning rate: 0.00199 
2024-12-12 13:01:31.582325: train_loss -0.9256 
2024-12-12 13:01:31.588343: val_loss -0.8989 
2024-12-12 13:01:31.591849: Pseudo dice [np.float32(0.9436)] 
2024-12-12 13:01:31.594861: Epoch time: 41.17 s 
2024-12-12 13:01:32.140326:  
2024-12-12 13:01:32.145840: Epoch 126 
2024-12-12 13:01:32.149353: Current learning rate: 0.00192 
2024-12-12 13:02:13.338012: train_loss -0.9176 
2024-12-12 13:02:13.344102: val_loss -0.8874 
2024-12-12 13:02:13.346157: Pseudo dice [np.float32(0.9377)] 
2024-12-12 13:02:13.350720: Epoch time: 41.2 s 
2024-12-12 13:02:13.894684:  
2024-12-12 13:02:13.900234: Epoch 127 
2024-12-12 13:02:13.903279: Current learning rate: 0.00185 
2024-12-12 13:02:55.086776: train_loss -0.8987 
2024-12-12 13:02:55.092923: val_loss -0.9179 
2024-12-12 13:02:55.095429: Pseudo dice [np.float32(0.9578)] 
2024-12-12 13:02:55.099441: Epoch time: 41.19 s 
2024-12-12 13:02:55.643873:  
2024-12-12 13:02:55.649393: Epoch 128 
2024-12-12 13:02:55.652904: Current learning rate: 0.00178 
2024-12-12 13:03:36.858294: train_loss -0.9212 
2024-12-12 13:03:36.863831: val_loss -0.8415 
2024-12-12 13:03:36.867439: Pseudo dice [np.float32(0.9279)] 
2024-12-12 13:03:36.869979: Epoch time: 41.22 s 
2024-12-12 13:03:37.561635:  
2024-12-12 13:03:37.566653: Epoch 129 
2024-12-12 13:03:37.570217: Current learning rate: 0.0017 
2024-12-12 13:04:18.732958: train_loss -0.9301 
2024-12-12 13:04:18.738994: val_loss -0.8666 
2024-12-12 13:04:18.742032: Pseudo dice [np.float32(0.9375)] 
2024-12-12 13:04:18.745548: Epoch time: 41.17 s 
2024-12-12 13:04:19.291844:  
2024-12-12 13:04:19.296855: Epoch 130 
2024-12-12 13:04:19.300367: Current learning rate: 0.00163 
2024-12-12 13:05:00.457088: train_loss -0.9073 
2024-12-12 13:05:00.463606: val_loss -0.8781 
2024-12-12 13:05:00.467120: Pseudo dice [np.float32(0.9418)] 
2024-12-12 13:05:00.469625: Epoch time: 41.17 s 
2024-12-12 13:05:01.019341:  
2024-12-12 13:05:01.025863: Epoch 131 
2024-12-12 13:05:01.029374: Current learning rate: 0.00156 
2024-12-12 13:05:42.138813: train_loss -0.9013 
2024-12-12 13:05:42.144368: val_loss -0.9161 
2024-12-12 13:05:42.147922: Pseudo dice [np.float32(0.9529)] 
2024-12-12 13:05:42.150464: Epoch time: 41.12 s 
2024-12-12 13:05:42.152996: Yayy! New best EMA pseudo Dice: 0.9369000196456909 
2024-12-12 13:05:42.867549:  
2024-12-12 13:05:42.872593: Epoch 132 
2024-12-12 13:05:42.875643: Current learning rate: 0.00148 
2024-12-12 13:06:24.000149: train_loss -0.9283 
2024-12-12 13:06:24.005790: val_loss -0.8399 
2024-12-12 13:06:24.008848: Pseudo dice [np.float32(0.9339)] 
2024-12-12 13:06:24.012411: Epoch time: 41.13 s 
2024-12-12 13:06:24.555841:  
2024-12-12 13:06:24.561380: Epoch 133 
2024-12-12 13:06:24.564894: Current learning rate: 0.00141 
2024-12-12 13:07:05.715684: train_loss -0.9215 
2024-12-12 13:07:05.721713: val_loss -0.8352 
2024-12-12 13:07:05.725803: Pseudo dice [np.float32(0.9349)] 
2024-12-12 13:07:05.728350: Epoch time: 41.16 s 
2024-12-12 13:07:06.280949:  
2024-12-12 13:07:06.286137: Epoch 134 
2024-12-12 13:07:06.289648: Current learning rate: 0.00133 
2024-12-12 13:07:47.403042: train_loss -0.9174 
2024-12-12 13:07:47.409065: val_loss -0.8738 
2024-12-12 13:07:47.411635: Pseudo dice [np.float32(0.9523)] 
2024-12-12 13:07:47.414688: Epoch time: 41.12 s 
2024-12-12 13:07:47.418701: Yayy! New best EMA pseudo Dice: 0.9380000233650208 
2024-12-12 13:07:48.133053:  
2024-12-12 13:07:48.138570: Epoch 135 
2024-12-12 13:07:48.141077: Current learning rate: 0.00126 
2024-12-12 13:08:29.297842: train_loss -0.9255 
2024-12-12 13:08:29.304880: val_loss -0.9153 
2024-12-12 13:08:29.308391: Pseudo dice [np.float32(0.9519)] 
2024-12-12 13:08:29.310899: Epoch time: 41.17 s 
2024-12-12 13:08:29.314409: Yayy! New best EMA pseudo Dice: 0.9394000172615051 
2024-12-12 13:08:30.035461:  
2024-12-12 13:08:30.039806: Epoch 136 
2024-12-12 13:08:30.043822: Current learning rate: 0.00118 
2024-12-12 13:09:11.233594: train_loss -0.9256 
2024-12-12 13:09:11.240108: val_loss -0.9076 
2024-12-12 13:09:11.243620: Pseudo dice [np.float32(0.9543)] 
2024-12-12 13:09:11.246123: Epoch time: 41.2 s 
2024-12-12 13:09:11.249630: Yayy! New best EMA pseudo Dice: 0.9409000277519226 
2024-12-12 13:09:12.130275:  
2024-12-12 13:09:12.135326: Epoch 137 
2024-12-12 13:09:12.138867: Current learning rate: 0.00111 
2024-12-12 13:09:53.327224: train_loss -0.9199 
2024-12-12 13:09:53.332852: val_loss -0.8673 
2024-12-12 13:09:53.335864: Pseudo dice [np.float32(0.951)] 
2024-12-12 13:09:53.339373: Epoch time: 41.2 s 
2024-12-12 13:09:53.342881: Yayy! New best EMA pseudo Dice: 0.9419000148773193 
2024-12-12 13:09:54.065253:  
2024-12-12 13:09:54.071315: Epoch 138 
2024-12-12 13:09:54.074392: Current learning rate: 0.00103 
2024-12-12 13:10:35.253907: train_loss -0.9232 
2024-12-12 13:10:35.259919: val_loss -0.8702 
2024-12-12 13:10:35.263931: Pseudo dice [np.float32(0.9343)] 
2024-12-12 13:10:35.266438: Epoch time: 41.19 s 
2024-12-12 13:10:35.822407:  
2024-12-12 13:10:35.828469: Epoch 139 
2024-12-12 13:10:35.831521: Current learning rate: 0.00095 
2024-12-12 13:11:16.984785: train_loss -0.9249 
2024-12-12 13:11:16.990366: val_loss -0.8163 
2024-12-12 13:11:16.993914: Pseudo dice [np.float32(0.9172)] 
2024-12-12 13:11:16.997467: Epoch time: 41.16 s 
2024-12-12 13:11:17.555457:  
2024-12-12 13:11:17.561915: Epoch 140 
2024-12-12 13:11:17.565460: Current learning rate: 0.00087 
2024-12-12 13:11:58.753220: train_loss -0.9233 
2024-12-12 13:11:58.759424: val_loss -0.9149 
2024-12-12 13:11:58.761976: Pseudo dice [np.float32(0.9551)] 
2024-12-12 13:11:58.764519: Epoch time: 41.2 s 
2024-12-12 13:11:59.323075:  
2024-12-12 13:11:59.328609: Epoch 141 
2024-12-12 13:11:59.332124: Current learning rate: 0.00079 
2024-12-12 13:12:40.524970: train_loss -0.9293 
2024-12-12 13:12:40.530636: val_loss -0.8746 
2024-12-12 13:12:40.535250: Pseudo dice [np.float32(0.9393)] 
2024-12-12 13:12:40.537758: Epoch time: 41.2 s 
2024-12-12 13:12:41.098863:  
2024-12-12 13:12:41.104397: Epoch 142 
2024-12-12 13:12:41.107413: Current learning rate: 0.00071 
2024-12-12 13:13:22.290564: train_loss -0.9327 
2024-12-12 13:13:22.297133: val_loss -0.9152 
2024-12-12 13:13:22.300712: Pseudo dice [np.float32(0.9587)] 
2024-12-12 13:13:22.303795: Epoch time: 41.19 s 
2024-12-12 13:13:22.306828: Yayy! New best EMA pseudo Dice: 0.9420999884605408 
2024-12-12 13:13:23.028031:  
2024-12-12 13:13:23.034058: Epoch 143 
2024-12-12 13:13:23.037567: Current learning rate: 0.00063 
2024-12-12 13:14:09.403258: train_loss -0.9314 
2024-12-12 13:14:09.408272: val_loss -0.8949 
2024-12-12 13:14:09.411781: Pseudo dice [np.float32(0.9501)] 
2024-12-12 13:14:09.417800: Epoch time: 46.38 s 
2024-12-12 13:14:09.421816: Yayy! New best EMA pseudo Dice: 0.9429000020027161 
2024-12-12 13:14:10.170670:  
2024-12-12 13:14:10.176190: Epoch 144 
2024-12-12 13:14:10.179697: Current learning rate: 0.00055 
2024-12-12 13:14:53.099501: train_loss -0.9361 
2024-12-12 13:14:53.105086: val_loss -0.8803 
2024-12-12 13:14:53.108611: Pseudo dice [np.float32(0.9447)] 
2024-12-12 13:14:53.111170: Epoch time: 42.93 s 
2024-12-12 13:14:53.115189: Yayy! New best EMA pseudo Dice: 0.9430999755859375 
2024-12-12 13:14:53.975028:  
2024-12-12 13:14:53.980566: Epoch 145 
2024-12-12 13:14:53.983594: Current learning rate: 0.00047 
2024-12-12 13:15:36.380492: train_loss -0.9296 
2024-12-12 13:15:36.385512: val_loss -0.8948 
2024-12-12 13:15:36.389022: Pseudo dice [np.float32(0.9492)] 
2024-12-12 13:15:36.391528: Epoch time: 42.41 s 
2024-12-12 13:15:36.395034: Yayy! New best EMA pseudo Dice: 0.9437000155448914 
2024-12-12 13:15:37.158975:  
2024-12-12 13:15:37.165114: Epoch 146 
2024-12-12 13:15:37.168136: Current learning rate: 0.00038 
2024-12-12 13:16:19.157794: train_loss -0.9329 
2024-12-12 13:16:19.162971: val_loss -0.8824 
2024-12-12 13:16:19.166480: Pseudo dice [np.float32(0.9383)] 
2024-12-12 13:16:19.168987: Epoch time: 42.0 s 
2024-12-12 13:16:19.765544:  
2024-12-12 13:16:19.770558: Epoch 147 
2024-12-12 13:16:19.774069: Current learning rate: 0.0003 
2024-12-12 13:17:01.584113: train_loss -0.9332 
2024-12-12 13:17:01.589219: val_loss -0.8781 
2024-12-12 13:17:01.592787: Pseudo dice [np.float32(0.9396)] 
2024-12-12 13:17:01.596029: Epoch time: 41.82 s 
2024-12-12 13:17:02.192468:  
2024-12-12 13:17:02.198039: Epoch 148 
2024-12-12 13:17:02.200564: Current learning rate: 0.00021 
2024-12-12 13:17:45.355411: train_loss -0.9268 
2024-12-12 13:17:45.360952: val_loss -0.8512 
2024-12-12 13:17:45.364578: Pseudo dice [np.float32(0.9319)] 
2024-12-12 13:17:45.367604: Epoch time: 43.16 s 
2024-12-12 13:17:45.941406:  
2024-12-12 13:17:45.946420: Epoch 149 
2024-12-12 13:17:45.949929: Current learning rate: 0.00011 
2024-12-12 13:18:28.843481: train_loss -0.9305 
2024-12-12 13:18:28.844484: val_loss -0.9212 
2024-12-12 13:18:28.850087: Pseudo dice [np.float32(0.9627)] 
2024-12-12 13:18:28.853640: Epoch time: 42.9 s 
2024-12-12 13:18:28.857144: Yayy! New best EMA pseudo Dice: 0.9437999725341797 
2024-12-12 13:18:29.798501: Training done. 
2024-12-12 13:18:29.833017: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset009_Spleen\splits_final.json 
2024-12-12 13:18:29.840017: The split file contains 5 splits. 
2024-12-12 13:18:29.847018: Desired fold for training: 0 
2024-12-12 13:18:29.852016: This split has 32 training and 9 validation cases. 
2024-12-12 13:18:29.857016: predicting spleen_10 
2024-12-12 13:18:29.864017: spleen_10, shape torch.Size([1, 172, 631, 631]), rank 0 
2024-12-12 13:19:10.316401: predicting spleen_13 
2024-12-12 13:19:10.341403: spleen_13, shape torch.Size([1, 120, 479, 479]), rank 0 
2024-12-12 13:19:21.510555: predicting spleen_14 
2024-12-12 13:19:21.524555: spleen_14, shape torch.Size([1, 169, 550, 550]), rank 0 
2024-12-12 13:19:49.494140: predicting spleen_17 
2024-12-12 13:19:49.522141: spleen_17, shape torch.Size([1, 148, 396, 396]), rank 0 
2024-12-12 13:20:01.478538: predicting spleen_31 
2024-12-12 13:20:01.488538: spleen_31, shape torch.Size([1, 175, 478, 478]), rank 0 
2024-12-12 13:20:20.278320: predicting spleen_33 
2024-12-12 13:20:20.294830: spleen_33, shape torch.Size([1, 259, 596, 596]), rank 0 
2024-12-12 13:21:24.374278: predicting spleen_44 
2024-12-12 13:21:24.426395: spleen_44, shape torch.Size([1, 287, 562, 562]), rank 0 
2024-12-12 13:22:17.396447: predicting spleen_47 
2024-12-12 13:22:17.446447: spleen_47, shape torch.Size([1, 275, 507, 507]), rank 0 
2024-12-12 13:23:02.858433: predicting spleen_56 
2024-12-12 13:23:02.899941: spleen_56, shape torch.Size([1, 144, 451, 451]), rank 0 
2024-12-12 13:23:23.834468: Validation complete 
2024-12-12 13:23:23.839467: Mean Validation Dice:  0.8844007990201739 
