
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-12-12 12:08:47.218711: do_dummy_2d_data_aug: True 
2024-12-12 12:08:47.221710: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset009_Spleen\splits_final.json 
2024-12-12 12:08:47.225813: The split file contains 5 splits. 
2024-12-12 12:08:47.228224: Desired fold for training: 0 
2024-12-12 12:08:47.230221: This split has 32 training and 9 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_original_unet_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 256, 256], 'median_image_size_in_voxels': [60.0, 512.0, 512.0], 'spacing': [5.0, 0.7929689884185791, 0.7929689884185791], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset009_Spleen', 'plans_name': 'nnUNetPlans_original_unet', 'original_median_spacing_after_transp': [5.0, 0.7929689884185791, 0.7929689884185791], 'original_median_shape_after_transp': [90, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1038.0, 'mean': 93.19259643554688, 'median': 97.0, 'min': -620.0, 'percentile_00_5': -42.0, 'percentile_99_5': 176.0, 'std': 40.78370666503906}}} 
 
2024-12-12 12:08:54.120113: unpacking dataset... 
2024-12-12 12:08:54.336410: unpacking done... 
2024-12-12 12:08:57.614259:  
2024-12-12 12:08:57.618840: Epoch 100 
2024-12-12 12:08:57.621912: Current learning rate: 0.00372 
2024-12-12 12:09:40.171791: train_loss -0.6933 
2024-12-12 12:09:40.177986: val_loss -0.6183 
2024-12-12 12:09:40.183126: Pseudo dice [np.float32(0.8992)] 
2024-12-12 12:09:40.186728: Epoch time: 42.56 s 
2024-12-12 12:09:40.691144:  
2024-12-12 12:09:40.696802: Epoch 101 
2024-12-12 12:09:40.699393: Current learning rate: 0.00365 
2024-12-12 12:10:19.506559: train_loss -0.6686 
2024-12-12 12:10:19.511580: val_loss -0.6488 
2024-12-12 12:10:19.514721: Pseudo dice [np.float32(0.9128)] 
2024-12-12 12:10:19.517804: Epoch time: 38.81 s 
2024-12-12 12:10:20.026794:  
2024-12-12 12:10:20.030823: Epoch 102 
2024-12-12 12:10:20.034518: Current learning rate: 0.00359 
2024-12-12 12:10:58.367553: train_loss -0.6562 
2024-12-12 12:10:58.372730: val_loss -0.6081 
2024-12-12 12:10:58.378814: Pseudo dice [np.float32(0.8756)] 
2024-12-12 12:10:58.381850: Epoch time: 38.34 s 
2024-12-12 12:10:58.883743:  
2024-12-12 12:10:58.889345: Epoch 103 
2024-12-12 12:10:58.892457: Current learning rate: 0.00352 
2024-12-12 12:11:37.181426: train_loss -0.647 
2024-12-12 12:11:37.186178: val_loss -0.6103 
2024-12-12 12:11:37.189725: Pseudo dice [np.float32(0.8809)] 
2024-12-12 12:11:37.192874: Epoch time: 38.3 s 
2024-12-12 12:11:37.804920:  
2024-12-12 12:11:37.810354: Epoch 104 
2024-12-12 12:11:37.813975: Current learning rate: 0.00345 
2024-12-12 12:12:16.305640: train_loss -0.6347 
2024-12-12 12:12:16.311201: val_loss -0.6391 
2024-12-12 12:12:16.314307: Pseudo dice [np.float32(0.8747)] 
2024-12-12 12:12:16.317382: Epoch time: 38.5 s 
2024-12-12 12:12:16.883578:  
2024-12-12 12:12:16.889240: Epoch 105 
2024-12-12 12:12:16.892374: Current learning rate: 0.00338 
2024-12-12 12:12:56.270260: train_loss -0.6363 
2024-12-12 12:12:56.275929: val_loss -0.6033 
2024-12-12 12:12:56.278435: Pseudo dice [np.float32(0.9226)] 
2024-12-12 12:12:56.281943: Epoch time: 39.39 s 
2024-12-12 12:12:56.917873:  
2024-12-12 12:12:56.922907: Epoch 106 
2024-12-12 12:12:56.925945: Current learning rate: 0.00332 
2024-12-12 12:13:35.641641: train_loss -0.6441 
2024-12-12 12:13:35.646827: val_loss -0.6731 
2024-12-12 12:13:35.649333: Pseudo dice [np.float32(0.9129)] 
2024-12-12 12:13:35.652921: Epoch time: 38.72 s 
2024-12-12 12:13:36.191307:  
2024-12-12 12:13:36.196473: Epoch 107 
2024-12-12 12:13:36.198983: Current learning rate: 0.00325 
2024-12-12 12:14:15.041155: train_loss -0.6502 
2024-12-12 12:14:15.046830: val_loss -0.6873 
2024-12-12 12:14:15.050344: Pseudo dice [np.float32(0.9114)] 
2024-12-12 12:14:15.053452: Epoch time: 38.85 s 
2024-12-12 12:14:15.629549:  
2024-12-12 12:14:15.636065: Epoch 108 
2024-12-12 12:14:15.639577: Current learning rate: 0.00318 
2024-12-12 12:14:54.254764: train_loss -0.6234 
2024-12-12 12:14:54.260865: val_loss -0.677 
2024-12-12 12:14:54.264087: Pseudo dice [np.float32(0.9095)] 
2024-12-12 12:14:54.267714: Epoch time: 38.63 s 
2024-12-12 12:14:54.846549:  
2024-12-12 12:14:54.851923: Epoch 109 
2024-12-12 12:14:54.855441: Current learning rate: 0.00311 
2024-12-12 12:15:34.574822: train_loss -0.6382 
2024-12-12 12:15:34.580850: val_loss -0.6612 
2024-12-12 12:15:34.584357: Pseudo dice [np.float32(0.9117)] 
2024-12-12 12:15:34.587373: Epoch time: 39.73 s 
2024-12-12 12:15:35.154024:  
2024-12-12 12:15:35.159051: Epoch 110 
2024-12-12 12:15:35.161172: Current learning rate: 0.00304 
2024-12-12 12:16:14.005133: train_loss -0.6489 
2024-12-12 12:16:14.010654: val_loss -0.655 
2024-12-12 12:16:14.014169: Pseudo dice [np.float32(0.9268)] 
2024-12-12 12:16:14.016672: Epoch time: 38.85 s 
2024-12-12 12:16:14.595571:  
2024-12-12 12:16:14.600644: Epoch 111 
2024-12-12 12:16:14.604186: Current learning rate: 0.00297 
2024-12-12 12:16:52.938653: train_loss -0.6655 
2024-12-12 12:16:52.944274: val_loss -0.6636 
2024-12-12 12:16:52.947645: Pseudo dice [np.float32(0.9296)] 
2024-12-12 12:16:52.950151: Epoch time: 38.34 s 
2024-12-12 12:16:53.665286:  
2024-12-12 12:16:53.671314: Epoch 112 
2024-12-12 12:16:53.676338: Current learning rate: 0.00291 
2024-12-12 12:17:32.060588: train_loss -0.6633 
2024-12-12 12:17:32.065739: val_loss -0.6776 
2024-12-12 12:17:32.069269: Pseudo dice [np.float32(0.919)] 
2024-12-12 12:17:32.071477: Epoch time: 38.4 s 
2024-12-12 12:17:32.643453:  
2024-12-12 12:17:32.649473: Epoch 113 
2024-12-12 12:17:32.652991: Current learning rate: 0.00284 
2024-12-12 12:18:11.508739: train_loss -0.638 
2024-12-12 12:18:11.515987: val_loss -0.6802 
2024-12-12 12:18:11.519537: Pseudo dice [np.float32(0.9188)] 
2024-12-12 12:18:11.523119: Epoch time: 38.87 s 
2024-12-12 12:18:12.117069:  
2024-12-12 12:18:12.123630: Epoch 114 
2024-12-12 12:18:12.127213: Current learning rate: 0.00277 
2024-12-12 12:18:50.983164: train_loss -0.6603 
2024-12-12 12:18:50.988878: val_loss -0.6858 
2024-12-12 12:18:50.993447: Pseudo dice [np.float32(0.9214)] 
2024-12-12 12:18:50.996505: Epoch time: 38.87 s 
2024-12-12 12:18:51.605329:  
2024-12-12 12:18:51.610439: Epoch 115 
2024-12-12 12:18:51.614405: Current learning rate: 0.0027 
2024-12-12 12:19:30.762553: train_loss -0.6655 
2024-12-12 12:19:30.769252: val_loss -0.6805 
2024-12-12 12:19:30.772401: Pseudo dice [np.float32(0.9156)] 
2024-12-12 12:19:30.775453: Epoch time: 39.16 s 
2024-12-12 12:19:31.350212:  
2024-12-12 12:19:31.355372: Epoch 116 
2024-12-12 12:19:31.358532: Current learning rate: 0.00263 
2024-12-12 12:20:09.780775: train_loss -0.6489 
2024-12-12 12:20:09.786455: val_loss -0.6763 
2024-12-12 12:20:09.788983: Pseudo dice [np.float32(0.9368)] 
2024-12-12 12:20:09.791517: Epoch time: 38.43 s 
2024-12-12 12:20:10.364719:  
2024-12-12 12:20:10.368764: Epoch 117 
2024-12-12 12:20:10.373429: Current learning rate: 0.00256 
2024-12-12 12:20:49.260299: train_loss -0.6681 
2024-12-12 12:20:49.265993: val_loss -0.658 
2024-12-12 12:20:49.269070: Pseudo dice [np.float32(0.9226)] 
2024-12-12 12:20:49.272163: Epoch time: 38.9 s 
2024-12-12 12:20:49.846880:  
2024-12-12 12:20:49.852973: Epoch 118 
2024-12-12 12:20:49.856016: Current learning rate: 0.00249 
2024-12-12 12:21:28.426669: train_loss -0.6424 
2024-12-12 12:21:28.432731: val_loss -0.6162 
2024-12-12 12:21:28.436405: Pseudo dice [np.float32(0.9158)] 
2024-12-12 12:21:28.439555: Epoch time: 38.58 s 
2024-12-12 12:21:29.013051:  
2024-12-12 12:21:29.018678: Epoch 119 
2024-12-12 12:21:29.022722: Current learning rate: 0.00242 
2024-12-12 12:22:07.392047: train_loss -0.6692 
2024-12-12 12:22:07.398709: val_loss -0.6482 
2024-12-12 12:22:07.401810: Pseudo dice [np.float32(0.932)] 
2024-12-12 12:22:07.404840: Epoch time: 38.38 s 
2024-12-12 12:22:08.141955:  
2024-12-12 12:22:08.147049: Epoch 120 
2024-12-12 12:22:08.150289: Current learning rate: 0.00235 
2024-12-12 12:22:46.572532: train_loss -0.654 
2024-12-12 12:22:46.578101: val_loss -0.6679 
2024-12-12 12:22:46.580610: Pseudo dice [np.float32(0.937)] 
2024-12-12 12:22:46.584120: Epoch time: 38.43 s 
2024-12-12 12:22:46.587625: Yayy! New best EMA pseudo Dice: 0.9204999804496765 
2024-12-12 12:22:47.431359:  
2024-12-12 12:22:47.437422: Epoch 121 
2024-12-12 12:22:47.440492: Current learning rate: 0.00228 
2024-12-12 12:23:26.228148: train_loss -0.6331 
2024-12-12 12:23:26.228654: val_loss -0.6225 
2024-12-12 12:23:26.234391: Pseudo dice [np.float32(0.9086)] 
2024-12-12 12:23:26.237429: Epoch time: 38.8 s 
2024-12-12 12:23:26.819550:  
2024-12-12 12:23:26.819550: Epoch 122 
2024-12-12 12:23:26.824739: Current learning rate: 0.00221 
2024-12-12 12:24:05.402328: train_loss -0.6706 
2024-12-12 12:24:05.403840: val_loss -0.6633 
2024-12-12 12:24:05.410134: Pseudo dice [np.float32(0.9428)] 
2024-12-12 12:24:05.412706: Epoch time: 38.58 s 
2024-12-12 12:24:05.417262: Yayy! New best EMA pseudo Dice: 0.9217000007629395 
2024-12-12 12:24:06.249484:  
2024-12-12 12:24:06.250485: Epoch 123 
2024-12-12 12:24:06.255705: Current learning rate: 0.00214 
2024-12-12 12:24:44.741600: train_loss -0.6714 
2024-12-12 12:24:44.742185: val_loss -0.6459 
2024-12-12 12:24:44.747251: Pseudo dice [np.float32(0.9272)] 
2024-12-12 12:24:44.750798: Epoch time: 38.49 s 
2024-12-12 12:24:44.754476: Yayy! New best EMA pseudo Dice: 0.9222000241279602 
2024-12-12 12:24:45.588916:  
2024-12-12 12:24:45.595013: Epoch 124 
2024-12-12 12:24:45.597801: Current learning rate: 0.00207 
2024-12-12 12:25:24.100313: train_loss -0.6862 
2024-12-12 12:25:24.106039: val_loss -0.6914 
2024-12-12 12:25:24.110246: Pseudo dice [np.float32(0.935)] 
2024-12-12 12:25:24.113415: Epoch time: 38.51 s 
2024-12-12 12:25:24.115963: Yayy! New best EMA pseudo Dice: 0.9235000014305115 
2024-12-12 12:25:24.965772:  
2024-12-12 12:25:24.971318: Epoch 125 
2024-12-12 12:25:24.974828: Current learning rate: 0.00199 
2024-12-12 12:26:03.910308: train_loss -0.6518 
2024-12-12 12:26:03.916322: val_loss -0.682 
2024-12-12 12:26:03.919829: Pseudo dice [np.float32(0.9451)] 
2024-12-12 12:26:03.922838: Epoch time: 38.95 s 
2024-12-12 12:26:03.926347: Yayy! New best EMA pseudo Dice: 0.9257000088691711 
2024-12-12 12:26:04.709984:  
2024-12-12 12:26:04.715516: Epoch 126 
2024-12-12 12:26:04.719069: Current learning rate: 0.00192 
2024-12-12 12:26:43.944537: train_loss -0.6869 
2024-12-12 12:26:43.950083: val_loss -0.7107 
2024-12-12 12:26:43.953626: Pseudo dice [np.float32(0.9374)] 
2024-12-12 12:26:43.956715: Epoch time: 39.24 s 
2024-12-12 12:26:43.959763: Yayy! New best EMA pseudo Dice: 0.926800012588501 
2024-12-12 12:26:44.745160:  
2024-12-12 12:26:44.750175: Epoch 127 
2024-12-12 12:26:44.753687: Current learning rate: 0.00185 
2024-12-12 12:27:24.021160: train_loss -0.6668 
2024-12-12 12:27:24.026772: val_loss -0.6819 
2024-12-12 12:27:24.029865: Pseudo dice [np.float32(0.9105)] 
2024-12-12 12:27:24.032888: Epoch time: 39.28 s 
2024-12-12 12:27:24.730012:  
2024-12-12 12:27:24.735575: Epoch 128 
2024-12-12 12:27:24.739232: Current learning rate: 0.00178 
2024-12-12 12:28:03.979456: train_loss -0.6613 
2024-12-12 12:28:03.985883: val_loss -0.6466 
2024-12-12 12:28:03.988401: Pseudo dice [np.float32(0.9147)] 
2024-12-12 12:28:03.991937: Epoch time: 39.25 s 
2024-12-12 12:28:04.532337:  
2024-12-12 12:28:04.537354: Epoch 129 
2024-12-12 12:28:04.540864: Current learning rate: 0.0017 
2024-12-12 12:28:43.784897: train_loss -0.6972 
2024-12-12 12:28:43.789943: val_loss -0.6844 
2024-12-12 12:28:43.794451: Pseudo dice [np.float32(0.9326)] 
2024-12-12 12:28:43.798470: Epoch time: 39.25 s 
2024-12-12 12:28:44.339493:  
2024-12-12 12:28:44.344601: Epoch 130 
2024-12-12 12:28:44.348667: Current learning rate: 0.00163 
2024-12-12 12:29:23.597079: train_loss -0.6686 
2024-12-12 12:29:23.602879: val_loss -0.6811 
2024-12-12 12:29:23.606422: Pseudo dice [np.float32(0.9374)] 
2024-12-12 12:29:23.609467: Epoch time: 39.26 s 
2024-12-12 12:29:24.153378:  
2024-12-12 12:29:24.158396: Epoch 131 
2024-12-12 12:29:24.161906: Current learning rate: 0.00156 
2024-12-12 12:30:03.426012: train_loss -0.6578 
2024-12-12 12:30:03.431578: val_loss -0.6843 
2024-12-12 12:30:03.434602: Pseudo dice [np.float32(0.9173)] 
2024-12-12 12:30:03.438149: Epoch time: 39.27 s 
2024-12-12 12:30:03.978449:  
2024-12-12 12:30:03.983977: Epoch 132 
2024-12-12 12:30:03.987490: Current learning rate: 0.00148 
2024-12-12 12:30:43.944015: train_loss -0.6623 
2024-12-12 12:30:43.949578: val_loss -0.6938 
2024-12-12 12:30:43.952630: Pseudo dice [np.float32(0.941)] 
2024-12-12 12:30:43.956172: Epoch time: 39.97 s 
2024-12-12 12:30:43.959205: Yayy! New best EMA pseudo Dice: 0.9269000291824341 
2024-12-12 12:30:44.727441:  
2024-12-12 12:30:44.732456: Epoch 133 
2024-12-12 12:30:44.735977: Current learning rate: 0.00141 
2024-12-12 12:31:24.058338: train_loss -0.6733 
2024-12-12 12:31:24.063352: val_loss -0.6929 
2024-12-12 12:31:24.066863: Pseudo dice [np.float32(0.935)] 
2024-12-12 12:31:24.070373: Epoch time: 39.33 s 
2024-12-12 12:31:24.073385: Yayy! New best EMA pseudo Dice: 0.9276999831199646 
2024-12-12 12:31:24.893260:  
2024-12-12 12:31:24.898293: Epoch 134 
2024-12-12 12:31:24.901381: Current learning rate: 0.00133 
2024-12-12 12:32:04.080141: train_loss -0.6669 
2024-12-12 12:32:04.085660: val_loss -0.6688 
2024-12-12 12:32:04.090674: Pseudo dice [np.float32(0.9041)] 
2024-12-12 12:32:04.093261: Epoch time: 39.19 s 
2024-12-12 12:32:04.641889:  
2024-12-12 12:32:04.646904: Epoch 135 
2024-12-12 12:32:04.650414: Current learning rate: 0.00126 
2024-12-12 12:32:42.657255: train_loss -0.6636 
2024-12-12 12:32:42.662900: val_loss -0.6496 
2024-12-12 12:32:42.665954: Pseudo dice [np.float32(0.9348)] 
2024-12-12 12:32:42.669002: Epoch time: 38.02 s 
2024-12-12 12:32:43.368403:  
2024-12-12 12:32:43.373945: Epoch 136 
2024-12-12 12:32:43.377559: Current learning rate: 0.00118 
2024-12-12 12:33:21.388494: train_loss -0.6706 
2024-12-12 12:33:21.394505: val_loss -0.6717 
2024-12-12 12:33:21.398519: Pseudo dice [np.float32(0.9409)] 
2024-12-12 12:33:21.401025: Epoch time: 38.02 s 
2024-12-12 12:33:21.404534: Yayy! New best EMA pseudo Dice: 0.9277999997138977 
2024-12-12 12:33:22.176980:  
2024-12-12 12:33:22.182540: Epoch 137 
2024-12-12 12:33:22.185079: Current learning rate: 0.00111 
2024-12-12 12:34:00.230386: train_loss -0.676 
2024-12-12 12:34:00.237142: val_loss -0.6685 
2024-12-12 12:34:00.240663: Pseudo dice [np.float32(0.9351)] 
2024-12-12 12:34:00.244269: Epoch time: 38.05 s 
2024-12-12 12:34:00.247801: Yayy! New best EMA pseudo Dice: 0.9284999966621399 
2024-12-12 12:34:01.045383:  
2024-12-12 12:34:01.050414: Epoch 138 
2024-12-12 12:34:01.054011: Current learning rate: 0.00103 
2024-12-12 12:34:39.057688: train_loss -0.6893 
2024-12-12 12:34:39.063706: val_loss -0.6554 
2024-12-12 12:34:39.067214: Pseudo dice [np.float32(0.9219)] 
2024-12-12 12:34:39.070225: Epoch time: 38.01 s 
2024-12-12 12:34:39.624759:  
2024-12-12 12:34:39.630274: Epoch 139 
2024-12-12 12:34:39.633788: Current learning rate: 0.00095 
2024-12-12 12:35:17.669046: train_loss -0.6617 
2024-12-12 12:35:17.674610: val_loss -0.626 
2024-12-12 12:35:17.677729: Pseudo dice [np.float32(0.9406)] 
2024-12-12 12:35:17.680271: Epoch time: 38.05 s 
2024-12-12 12:35:17.684323: Yayy! New best EMA pseudo Dice: 0.929099977016449 
2024-12-12 12:35:18.460521:  
2024-12-12 12:35:18.466565: Epoch 140 
2024-12-12 12:35:18.469082: Current learning rate: 0.00087 
2024-12-12 12:35:56.504184: train_loss -0.6495 
2024-12-12 12:35:56.509254: val_loss -0.7684 
2024-12-12 12:35:56.513801: Pseudo dice [np.float32(0.9542)] 
2024-12-12 12:35:56.516827: Epoch time: 38.04 s 
2024-12-12 12:35:56.519343: Yayy! New best EMA pseudo Dice: 0.9315999746322632 
2024-12-12 12:35:57.308904:  
2024-12-12 12:35:57.314419: Epoch 141 
2024-12-12 12:35:57.317931: Current learning rate: 0.00079 
2024-12-12 12:36:35.344001: train_loss -0.6836 
2024-12-12 12:36:35.349052: val_loss -0.7236 
2024-12-12 12:36:35.352564: Pseudo dice [np.float32(0.935)] 
2024-12-12 12:36:35.355072: Epoch time: 38.04 s 
2024-12-12 12:36:35.358580: Yayy! New best EMA pseudo Dice: 0.9319999814033508 
2024-12-12 12:36:36.137704:  
2024-12-12 12:36:36.143270: Epoch 142 
2024-12-12 12:36:36.146316: Current learning rate: 0.00071 
2024-12-12 12:37:14.166206: train_loss -0.6802 
2024-12-12 12:37:14.171723: val_loss -0.6932 
2024-12-12 12:37:14.175234: Pseudo dice [np.float32(0.9399)] 
2024-12-12 12:37:14.177742: Epoch time: 38.03 s 
2024-12-12 12:37:14.181755: Yayy! New best EMA pseudo Dice: 0.932699978351593 
2024-12-12 12:37:15.101525:  
2024-12-12 12:37:15.107095: Epoch 143 
2024-12-12 12:37:15.110203: Current learning rate: 0.00063 
2024-12-12 12:37:53.108906: train_loss -0.6598 
2024-12-12 12:37:53.114920: val_loss -0.6826 
2024-12-12 12:37:53.117933: Pseudo dice [np.float32(0.9468)] 
2024-12-12 12:37:53.121445: Epoch time: 38.01 s 
2024-12-12 12:37:53.123954: Yayy! New best EMA pseudo Dice: 0.9341999888420105 
2024-12-12 12:37:53.967540:  
2024-12-12 12:37:53.973621: Epoch 144 
2024-12-12 12:37:53.976691: Current learning rate: 0.00055 
2024-12-12 12:38:31.997508: train_loss -0.6658 
2024-12-12 12:38:32.003524: val_loss -0.7227 
2024-12-12 12:38:32.006029: Pseudo dice [np.float32(0.9523)] 
2024-12-12 12:38:32.009533: Epoch time: 38.03 s 
2024-12-12 12:38:32.012542: Yayy! New best EMA pseudo Dice: 0.9359999895095825 
2024-12-12 12:38:32.802508:  
2024-12-12 12:38:32.808133: Epoch 145 
2024-12-12 12:38:32.811172: Current learning rate: 0.00047 
2024-12-12 12:39:10.853070: train_loss -0.6792 
2024-12-12 12:39:10.859586: val_loss -0.7026 
2024-12-12 12:39:10.863102: Pseudo dice [np.float32(0.9394)] 
2024-12-12 12:39:10.865609: Epoch time: 38.05 s 
2024-12-12 12:39:10.869119: Yayy! New best EMA pseudo Dice: 0.9362999796867371 
2024-12-12 12:39:11.655517:  
2024-12-12 12:39:11.661095: Epoch 146 
2024-12-12 12:39:11.664679: Current learning rate: 0.00038 
2024-12-12 12:39:49.675073: train_loss -0.6694 
2024-12-12 12:39:49.680744: val_loss -0.7224 
2024-12-12 12:39:49.683250: Pseudo dice [np.float32(0.9454)] 
2024-12-12 12:39:49.686760: Epoch time: 38.02 s 
2024-12-12 12:39:49.690270: Yayy! New best EMA pseudo Dice: 0.9372000098228455 
2024-12-12 12:39:50.480425:  
2024-12-12 12:39:50.485436: Epoch 147 
2024-12-12 12:39:50.488945: Current learning rate: 0.0003 
2024-12-12 12:40:28.537135: train_loss -0.6745 
2024-12-12 12:40:28.542746: val_loss -0.746 
2024-12-12 12:40:28.545301: Pseudo dice [np.float32(0.9521)] 
2024-12-12 12:40:28.547863: Epoch time: 38.06 s 
2024-12-12 12:40:28.551933: Yayy! New best EMA pseudo Dice: 0.9387000203132629 
2024-12-12 12:40:29.331479:  
2024-12-12 12:40:29.337519: Epoch 148 
2024-12-12 12:40:29.340075: Current learning rate: 0.00021 
2024-12-12 12:41:07.345833: train_loss -0.6885 
2024-12-12 12:41:07.351347: val_loss -0.622 
2024-12-12 12:41:07.355867: Pseudo dice [np.float32(0.9506)] 
2024-12-12 12:41:07.358931: Epoch time: 38.01 s 
2024-12-12 12:41:07.362442: Yayy! New best EMA pseudo Dice: 0.9398999810218811 
2024-12-12 12:41:08.151474:  
2024-12-12 12:41:08.156484: Epoch 149 
2024-12-12 12:41:08.159498: Current learning rate: 0.00011 
2024-12-12 12:41:46.149974: train_loss -0.6496 
2024-12-12 12:41:46.155588: val_loss -0.7417 
2024-12-12 12:41:46.159675: Pseudo dice [np.float32(0.9479)] 
2024-12-12 12:41:46.162209: Epoch time: 38.0 s 
2024-12-12 12:41:46.165806: Yayy! New best EMA pseudo Dice: 0.9406999945640564 
2024-12-12 12:41:47.213988: Training done. 
2024-12-12 12:41:47.248494: Using splits from existing split file: C:\Users\linch\fyp\nnUNet_preprocessed\Dataset009_Spleen\splits_final.json 
2024-12-12 12:41:47.254496: The split file contains 5 splits. 
2024-12-12 12:41:47.259495: Desired fold for training: 0 
2024-12-12 12:41:47.263496: This split has 32 training and 9 validation cases. 
2024-12-12 12:41:47.268495: predicting spleen_10 
2024-12-12 12:41:47.274496: spleen_10, shape torch.Size([1, 55, 631, 631]), rank 0 
2024-12-12 12:41:55.934460: predicting spleen_13 
2024-12-12 12:41:55.949460: spleen_13, shape torch.Size([1, 38, 479, 479]), rank 0 
2024-12-12 12:41:58.923165: predicting spleen_14 
2024-12-12 12:41:58.933167: spleen_14, shape torch.Size([1, 54, 550, 550]), rank 0 
2024-12-12 12:42:06.823077: predicting spleen_17 
2024-12-12 12:42:06.836583: spleen_17, shape torch.Size([1, 48, 396, 396]), rank 0 
2024-12-12 12:42:09.808285: predicting spleen_31 
2024-12-12 12:42:09.818288: spleen_31, shape torch.Size([1, 56, 478, 478]), rank 0 
2024-12-12 12:42:14.269469: predicting spleen_33 
2024-12-12 12:42:14.283468: spleen_33, shape torch.Size([1, 83, 596, 596]), rank 0 
2024-12-12 12:42:27.467438: predicting spleen_44 
2024-12-12 12:42:27.486437: spleen_44, shape torch.Size([1, 92, 562, 562]), rank 0 
2024-12-12 12:42:40.663340: predicting spleen_47 
2024-12-12 12:42:40.681340: spleen_47, shape torch.Size([1, 88, 507, 507]), rank 0 
2024-12-12 12:42:48.122970: predicting spleen_56 
2024-12-12 12:42:48.139972: spleen_56, shape torch.Size([1, 46, 451, 451]), rank 0 
2024-12-12 12:42:56.972785: Validation complete 
2024-12-12 12:42:56.978783: Mean Validation Dice:  0.9100426154050769 
